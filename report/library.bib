Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Jain2013a,
abstract = {This paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modified learning technique that learns low-level features and higher-level weak spatial models. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows significant improvement over the current state-of-the-art results. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to outperform all existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on features that might even just cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent then expected. Many researchers previously argued that the kinematic structure and top-down information is crucial for this domain, but with our purely bottom up, and weak spatial model, we could improve other more complicated architectures that currently produce the best results. This mirrors what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.7302v3},
author = {Jain, Arjun and Tompson, Jonathan and Andriluka, Mykhaylo and Taylor, Graham W. and Bregler, Christoph},
eprint = {arXiv:1312.7302v3},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Jain2014.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
pages = {1--10},
title = {{Learning Human Pose Estimation Features with Convolutional Networks}},
url = {http://arxiv.org/abs/1312.7302},
year = {2013}
}
@book{LeCun1998a,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Y. and Bottou, L. and Orr, G. and Muller, K.},
booktitle = {Springer Lecture Notes in Computer Sciences},
doi = {10.1007/3-540-49430-8},
isbn = {9783642352881},
issn = {0302-9743},
title = {{Neural Networks: Tricks of the Trade}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Neural+Networks:+Tricks+of+the+Trade\#3},
year = {1998}
}
@article{Sermanet2013a,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:Users/ben/Library/Application Support/Mendeley Desktop/Downloaded/Sermanet et al. - 2013 - OverFeat Integrated Recognition , Localization and Detection using Convolutional Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6229},
pages = {1--15},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Tao,
author = {Tao, Lili and Paiement, Adeline and Damen, Dima and Mirmehdi, Majid and Hannuna, Sion and Camplani, Massimo and Burghardt, Tilo and Craddock, Ian},
file = {:Users/ben/Downloads/body\_pose\_representation\_pre-submit.pdf:pdf},
journal = {Computer Vision and Image Understanding - SI: Assistive Computer Vision and Robotics, Under review Submitted in March 2015.},
keywords = {continuous-state hmm motion analysis,human motion assessment,human motion quality,motion abnormality detection},
title = {{A Comparative Study of Pose Representation and Dynamics Modelling for Online Motion Quality Assessment}}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/ADADELTA.pdf:pdf},
journal = {arXiv preprint arXiv:1212.5701},
pages = {6},
title = {{AsaDelta: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@misc{Hamacher2011,
abstract = {Falls not only present a considerable health threat, but the resulting treatment and loss of working days also place a heavy economic burden on society. Gait instability is a major fall risk factor, particularly in geriatric patients, and walking is one of the most frequent dynamic activities of daily living. To allow preventive strategies to become effective, it is therefore imperative to identify individuals with an unstable gait. Assessment of dynamic stability and gait variability via biomechanical measures of foot kinematics provides a viable option for quantitative evaluation of gait stability, but the ability of these methods to predict falls has generally not been assessed. Although various methods for assessing gait stability exist, their sensitivity and applicability in a clinical setting, as well as their cost-effectiveness, need verification. The objective of this systematic review was therefore to evaluate the sensitivity of biomechanical measures that quantify gait stability among elderly individuals and to evaluate the cost of measurement instrumentation required for application in a clinical setting. To assess gait stability, a comparative effect size (Cohen's d) analysis of variability and dynamic stability of foot trajectories during level walking was performed on 29 of an initial yield of 9889 articles from four electronic databases. The results of this survey demonstrate that linear variability of temporal measures of swing and stance was most capable of distinguishing between fallers and non-fallers, whereas step width and stride velocity prove more capable of discriminating between old versus young (OY) adults. In addition, while orbital stability measures (Floquet multipliers) applied to gait have been shown to distinguish between both elderly fallers and non-fallers as well as between young and old adults, local stability measures ($\lambda$s) have been able to distinguish between young and old adults. Both linear and nonlinear measures of foot time series during gait seem to hold predictive ability in distinguishing healthy from fall-prone elderly adults. In conclusion, biomechanical measurements offer promise for identifying individuals at risk of falling and can be obtained with relatively low-cost tools. Incorporation of the most promising measures in combined retrospective and prospective studies for understanding fall risk and designing preventive strategies is warranted.},
author = {Hamacher, D. and Singh, N. B. and {Van Dieen}, J. H. and Heller, M. O. and Taylor, W. R.},
booktitle = {Journal of The Royal Society},
doi = {10.1098/rsif.2011.0416},
isbn = {1742-5662 (Electronic)$\backslash$r1742-5662 (Linking)},
issn = {1742-5689},
pages = {1682--1698},
pmid = {21880615},
title = {{Kinematic Measures for Assessing Gait Stability in Elderly Individuals: A Systematic Review}},
volume = {8},
year = {2011}
}
@article{Jain2014,
abstract = {In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.},
archivePrefix = {arXiv},
arxivId = {1409.7963},
author = {Jain, Arjun and Tompson, Jonathan and LeCun, Yann and Bregler, Christoph},
eprint = {1409.7963},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/jain2014MoDeep- A Deep Learning Framework Using Motion Features for Human Pose Estimation.pdf:pdf},
pages = {1--15},
title = {{MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation}},
url = {http://arxiv.org/abs/1409.7963},
year = {2014}
}
@inproceedings{Miao2012,
abstract = {The emergence of Kinect facilitates the possibility of depth capture$\backslash$nin real-time and with low cost by consumers. It also provides powerful$\backslash$ntool and inspiration for researchers to engage in new array of technology$\backslash$ndevelopment. However, the quality of the depth map captured from$\backslash$nKinect is still inadequate for many applications due to holes, noises$\backslash$nand artifacts existing within the depth information. In this paper,$\backslash$nwe present a texture assisted Kinect depth inpainting framework,$\backslash$naiming at obtaining improved depth information. In this framework,$\backslash$nthe relationship between texture and depth is investigated, and the$\backslash$ncharacteristics of depth are also exploited. More specifically, texture$\backslash$nedge information is extracted to assist the depth inpainting. Furthermore,$\backslash$nfiltering and diffusion are designed for hole-filling and edge alignment.$\backslash$nExperiment results demonstrate that the Kinect depth can be appropriately$\backslash$nrepaired in both smooth and edge region. Comparing with the original$\backslash$ndepth, the inpainted depth information enhances the quality of advanced$\backslash$nprocessing such as 3D reconstruction.},
author = {Miao, Dan and Fu, Jingjing and Lu, Yan and Li, Shipeng and Chen, Chang Wen},
booktitle = {ISCAS 2012 - 2012 IEEE International Symposium on Circuits and Systems},
doi = {10.1109/ISCAS.2012.6272103},
file = {:Users/ben/Downloads/Texture-assisted Kinect depth inpainting.pdf:pdf},
isbn = {978-1-4673-0219-7},
issn = {0271-4302},
pages = {604--607},
title = {{Texture-assisted Kinect depth inpainting}},
year = {2012}
}
@article{Szegedy2014,
archivePrefix = {arXiv},
arxivId = {1409.4842v1},
author = {Szegedy, Christian and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {1409.4842v1},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Szegedy2014a.pdf:pdf},
journal = {arXiv preprint arXiv:1409.4842},
pages = {1--12},
title = {{Going deeper with convolutions}},
year = {2014}
}
@article{Bonnechere2013,
author = {Bonnech\`{e}re, Bruno and Jansen, Bart},
file = {:Users/ben/Documents/csMSc/project/gait/research/Can the KinectTM sensors be used for motion analysis?.pdf:pdf},
journal = {Transaction on Electrical \ldots},
keywords = {biomechanics,motion analysis,new technology},
number = {1},
pages = {1--6},
title = {{Can the Kinect™ sensors be used for motion analysis?}},
url = {http://www.tsest.org/index.php/TEECS/article/view/202},
volume = {4},
year = {2013}
}
@article{Toshev,
archivePrefix = {arXiv},
arxivId = {1312.4659},
author = {Toshev, Alexander and Szegedy, Christian},
eprint = {1312.4659},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/DeepPose- Human Pose Estimation via Deep Neural Networks.pdf:pdf},
journal = {CVPR},
title = {{DeepPose: Human Pose Estimation via Deep Neural Networks}},
year = {2014}
}
@article{Chiu2014,
abstract = {In this study, a super-resolution (SR) reconstruction approach for Kinect 3D data is proposed. The proposed approach contains four steps: (1) extract the edge maps from the low-resolution (LR) depth map and the high-resolution (HR) color image using Canny edge detector, subsample the edge map of the HR color image, and segment the HR color image using mean shift segmentation, (2) detect and fill depth holes in the LR depth map, (3) upsample the LR depth map and reduce edge artifacts using local edge enhancement, and (4) perform HR depth map determination by energy cost minimization and refine the final HR depth map by joint bilateral filtering. Based on the experimental results obtained in this study, the performance of the proposed approach is better than those of four comparison approaches.},
author = {Chiu, Yu-Ping and Leou, Jin-Jang and Hsiao, Han-Hui},
doi = {10.1109/ISCAS.2014.6865733},
file = {:Users/ben/Downloads/Super-resolution Reconstruction for Kinect 3D Data.pdf:pdf},
isbn = {978-1-4799-3432-4},
issn = {02714310},
journal = {2014 IEEE International Symposium on Circuits and Systems (ISCAS)},
keywords = {Canny edge detector,Color,Image edge detection,Image reconstruction,Image resolution,Image segmentation,Joints,Kinect 3D data,Three-dimensional displays,depth hole filling,energy cost minimization,super-resolution reconstruction},
pages = {2712--2715},
title = {{Super-resolution reconstruction for Kinect 3D data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6865733},
year = {2014}
}
@inproceedings{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
doi = {10.1.1.207.2059},
issn = {15324435},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2010\_GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Schmitz2014,
abstract = {Markerless motion capture systems have developed in an effort to evaluate human movement in a natural setting. However, the accuracy and reliability of these systems remain understudied. Therefore, the goals of this study were to quantify the accuracy and repeatability of joint angles using a single camera markerless motion capture system and to compare the markerless system performance with that of a marker-based system. A jig was placed in multiple static postures with marker trajectories collected using a ten camera motion analysis system. Depth and color image data were simultaneously collected from a single Microsoft Kinect camera, which was subsequently used to calculate virtual marker trajectories. A digital inclinometer provided a measure of ground-truth for sagittal and frontal plane joint angles. Joint angles were calculated with marker data from both motion capture systems using successive body-fixed rotations. The sagittal and frontal plane joint angles calculated from the marker-based and markerless system agreed with inclinometer measurements by <0.5°. The systems agreed with each other by <0.5° for sagittal and frontal plane joint angles and <2° for transverse plane rotation. Both systems showed a coefficient of reliability <0.5° for all angles. These results illustrate the feasibility of a single camera markerless motion capture system to accurately measure lower extremity kinematics and provide a first step in using this technology to discern clinically relevant differences in the joint kinematics of patient populations. © 2013 Elsevier Ltd.},
author = {Schmitz, Anne and Ye, Mao and Shapiro, Robert and Yang, Ruigang and Noehren, Brian},
doi = {10.1016/j.jbiomech.2013.11.031},
file = {:Users/ben/Documents/csMSc/project/gait/research/Accuracy and repeatability of joint angles measured using a single camera markerless motion capture system.pdf:pdf},
isbn = {0021-9290},
issn = {00219290},
journal = {Journal of Biomechanics},
keywords = {Accuracy and reliability,Joint angles,Microsoft Kinect,Motion capture},
number = {2},
pages = {587--591},
pmid = {24315287},
publisher = {Elsevier},
title = {{Accuracy and repeatability of joint angles measured using a single camera markerless motion capture system}},
url = {http://dx.doi.org/10.1016/j.jbiomech.2013.11.031},
volume = {47},
year = {2014}
}
@article{Zhu2010,
abstract = {This paper addresses the problem of accurate and robust tracking of 3D human body pose from depth image sequences. Recovering the large number of degrees of freedom in human body movements from a depth image sequence is challenging due to the need to resolve the depth ambiguity caused by self-occlusions and the difficulty to recover from tracking failure. Human body poses could be estimated through model fitting using dense correspondences between depth data and an articulated human model (local optimization method). Although it usually achieves a high accuracy due to dense correspondences, it may fail to recover from tracking failure. Alternately, human pose may be reconstructed by detecting and tracking human body anatomical landmarks (key-points) based on low-level depth image analysis. While this method (key-point based method) is robust and recovers from tracking failure, its pose estimation accuracy depends solely on image-based localization accuracy of key-points. To address these limitations, we present a flexible Bayesian framework for integrating pose estimation results obtained by methods based on key-points and local optimization. Experimental results are shown and performance comparison is presented to demonstrate the effectiveness of the proposed approach.},
author = {Zhu, Youding and Fujimura, Kikuo},
doi = {10.3390/s100505280},
issn = {1424-8220},
journal = {Sensors (Basel, Switzerland)},
keywords = {Algorithms,Bayes Theorem,Biomechanical Phenomena,Humans,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Models, Anatomic,Posture},
number = {5},
pages = {5280--93},
pmid = {22399933},
title = {{A Bayesian framework for human body pose tracking from depth image sequences.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3292173\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2010}
}
@article{Giovanni,
author = {Giovanni, Matheus and Beleboni, Soares},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/A brief overview of Microsoft Kinect and its applications.pdf:pdf},
keywords = {computer vision,depth sensor,gesture recognition,microsoft kinect,natural interaction,object recognition,rgb-d,robotics applications,scene recognition,skeletal tracking},
pages = {1--6},
title = {{A brief overview of Microsoft Kinect and its applications}}
}
@article{Werfel2005,
abstract = {Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are sometimes used to overcome these difficulties. We analyze three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. Learning speed is defined as the rate of exponential decay in the learning curves. When the scalar parameter that controls the size of weight updates is chosen to maximize learning speed, node perturbation is slower than direct gradient descent by a factor equal to the number of output units; weight perturbation is slower still by an additional factor equal to the number of input units. Parallel perturbation allows faster learning than sequential perturbation, by a factor that does not depend on network size. We also characterize how uncertainty in quantities used in the stochastic updates affects the learning curves. This study suggests that in practice, weight perturbation may be slow for large networks, and node perturbation can have performance comparable to that of direct gradient descent when there are few output units. However, these statements depend on the specifics of the learning problem, such as the input distribution and the target function, and are not universally applicable.},
author = {Werfel, Justin and Xie, Xiaohui and Seung, H Sebastian},
doi = {10.1162/089976605774320539},
isbn = {0899-7667 (Print)$\backslash$n0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {12},
pages = {2699--2718},
pmid = {16212768},
title = {{Learning curves for stochastic gradient descent in linear feedforward networks.}},
volume = {17},
year = {2005}
}
@article{Chaudhry2013,
abstract = {Over the last few years, with the immense popularity of the Kinect, there has been renewed interest in developing methods for human gesture and action recognition from 3D data. A number of approaches have been proposed that extract representative features from 3D depth data, a reconstructed 3D surface mesh or more commonly from the recovered estimate of the human skeleton. Recent advances in neuroscience have discovered a neural encoding of static 3D shapes in primate infero-temporal cortex that can be represented as a hierarchy of medial axis and surface features. We hypothesize a similar neural encoding might also exist for 3D shapes in motion and propose a hierarchy of dynamic medial axis structures at several spatio-temporal scales that can be modeled using a set of Linear Dynamical Systems (LDSs). We then propose novel discriminative metrics for comparing these sets of LDSs for the task of human activity recognition. Combined with simple classification frameworks, our proposed features and corresponding hierarchical dynamical models provide the highest human activity recognition rates as compared to state-of-the-art methods on several skeletal datasets.},
author = {Chaudhry, Rizwan and Ofli, Ferda and Kurillo, Gregorij and Bajcsy, Ruzena and Vidal, Rene},
doi = {10.1109/CVPRW.2013.153},
file = {:Users/ben/Documents/csMSc/project/gait/research/Bio-inspired Dynamic 3D Discriminative Skeletal Features for Human Action Recognition.pdf:pdf},
isbn = {9780769549903},
issn = {21607508},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {471--478},
title = {{Bio-inspired dynamic 3D discriminative skeletal features for human action recognition}},
year = {2013}
}
@article{Horaud2014,
author = {Horaud, Georgios Evangelidis; Gurkirt Singh; Radu},
file = {:Users/ben/Documents/csMSc/project/gait/research/ECCVW2014.pdf:pdf},
journal = {European Conference on Computer Vision (ECCV) 2014 Chalearn Workshop},
title = {{Continuous gesture recognition from articulated poses}},
year = {2014}
}
@inproceedings{HerreraC.2011,
abstract = {We present an algorithm that simultaneously calibrates a color camera, a depth camera, and the relative pose between them. The method is designed to have three key features that no other available algorithm currently has: accurate, practical, applicable to a wide range of sensors. The method requires only a planar surface to be imaged from various poses. The calibration does not use color or depth discontinuities in the depth image which makes it flexible and robust to noise. We perform experiments with particular depth sensor and achieve the same accuracy as the propietary calibration procedure of the manufacturer.},
author = {{Herrera C.}, Daniel and Kannala, Juho and Heikkil\"{a}, Janne},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-23678-5\_52},
isbn = {9783642236778},
issn = {03029743},
keywords = {calibration,camera pair,depth camera},
number = {PART 2},
pages = {437--445},
title = {{Accurate and practical calibration of a depth and color camera pair}},
volume = {6855 LNCS},
year = {2011}
}
@article{Sharif2014,
abstract = {Recent results indicate that the generic descriptors ex- tracted from the convolutional neural networks are very powerful [ 10 , 29 , 48 ]. This paper adds to the mount- ing evidence that this is indeed the case. We report on a series of experiments conducted for different recogni- tion tasks using the publicly available code and model of the OverFeat network which was trained to perform ob- ject classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image represen- tation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the orig- inal task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L 2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representa- tions are further modified using simple augmentation tech- niques e.g. jittering. The results strongly suggest that fea- tures obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Sharif, Ali and Hossein, Razavian and Josephine, Azizpour and Stefan, Sullivan and Royal, K T H},
doi = {10.1109/CVPRW.2014.131},
eprint = {1403.6382},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/transferLearning/Razavian2014.pdf:pdf},
isbn = {9781479943081},
journal = {Cvpr'2014},
keywords = {CNN features,feature description,overfeat},
title = {{CNN Features off-the-shelf : an Astounding Baseline for Recognition}},
year = {2014}
}
@article{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann a. and Bottou, L\'{e}on and Orr, Genevieve B. and M\"{u}ller, Klaus Robert},
doi = {10.1007/978-3-642-35289-8-3},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/lecunEfficientBackProp.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {9--48},
title = {{Efficient backprop}},
volume = {7700 LECTU},
year = {2012}
}
@article{Chattopadhyay2014a,
abstract = {We explore the applicability of Kinect RGB-D streams in recognizing gait patterns of individuals. Gait energy volume (GEV) is a recently proposed feature that performs gait recognition in frontal view using only depth image frames from Kinect. Since depth frames from Kinect are inherently noisy, corresponding silhouette shapes are inaccurate, often merging with the background. We register the depth and RGB frames from Kinect to obtain smooth silhouette shape along with depth information. A partial volume reconstruction of the frontal surface of each silhouette is done and a novel feature termed as Pose Depth Volume (PDV) is derived from this volumetric model. Recognition performance of the proposed approach has been tested on a data set captured using Microsoft Kinect in an indoor environment. Experimental results clearly demonstrate the effectiveness of the approach in comparison with other existing methods. © 2013 Elsevier Inc. All rights reserved.},
author = {Chattopadhyay, Pratik and Roy, Aditi and Sural, Shamik and Mukhopadhyay, Jayanta},
doi = {10.1016/j.jvcir.2013.02.010},
file = {:Users/ben/Documents/csMSc/project/gait/research/Pose Depth Volume extraction from RGB-D streams for frontal gait recognition.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Depth registered silhouette,Frontal gait recognition,Key pose,Microsoft Kinect,Pose Depth Volume,RGB-D stream,Silhouette,Voxel volume},
number = {1},
pages = {53--63},
publisher = {Elsevier Inc.},
title = {{Pose depth Volume extraction from RGB-D streams for frontal gait recognition}},
url = {http://dx.doi.org/10.1016/j.jvcir.2013.02.010},
volume = {25},
year = {2014}
}
@article{Malinowski2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00221v1},
author = {Malinowski, M J and Matsinos, E},
eprint = {arXiv:1504.00221v1},
file = {:Users/ben/Downloads/Comparative study of the two versions of the.pdf:pdf},
keywords = {biomechanics,kinect,motion analysis,treadmill},
number = {July},
title = {{Comparative study of the two versions of the Microsoft Kinect TM sensor in regard to the analysis of human motion}},
year = {2014}
}
@article{Tompson,
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1115263},
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and Lecun, Yann and Bregler, Christopher},
eprint = {1115263},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Tompson2014Localization of Humans in Images Using Convolutional Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1411.4280},
primaryClass = {arXiv:submit},
title = {{Efficient Object Localization Using Convolutional Networks}},
year = {2014}
}
@article{Liu2013a,
abstract = {Depth maps provided by Microsoft Kinect often contain large dark holes around depth boundaries and occasional missing pixels in non-occluded regions, as well as noise, which prevent their further usage in real-world applications. In this paper, we present a graph Laplacian based framework to restore missing pixels based on the strong correlation between color image and depth map. To preserve sharp edges and remove noise, the TV21 (Total Variation) prior of depth maps is then integrated as an additional regularizer to the framework. Finally, an efficient and effective iterative optimization method with a closed-form solution at each iteration is presented to address this issue. Experiments conducted on both real scene images and synthetic images demonstrate that our approach gives better performance than commonly-used depth in painting schemes.},
author = {Liu, Shaoguo and Wang, Ying and Wang, Haibo and Pan, Chunhong},
doi = {10.1109/ACPR.2013.35},
file = {:Users/ben/Downloads/Kinect Depth Inpainting via Graph Laplacian with.pdf:pdf},
isbn = {978-1-4799-2190-4},
journal = {2013 2nd IAPR Asian Conference on Pattern Recognition},
keywords = {Color,Correlation,Depth Denoising,Depth Inpainting,Energy Minimization,Image color analysis,Image edge detection,Image restoration,Kinect depth inpainting,Laplace equations,Microsoft Kinect,Noise,TV21 Prior,TV21 regularization,closed-form solution,color image,depth boundaries,depth maps,graph Laplacian based framework,graph theory,image colour analysis,image denoising,image restoration,image sensors,iterative methods,iterative optimization method,missing pixels restoration,noise removal,nonoccluded regions,optimisation,real scene images,sharp edges,synthetic images,total variation},
pages = {251--255},
title = {{Kinect Depth Inpainting via Graph Laplacian with TV21 Regularization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6778320},
year = {2013}
}
@article{LeCun2004,
abstract = {We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13\% for SVM and 7\% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7\% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.},
author = {LeCun, Y. and Huang, Fu Jie and Bottou, L.},
doi = {10.1109/CVPR.2004.1315150},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting.pdf:pdf},
isbn = {0-7695-2158-4},
issn = {1063-6919},
journal = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
title = {{Learning methods for generic object recognition with invariance to pose and lighting}},
volume = {2},
year = {2004}
}
@misc{Ji2012,
abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep models that can act directly on the raw inputs. However, such models are currently limited to handle 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance of 3D CNN models, we propose to regularize the models with high-level features and combine the outputs of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and it achieves superior performance in comparison to baseline methods.},
author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2012.59},
issn = {0162-8828},
pmid = {22392705},
title = {{3D Convolutional Neural Networks for Human Action Recognition}},
year = {2012}
}
@misc{Khoshelham2012,
abstract = {This paper presents an investigation of the geometric quality of depth data obtained by the Kinect sensor. Based on the mathematical model of depth measurement by the sensor a theoretical error analysis is presented, which provides an insight into the factors influencing the accuracy of the data. Experimental results show that the random error of depth measurement increases with increasing distance to the sensor, and ranges from a few millimetres up to about 4 cm at the maximum range of the sensor. The accuracy of the data is also found to be influenced by the low resolution of the depth measurements.},
author = {Khoshelham, K.},
booktitle = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprsarchives-XXXVIII-5-W12-133-2011},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/ACCURACY ANALYSIS OF KINECT DEPTH DATA .pdf:pdf},
issn = {1682-1777},
pages = {133--138},
title = {{ACCURACY ANALYSIS OF KINECT DEPTH DATA}},
volume = {XXXVIII-5/},
year = {2012}
}
@article{Ciresan2011,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
author = {Ciresan, Dc and Meier, Ueli and Masci, Jonathan},
doi = {10.5591/978-1-57735-516-8/ijcai11-210},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/arch/Cires¸an2011.pdf:pdf},
isbn = {978-1-57735-514-4},
issn = {10450823},
journal = {International Joint Conference on Artificial Intelligence},
pages = {1237--1242},
title = {{Flexible, high performance convolutional neural networks for image classification}},
url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425},
year = {2011}
}
@article{Holte2012,
abstract = {This paper presents a review and comparative study of recent multi-view approaches for human 3D pose estimation and activity recognition. We discuss the application domain of human pose estimation and activity recognition and the associated requirements, covering: advanced human–computer interaction (HCI), assisted living, gesture-based interactive games, intelligent driver assistance systems, movies, 3D TV and animation, physical therapy, autonomous mental development, smart environments, sport motion analysis, video surveillance, and video annotation. Next, we review and categorize recent approaches which have been proposed to comply with these requirements. We report a comparison of the most promising methods for multi-view human action recognition using two publicly available datasets: the INRIA Xmas Motion Acquisition Sequences (IXMAS) Multi-View Human Action Dataset, and the i3DPost Multi-View Human Action and Interaction Dataset. To compare the proposed methods, we give a qualitative assessment of methods which cannot be compared quantitatively, and analyze some prominent 3D pose estimation techniques for application, where not only the performed action needs to be identified but a more detailed description of the body pose and joint configuration. Finally, we discuss some of the shortcomings of multi-view camera setups and outline our thoughts on future directions of 3D body pose estimation and human action recognition.},
author = {Holte, Michael B. and Tran, Cuong and Trivedi, Mohan M. and Moeslund, Thomas B.},
doi = {10.1109/JSTSP.2012.2196975},
file = {:Users/ben/Documents/csMSc/project/gait/research/Human Pose Estimation and Activity Recognition From Multi-View Videos- Comparative Explorations of Recent Developments.pdf:pdf},
issn = {19324553},
journal = {IEEE Journal on Selected Topics in Signal Processing},
keywords = {3-D,INRIA Xmas Motion Acquisition Sequences (IXMAS),comparative study,human action recognition,human pose estimation,i3DPost,maker-less,multi-view,survey,view-invariance,vision-based,volumetric reconstruction},
number = {5},
pages = {538--552},
title = {{Human pose estimation and activity recognition from multi-view videos: Comparative explorations of recent developments}},
volume = {6},
year = {2012}
}
@inproceedings{Taylor2010,
abstract = {We address the problem of learning good features for understanding video data. We introduce a model that learns latent representations of image sequences from pairs of successive images. The convolutional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization. In experiments on the NORB dataset, we show our model extracts latent “flow fields” which correspond to the transformation between the pair of input frames. We also use our model to extract low-level motion features in a multi-stage architecture for action recognition, demonstrating competitive performance on both the KTH and Hollywood2 datasets.},
author = {Taylor, Graham W. and Fergus, Rob and LeCun, Yann and Bregler, Christoph},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15567-3\_11},
isbn = {3642155669},
issn = {03029743},
keywords = {activity recognition,convolutional nets,optical flow,restricted Boltzmann machines,unsupervised learning,video analysis},
number = {PART 6},
pages = {140--153},
title = {{Convolutional learning of spatio-temporal features}},
volume = {6316 LNCS},
year = {2010}
}
@misc{Johansson1973a,
abstract = {This paper reports the first phase of a research program on visual perception of motion patterns characteristic of living organisms in locomotion. Such motion patterns in animals and men are termed here as biological motion. They are characterized by a far higher degree of complexity than the patterns of simple mechanical motions usually studied in our laboratories. In everyday perceptions, the visual information from biological motion and from the corresponding figurative contour patterns (the shape of the body) are intermingled. A method for studying information from the motion pattern per se without interference with the form aspect was devised. In short, the motion of the living body was represented by a few bright spots describing the motions of the main joints. It is found that 10–12 such elements in adequate motion combinations in proximal stimulus evoke a compelling impression of human walking, running, dancing, etc. The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to these biological motion patterns. The validity of this model in the present context was experimentally tested and the results turned out to be highly positive.},
archivePrefix = {arXiv},
arxivId = {19433921},
author = {Johansson, Gunnar},
booktitle = {Perception \& Psychophysics},
doi = {10.3758/BF03212378},
eprint = {19433921},
isbn = {0031-5117},
issn = {0031-5117},
pages = {201--211},
pmid = {22024246},
title = {{Visual perception of biological motion and a model for its analysis}},
volume = {14},
year = {1973}
}
@article{Zhao,
author = {Zhao, Yang and Liu, Zicheng and Yang, Lu and Cheng, Hong},
file = {:Users/ben/Documents/csMSc/project/gait/research/Combing RGB and Depth Map Features for Human Activity Recognition.pdf:pdf},
journal = {!!!},
title = {{Combing RGB and Depth Map Features for Human Activity Recognition}}
}
@article{Feng2013,
annote = {no mention of time},
author = {Feng, Litong and Po, Lai Man and Xu, Xuyuan and Ng, Ka Ho and Cheung, Chun Ho and Cheung, Kwok Wai},
doi = {10.1109/IECON.2013.6699501},
file = {:Users/ben/Downloads/AN ADAPTIVE BACKGROUND BIASED DEPTH MAP HOLE-FILLING.pdf:pdf},
isbn = {9781479902248},
issn = {1553-572X},
journal = {IECON Proceedings (Industrial Electronics Conference)},
keywords = {Automatic thresholding,Depth map in-painting,Hole-filling,Image enhancement,Kinect,Random walks},
pages = {2366--2371},
title = {{An adaptive background biased depth map hole-filling method for Kinect}},
year = {2013}
}
@article{VanSwearingen1996,
abstract = {BACKGROUND AND PURPOSE: The purpose of this study was to determine the reliability and validity of measurements obtained with a seven-item modified version of the Gait Abnormality Rating Scale (GARS-M), an assessment of gait designed to predict risk of falling among community-dwelling, frail older persons. SUBJECTS: Fifty-two community-dwelling, frail older persons, with a mean age of 74.8 years (SD = 6.75), participated. METHODS: A history of falls was determined from self-report or by proxy report. The GARS-M was scored from videotapes of subjects walking at self-selected paces. Gait characteristics were recorded during a timed walk on a 6-m brown-paper walkway. RESULTS: Scores obtained by three raters for 23 subjects demonstrated moderate to substantial intrarater and interrater reliability. Concurrent validity, as assessed by Spearman rank-order correlation coefficients, was demonstrated for the relationship between GARS-M scores and stride length (r = -.754) and for the relationship between GARS-M scores and walking speed (r = -.679). Mean GARS-M scores distinguished between frail older persons with and without a history of recurrent falls (mean GARS-M scores of 9.0 and 3.8, respectively). CONCLUSION AND DISCUSSION: The GARS-M is a reliable and valid measure for documenting gait features associated with an increased risk of falling among community-dwelling, frail older persons and may provide a clinically useful alternative to established quantitative gait-assessment methods.},
author = {VanSwearingen, J. M. and Paschal, K. A. and Bonino, P. and Yang, J. F.},
file = {:Users/ben/Documents/csMSc/project/gait/research/introduction/The Modified Gait Abnormality Rating Scale for Recognizing the Risk of Recurrent Falls in CommunitvDwelling Elderly Adults .pdf:pdf},
issn = {0031-9023},
journal = {Physical therapy},
number = {9},
pages = {994--1002},
pmid = {8790277},
title = {{The modified Gait Abnormality Rating Scale for recognizing the risk of recurrent falls in community-dwelling elderly adults.}},
volume = {76},
year = {1996}
}
@article{Ji2013b,
abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
author = {Ji, Shuiwang and Yang, Ming and Yu, Kai and Xu, Wei},
doi = {10.1109/TPAMI.2012.59},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/ji2010.pdf:pdf},
isbn = {9781605589077},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Decision Support Techniques,Image Interpretation,Imaging,Movement,Movement: physiology,Neural Networks (Computer),Pattern Recognition,Subtraction Technique,Three-Dimensional,Three-Dimensional: methods},
number = {1},
pages = {221--31},
pmid = {22392705},
title = {{3D convolutional neural networks for human action recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6165309$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/22392705},
volume = {35},
year = {2013}
}
@article{Hertzmann2004,
author = {Hertzmann, Aaron and Hertzmann, Aaron},
doi = {10.1145/1103900.1103922},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Introduction to Bayesian Learning Aaron Hertzmann.pdf:pdf},
isbn = {0111456789},
journal = {Proceedings of the conference on SIGGRAPH 2004 course notes - GRAPH '04},
pages = {22--es},
title = {{Introduction to Bayesian learning}},
url = {http://portal.acm.org/citation.cfm?doid=1103900.1103922},
year = {2004}
}
@article{Zhang2013,
abstract = {This paper presents a novel method to extract skeletons of complex articulated objects from 3D point cloud sequences collected by the Kinect. Our approach is more robust than the traditional video-based and stereo-based approaches, as the Kinect directly provides 3D information without any markers, 2D-to-3D-transition assumptions, and feature point extraction. We track all the raw 3D points on the object, and utilize the point trajectories to determine the object skeleton. The point tracking is achieved by the 3D non-rigid matching based on the Markov Random Field (MRF) Deformation Model. To reduce the large computational cost of the non-rigid matching, a coarse-to-fine procedure is proposed. To the best of our knowledge, this is the first to extract skeletons of highly deformable objects from 3D point cloud sequences by point tracking. Experiments prove our method's good performance, and the extracted skeletons are successfully applied to the motion capture. © 2012 Elsevier B.V.},
annote = {tracks trajectories},
author = {Zhang, Quanshi and Song, Xuan and Shao, Xiaowei and Shibasaki, Ryosuke and Zhao, Huijing},
doi = {10.1016/j.neucom.2011.11.032},
file = {:Users/ben/Documents/csMSc/project/gait/research/Unsupervised skeleton extraction and motion capture from 3D deformable matching.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {3D point cloud sequence,Skeleton extraction},
pages = {170--182},
publisher = {Elsevier},
title = {{Unsupervised skeleton extraction and motion capture from 3D deformable matching}},
url = {http://dx.doi.org/10.1016/j.neucom.2011.11.032},
volume = {100},
year = {2013}
}
@inproceedings{Karpathy2014,
abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
author = {Karpathy, Andrej and Leung, Thomas},
booktitle = {Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.223},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/karpathy14.pdf:pdf},
isbn = {978-1-4799-5118-5},
pages = {1725--1732},
title = {{Large-scale Video Classification with Convolutional Neural Networks}},
year = {2014}
}
@article{Sung2011,
abstract = {Being able to detect and recognize human activities is important for making personal assistant robots useful in performing assistive tasks. The challenge is to develop a system that is low-cost, reliable in unstructured home settings, and also straightforward to use. In this paper, we use a RGBD sensor (Microsoft Kinect) as the input sensor, and present learning algorithms to infer the activities. Our algorithm is based on a hierarchical maximum entropy Markov model (MEMM). It considers a person's activity as composed of a set of sub-activities, and infers the two-layered graph structure using a dynamic programming approach. We test our algorithm on detecting and recognizing twelve different activities performed by four people in different environments, such as a kitchen, a living room, an office, etc., and achieve an average performance of 84.3\% when the person was seen before in the training set (and 64.2\% when the person was not seen before).},
archivePrefix = {arXiv},
arxivId = {1107.0169},
author = {Sung, Jaeyong and Ponce, Colin and Selman, Bart and Saxena, Ashutosh},
doi = {citeulike-article-id:9510173},
eprint = {1107.0169},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/datasets/Sung2011.pdf:pdf},
isbn = {9781577355328},
journal = {AAAI workshop on Pattern, Activity and Intent Recognition (PAIR)},
title = {{Human Activity Detection from RGBD Images}},
url = {http://arxiv.org/abs/1107.0169},
year = {2011}
}
@article{StoyanovTodorandLouloudiAthanasiaandAndreassonHenrikandLilienthal2011a,
abstract = {3D range sensing is one of the important topics in robotics, as it is often a component in vital autonomous subsystems like collision avoidance, mapping and semantic perception. The development of affordable, high frame rate and precise 3D range sensors is thus of considerable interest. Recent advances in sensing technology have produced several novel sensors that attempt to meet these requirements. This work is concerned with the development of a holistic method for accuracy evaluation of the measurements produced by such devices. A method for comparison of range sensor output to a set of reference distance measurements is proposed. The ap- proach is then used to compare the behavior of three integrated range sensing devices, to that of a standard actuated laser range sensor. Test cases in an uncontrolled indoor environment are performed in order to evaluate the sensors’ performance in a challenging, realistic application scenario.},
author = {{Stoyanov, T Louloudi, A Andreasson, H Lilienthal}, A},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/Comparative Evaluation of Range Sensor Accuracy in Indoor Environments.pdf:pdf},
journal = {Proceedings of the 5th European Conference on Mobile Robots, ECMR 2011},
pages = {19--24},
title = {{Comparative evaluation of range sensor accuracy in indoor environments}},
url = {http://www.aass.oru.se/Research/Learning/publications/2011/Stoyanov\_etal\_2011-ECMR11-Comparative\_Evaluation\_of\_Range\_Sensor\_Accuracy\_in\_Indoor\_Environments.pdf$\backslash$nhttp://oru.diva-portal.org/smash/record.jsf?pid=diva2:540987},
year = {2011}
}
@article{Bottou2010,
abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
author = {Bottou, Le\'{o}n},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Bottou2010.pdf:pdf},
journal = {Proceedings of COMPSTAT'2010},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
title = {{Large-Scale Machine Learning with Stochastic Gradient Descent}},
year = {2010}
}
@article{Chen2011a,
abstract = {In this paper we propose a new examplar-based approach to recover 3D human poses from monocular images. Given the visual feature of each frame, pose retrieval is first conducted in the examplar database to find relevant pose candidates. Then, dynamic programming is applied on the pose candidates to recover a continuous pose sequence. We make two contributions within this framework. First, we propose to use an efficient feature selection algorithm to select effective visual feature components. The task is formulated as a trace-ratio criterion which measures the score of the selected feature component subset, and the criterion is efficiently optimized to achieve the global optimum. The selected components are used instead of the original full feature set to improve the accuracy and efficiency of pose recovery. As second contribution, we propose to use sparse representation to retrieve the pose candidates, where the measured visual feature is expressed as a sparse linear combination of the examplars in the database. Sparse representation ensures that semantically similar poses have larger probability to be retrieved. The effectiveness of our approach is validated quantitatively through extensive evaluations on both synthetic and real data, and qualitatively by inspecting the results of the real time system we have implemented. ?? 2010 Elsevier Inc. All rights reserved.},
author = {Chen, Cheng and Yang, Yi and Nie, Feiping and Odobez, Jean Marc},
doi = {10.1016/j.cviu.2010.11.007},
isbn = {0818608773},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Feature selection,Motion understanding,Pose recovery,Sparse representation},
pages = {290--299},
title = {{3D human pose recovery from image by efficient visual feature selection}},
volume = {115},
year = {2011}
}
@article{Goodfellow2013,
abstract = {Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over \$96\backslash\%\$ accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving \$97.84\backslash\%\$ accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over \$90\backslash\%\$ accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a \$99.8\backslash\%\$ accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.},
archivePrefix = {arXiv},
arxivId = {1312.6082},
author = {Goodfellow, Ian J and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
eprint = {1312.6082},
journal = {CoRR},
pages = {1--13},
title = {{Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1312.6082$\backslash$nhttp://arxiv.org/pdf/1312.6082v4.pdf},
volume = {abs/1312.6},
year = {2013}
}
@inproceedings{Zhang2011b,
abstract = {Commodity depth cameras have created many interesting new applications in the research community recently. These applications often require the calibration information between the color and the depth cameras. Traditional checkerboard based calibration schemes fail to work well for the depth camera, since its corner features cannot be reliably detected in the depth image. In this paper, we present a maximum likelihood solution for the joint depth and color calibration based on two principles. First, in the depth image, points on the checker-board shall be co-planar, and the plane is known from color camera calibration. Second, additional point correspondences between the depth and color images may be manually specified or automatically established to help improve calibration accuracy. Uncertainty in depth values has been taken into account systematically. The proposed algorithm is reliable and accurate, as demonstrated by extensive experimental results on simulated and real-world examples.},
author = {Zhang, Cha and Zhang, Zhengyou},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2011.6012191},
isbn = {9781612843490},
issn = {19457871},
keywords = {calibration,depth camera},
title = {{Calibration between depth and color sensors for commodity depth cameras}},
year = {2011}
}
@article{Paiement,
author = {Paiement, A. and Tao, L. and Hannuna, S. and Camplani, M. and Damen, D. and Mirmehdi, M.},
file = {:Users/ben/Documents/csMSc/project/gait/research/introduction/Online quality assessment of human motion from skeleton data.pdf:pdf},
journal = {BMVA press},
title = {{Online quality assessment of human movement from skeleton data}},
year = {2014}
}
@misc{Vapnik1994,
abstract = {A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.},
author = {Vapnik, Vladimir and Levin, Esther and Cun, Yann Le},
booktitle = {Neural Computation},
doi = {10.1162/neco.1994.6.5.851},
issn = {0899-7667},
number = {5},
pages = {851--876},
title = {{Measuring the VC-Dimension of a Learning Machine. Neural Computation}},
volume = {6},
year = {1994}
}
@article{Knoop2009,
abstract = {In this article, we present an approach for the fusion of 2d and 3d measurements for model-based person tracking, also known as Human Motion Capture. The applied body model is defined geometrically with generalized cylinders, and is set up hierarchically with connecting joints of different types. The joint model can be parameterized to control the degrees of freedom, adhesion and stiffness. This results in an articulated body model with constrained kinematic degrees of freedom. The fusion approach incorporates this model knowledge together with the measurements, and tracks the target body iteratively with an extended Iterative Closest Point (ICP) approach. Generally, the ICP is based on the concept of correspondences between measurements and model, which is normally exploited to incorporate 3d point cloud measurements. The concept has been generalized to represent and incorporate also 2d image space features. Together with the 3D point cloud from a 3d time-of-flight (ToF) camera, arbitrary features, derived from 2D camera images, are used in the fusion algorithm for tracking of the body. This gives complementary information about the tracked body, enabling not only tracking of depth motions but also turning movements of the human body, which is normally a hard problem for markerless human motion capture systems. The resulting tracking system, named VooDoo is used to track humans in a Human-Robot Interaction (HRI) context. We only rely on sensors on board the robot, i.e. the color camera, the ToF camera and a laser range finder. The system runs in realtime (∼20 Hz) and is able to robustly track a human in the vicinity of the robot. © 2008 Elsevier B.V. All rights reserved.},
author = {Knoop, Steffen and Vacek, Stefan and Dillmann, R\"{u}diger},
doi = {10.1016/j.robot.2008.10.017},
file = {:Users/ben/Documents/csMSc/project/gait/research/Fusion of 2d and 3d sensor data for articulated body tracking.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {3D body model,Human motion capture,Human robot interaction,Sensor fusion,Time-of-flight},
number = {3},
pages = {321--329},
publisher = {Elsevier B.V.},
title = {{Fusion of 2d and 3d sensor data for articulated body tracking}},
url = {http://dx.doi.org/10.1016/j.robot.2008.10.017},
volume = {57},
year = {2009}
}
@article{Gabel2012,
abstract = {Human gait is an important indicator of health, with applications ranging from diagnosis, monitoring, and rehabilitation. In practice, the use of gait analysis has been limited. Existing gait analysis systems are either expensive, intrusive, or require well-controlled environments such as a clinic or a laboratory. We present an accurate gait analysis system that is economical and non-intrusive. Our system is based on the Kinect sensor and thus can extract comprehensive gait information from all parts of the body. Beyond standard stride information, we also measure arm kinematics, demonstrating the wide range of parameters that can be extracted. We further improve over existing work by using information from the entire body to more accurately measure stride intervals. Our system requires no markers or battery-powered sensors, and instead relies on a single, inexpensive commodity 3D sensor with a large preexisting install base. We suggest that the proposed technique can be used for continuous gait tracking at home.},
author = {Gabel, Moshe and Gilad-Bachrach, Ran and Renshaw, Erin and Schuster, Assaf},
doi = {10.1109/EMBC.2012.6346340},
file = {:Users/ben/Downloads/Full Body Gait Analysis with Kinect∗.pdf:pdf},
isbn = {9781424441198},
issn = {1557170X},
journal = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
pages = {1964--1967},
pmid = {23366301},
title = {{Full body gait analysis with Kinect}},
year = {2012}
}
@book{Hutchison1973,
author = {Hutchison, David and Mitchell, John C},
file = {:Users/ben/Documents/csMSc/project/gait/neuralNetworksTricksOfTheTrade.pdf:pdf},
isbn = {9783642352881},
title = {{Neural Networks : Tricks of the Trade Second Edition}},
year = {1973}
}
@article{Osadchy2007,
abstract = {We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a low-dimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets-for frontal views, rotated faces, and profiles is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.},
author = {Osadchy, Margarita and {Le Cun}, Yann and Miller, Matthew L},
doi = {10.1007/11957959},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Synergistic Face Detection and Pose Estimation with Energy-Based Models\_Adeline Suggestion.pdf:pdf},
isbn = {9783540687948},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {convolutional networks,energy based models,face detection,object recognition,pose estimation},
pages = {1197--1215},
title = {{Synergistic face detection and pose estimation with energy-based models}},
volume = {8},
year = {2007}
}
@article{Schwarz2015,
author = {Schwarz, Max and Schulz, Hannes and Behnke, Sven},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/transferLearning/Schwarz2015.pdf:pdf},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
keywords = {Computer Vision for Robotics and Automation,Object detection,categorization,segmentation},
title = {{RGB-D Object Recognition and Pose Estimation based on Pre-trained Convolutional Neural Network Features}},
year = {2015}
}
@article{Clark2012a,
abstract = {Clinically feasible methods of assessing postural control such as timed standing balance and functional reach tests provide important information, however, they cannot accurately quantify specific postural control mechanisms. The Microsoft Kinect??? system provides real-time anatomical landmark position data in three dimensions (3D), and given that it is inexpensive, portable and simple to setup it may bridge this gap. This study assessed the concurrent validity of the Microsoft Kinect??? against a benchmark reference, a multiple-camera 3D motion analysis system, in 20 healthy subjects during three postural control tests: (i) forward reach, (ii) lateral reach, and (iii) single-leg eyes-closed standing balance. For the reach tests, the outcome measures consisted of distance reached and trunk flexion angle in the sagittal (forward reach) and coronal (lateral reach) planes. For the standing balance test the range and deviation of movement in the anatomical landmark positions for the sternum, pelvis, knee and ankle and the lateral and anterior trunk flexion angle were assessed. The Microsoft Kinect??? and 3D motion analysis systems had comparable inter-trial reliability (ICC difference=0.06. ??. 0.05; range, 0.00-0.16) and excellent concurrent validity, with Pearson's . r-values >0.90 for the majority of measurements (r=0.96. ??. 0.04; range, 0.84-0.99). However, ordinary least products analyses demonstrated proportional biases for some outcome measures associated with the pelvis and sternum. These findings suggest that the Microsoft Kinect??? can validly assess kinematic strategies of postural control. Given the potential benefits it could therefore become a useful tool for assessing postural control in the clinical setting. ?? 2012 Elsevier B.V.},
author = {Clark, Ross A. and Pua, Yong Hao and Fortin, Karine and Ritchie, Callan and Webster, Kate E. and Denehy, Linda and Bryant, Adam L.},
doi = {10.1016/j.gaitpost.2012.03.033},
isbn = {1879-2219},
issn = {09666362},
journal = {Gait and Posture},
keywords = {Assessment,Balance,Falls,Kinematic,Posture},
pages = {372--377},
pmid = {22633015},
title = {{Validity of the Microsoft Kinect for assessment of postural control}},
volume = {36},
year = {2012}
}
@article{Yang2015,
author = {Yang, Lin and Zhang, Longyu and Dong, Haiwei and Alelaiwi, Abdulhameed and {El Saddik}, Abdulmotaleb},
doi = {10.1109/JSEN.2015.2416651},
file = {:Users/ben/Downloads/Evaluating and Improving the Depth Accuracy of.pdf:pdf},
issn = {1530-437X},
journal = {IEEE Sensors Journal},
number = {c},
pages = {1--1},
title = {{Evaluating and Improving the Depth Accuracy of Kinect for Windows v2}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7067384},
year = {2015}
}
@article{Sutskever2013a,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6639346},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/sutskever13.pdf:pdf},
isbn = {978-1-4799-0356-6},
journal = {JMLR W\&CP},
pages = {1139--1147},
title = {{On the importance of initialization and momentum in deep learning}},
volume = {28},
year = {2013}
}
@article{Szeliski2010,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
author = {Szeliski, Richard},
doi = {10.1007/978-1-84882-935-0},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Computer Vision- Algorithms and Applications Richard Szeliski.pdf:pdf},
isbn = {1848829345},
issn = {10636919},
journal = {Computer},
pages = {832},
pmid = {16259003},
title = {{Computer Vision : Algorithms and Applications}},
url = {http://research.microsoft.com/en-us/um/people/szeliski/book/drafts/szelski\_20080330am\_draft.pdf},
volume = {5},
year = {2010}
}
@article{Simonyan2014,
abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to incorporate into the network design aspects of the best performing hand-crafted features. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it matches the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2199v1},
author = {Simonyan, Karen and Zisserman, A},
eprint = {arXiv:1406.2199v1},
journal = {arXiv preprint arXiv:1406.2199},
pages = {1--11},
title = {{Two-Stream Convolutional Networks for Action Recognition in Videos}},
url = {http://arxiv.org/abs/1406.2199},
year = {2014}
}
@article{Zhang2011c,
abstract = {Commodity depth cameras have created many interesting new applications in the research community recently. These applications often require the calibration information between the color and the depth cameras. Traditional checkerboard based calibration schemes fail to work well for the depth camera, since its corner features cannot be reliably detected in the depth image. In this paper, we present a maximum likelihood solution for the joint depth and color calibration based on two principles. First, in the depth image, points on the checker-board shall be co-planar, and the plane is known from color camera calibration. Second, additional point correspondences between the depth and color images may be manually specified or automatically established to help improve calibration accuracy. Uncertainty in depth values has been taken into account systematically. The proposed algorithm is reliable and accurate, as demonstrated by extensive experimental results on simulated and real-world examples.},
author = {Zhang, Cha and Zhang, Zhengyou},
doi = {10.1109/ICME.2011.6012191},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/CALIBRATION BETWEEN DEPTH AND COLOR SENSORS FOR COMMODITY DEPTH CAMERAS.pdf:pdf},
isbn = {9781612843490},
issn = {19457871},
journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
keywords = {calibration,depth camera},
title = {{Calibration between depth and color sensors for commodity depth cameras}},
year = {2011}
}
@misc{Zhang2012,
abstract = {Recent advances in 3D depth cameras such as Microsoft Kinect sensors (www.xbox.com/en-US/kinect) have created many opportunities for multimedia computing. The Kinect sensor lets the computer directly sense the third dimension (depth) of the players and the environment. It also understands when users talk, knows who they are when they walk up to it, and can interpret their movements and translate them into a format that developers can use to build new experiences. While the Kinect sensor incorporates several advanced sensing hardware, this article focuses on the vision aspect of the Kinect sensor and its impact beyond the gaming industry.},
author = {Zhang, Zhengyou},
booktitle = {IEEE Multimedia},
doi = {10.1109/MMUL.2012.24},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/Microsoft Kinect Sensor and Its Effect.pdf:pdf},
isbn = {1070-986X},
issn = {1070986X},
keywords = {Microsoft Kinect,computer vision,human-computer interaction,motion capture,multimedia},
pages = {4--10},
title = {{Microsoft kinect sensor and its effect}},
volume = {19},
year = {2012}
}
@article{Ionescu2011,
abstract = {We present an approach for automatic 3D human pose reconstruction from monocular images, based on a discriminative formulation with latent segmentation inputs. We advanced the field of structured prediction and human pose reconstruction on several fronts. First, by working with a pool of figure-ground segment hypotheses, the prediction problem is formulated in terms of combined learning and inference over segment hypotheses and 3D human articular configurations. Beside constructing tractable formulations for the combined segment selection and pose estimation problem, we propose new augmented kernels that can better encode complex dependencies between output variables. Furthermore, we provide primal linear re-formulations based on Fourier kernel approximations, in order to scale-up the non-linear latent structured prediction methodology. The proposed models are shown to be competitive in the HumanEva benchmark and are also illustrated in a clip collected from a Hollywood movie, where the model can infer human poses from monocular images captured in complex environments.},
author = {Ionescu, Catalin and Li, Fuxin and Sminchisescu, Cristian},
doi = {10.1109/ICCV.2011.6126500},
isbn = {978-1-4577-1100-8},
issn = {1550-5499},
journal = {2011 International Conference on Computer Vision},
pages = {2220--2227},
title = {{Latent structured models for human pose estimation}},
year = {2011}
}
@article{Gokturk2004,
abstract = {This paper describes a CMOS-based time-of-flight depth sensor and presents some experimental data while addressing various issues arising from its use. Our system is a single-chip solution based on a special CMOS pixel structure that can extract phase information from the received light pulses. The sensor chip integrates a 64x64 pixel array with a high-speed clock generator and ADC. A unique advantage of the chip is that it can be manufactured with an ordinary CMOS process. Compared with other types of depth sensors reported in the literature, our solution offers significant advantages, including superior accuracy, high frame rate, cost effectiveness and a drastic reduction in processing required to construct the depth maps. We explain the factors that determine the resolution of our system, discuss various problems that a time-of-flight depth sensor might face, and propose practical solutions.},
author = {Gokturk, Sb and Yalcin, H and Bamji, C},
doi = {10.1109/CVPR.2004.291},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/A Time-Of-Flight Depth Sensor – System Description, Issues and Solutions .pdf:pdf},
isbn = {0769521584},
journal = {Computer Vision and Pattern \ldots},
number = {C},
pages = {35--35},
title = {{A time-of-flight depth sensor-system description, issues and solutions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1384826$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1384826},
volume = {00},
year = {2004}
}
@article{Shotton2013,
abstract = {We describe two new approaches to human pose estimation. Both can quickly and accurately predict the 3D positions of body joints from a single depth image without using any temporal information. The key to both approaches is the use of a large, realistic, and highly varied synthetic set of training images. This allows us to learn models that are largely invariant to factors such as pose, body shape, field-of-view cropping, and clothing. Our first approach employs an intermediate body parts representation, designed so that an accurate per-pixel classification of the parts will localize the joints of the body. The second approach instead directly regresses the positions of body joints. By using simple depth pixel comparison features and parallelizable decision forests, both approaches can run super-real time on consumer hardware. Our evaluation investigates many aspects of our methods, and compares the approaches to each other and to the state of the art. Results on silhouettes suggest broader applicability to other imaging modalities.},
author = {Shotton, Jamie and Girshick, Ross and Fitzgibbon, Andrew and Sharp, Toby and Cook, Mat and Finocchio, Mark and Moore, Richard and Kohli, Pushmeet and Criminisi, Antonio and Kipman, Alex and Blake, Andrew},
doi = {10.1109/TPAMI.2012.241},
isbn = {978-1-4577-0394-2},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer vision,depth cues,games,machine learning,pixel classification,range data},
pages = {2821--2840},
pmid = {24136424},
title = {{Efficient human pose estimation from single depth images}},
volume = {35},
year = {2013}
}
@inproceedings{Wang2012,
abstract = {Full end-to-end text recognition in natural images is a challenging problem that has received much attention recently. Traditional systems in this area have relied on elaborate models incorporating carefully hand-engineered features or large amounts of prior knowledge. In this paper, we take a different route and combine the representational power of large, multilayer neural networks together with recent developments in unsupervised feature learning, which allows us to use a common framework to train highly-accurate text detector and character recognizer modules. Then, using only simple off-the-shelf methods, we integrate these two modules into a full end-to-end, lexicon-driven, scene text recognition system that achieves state-of-the-art performance on standard benchmarks, namely Street View Text and ICDAR 2003.},
author = {Wang, Tao and Wu, D. and Coates, A. and Ng, A.},
booktitle = {Pattern Recognition (ICPR), 2012 21st International Conference on},
file = {:Users/ben/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2012 - End-to-end text recognition with convolutional neural networks.pdf:pdf},
isbn = {1467322164},
issn = {1051-4651},
keywords = {Accuracy,Benchmark testing,Character recognition,Detectors,ICDAR 2003,Neural networks,Standards,Street View Text,Text recognition,character recognizer module,convolutional neural network,end-to-end text recognition,feature extraction,handwritten character recognition,lexicon driven recognition system,multilayer neural network,multilayer perceptrons,natural image processing,natural scenes,off-the-shelf method,scene text recognition system,text detection,unsupervised feature learning,unsupervised learning},
pages = {3304--3308},
title = {{End-to-end text recognition with convolutional neural networks}},
url = {http://www-cs.stanford.edu/people/ang/papers/ICPR12-TextRecognitionConvNeuralNets.pdf},
year = {2012}
}
@article{Zhang2000a,
abstract = { We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.},
author = {Zhang, Zhengyou},
doi = {10.1109/34.888718},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {2d pattern,Absolute conic,Calibration from planes,Camera calibration,Closed-form solution,Flexible plane-based calibration,Flexible setup,Lens distortion,Maximum likelihood estimation,Projective mapping},
number = {11},
pages = {1330--1334},
pmid = {131},
title = {{A flexible new technique for camera calibration}},
volume = {22},
year = {2000}
}
@article{Wager2013,
abstract = {Dropout and other feature noising schemes control overﬁtting by artiﬁcially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is ﬁrst-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and ﬁnd that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classiﬁcation tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.1493v2},
author = {Wager, Stefan and Wang, Sida and Liang, Percy},
eprint = {arXiv:1307.1493v2},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/drpOut.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information \ldots},
pages = {1--11},
title = {{Dropout Training as Adaptive Regularization}},
url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization},
year = {2013}
}
@misc{Seung1992,
abstract = {Learning from examples in feedforward neural networks is studied within$\backslash$na statistical-mechanical framework. Training is assumed to be stochastic,$\backslash$nleading to a Gibbs distribution of networks characterized by a temperature$\backslash$nparameter T. Learning of realizable rules as well as of unrealizable$\backslash$nrules is considered. In the latter case, the target rule cannot be$\backslash$nperfectly realized by a network of the given architecture. Two useful$\backslash$napproximate theories of learning from examples are studied: the high-temperature$\backslash$nlimit and the annealed approximation. Exact treatment of the quenched$\backslash$ndisorder generated by the random sampling of the examples leads to$\backslash$nthe use of the replica theory. Of primary interest is the generalization$\backslash$ncurve, namely, the average generalization error ?g versus the number$\backslash$nof examples P used for training. The theory implies that, for a reduction$\backslash$nin ?g that remains finite in the large-N limit, P should generally$\backslash$nscale as ?N, where N is the number of independently adjustable weights$\backslash$nin the network. We show that for smooth networks, i.e., those with$\backslash$ncontinuously varying weights and smooth transfer functions, the generalization$\backslash$ncurve asymptotically obeys an inverse power law. In contrast, for$\backslash$nnonsmooth networks other behaviors can appear, depending on the nature$\backslash$nof the nonlinearities as well as the realizability of the rule. In$\backslash$nparticular, a discontinuous learning transition from a state of poor$\backslash$nto a state of perfect generalization can occur in nonsmooth networks$\backslash$nlearning realizable rules. We illustrate both gradual and continuous$\backslash$nlearning with a detailed analytical and numerical study of several$\backslash$nsingle-layer perceptron models. Comparing with the exact replica$\backslash$ntheory of perceptron learning, we find that for realizable rules$\backslash$nthe high-temperature and annealed theories provide very good approximations$\backslash$nto the generalization performance. Assuming this to hold for multilayer$\backslash$nnetworks as well, we propose a classification of possible asymptotic$\backslash$nforms of learning curves in general realizable models. For unrealizable$\backslash$nrules we find that the above approximations fail in general to predict$\backslash$ncorrectly the shapes of the generalization curves. Another indication$\backslash$nof the important role of quenched disorder for unrealizable rules$\backslash$nis that the generalization error is not necessarily a monotonically$\backslash$nincreasing function of temperature. Also, unrealizable rules can$\backslash$npossess genuine spin-glass phases indicative of degenerate minima$\backslash$nseparated by high barriers.},
author = {Seung, H. and Sompolinsky, H. and Tishby, N.},
booktitle = {Physical Review A},
doi = {10.1103/PhysRevA.45.6056},
isbn = {1050-2947 (Print)$\backslash$r1050-2947 (Linking)},
issn = {1050-2947},
number = {8},
pages = {6056--6091},
pmid = {9907706},
title = {{Statistical Mechanics of Learning from Examples. Physical Review A}},
volume = {45},
year = {1992}
}
@misc{Dutta2012,
abstract = {Recording posture and movement is important for determining risk of musculoskeletal injury in the workplace, but existing motion capture systems are not suited for field work. Estimates of the 3-D relative positions of four 0.10. m cubes from the Kinect were compared to estimates from a Vicon motion capture system to determine whether the hardware sensing components were sensitive enough to be used as a portable 3-D motion capture system for workplace ergonomic assessments. The root-mean-squared errors (SD) were 0.0065. m (0.0048. m), 0.0109. m (0.0059. m), 0.0057. m (0.0042. m) in the x, y and z directions (with x axis to the right, y axis away from the sensor and z axis upwards). These data were collected over a range of 1.0-3.0. m from the device covering a field of view of 54.0 degrees horizontally and 39.1 degrees vertically. Requirements for software, hardware and subject preparation were also considered to determine the usability of the Kinect in the field. ?? 2011 Elsevier Ltd and The Ergonomics Society.},
author = {Dutta, Tilak},
booktitle = {Applied Ergonomics},
doi = {10.1016/j.apergo.2011.09.011},
file = {:Users/ben/Documents/csMSc/project/gait/research/Evaluation of the Kinect sensor for 3-D kinematic measurement in the workplace.html:html},
isbn = {1872-9126 (Electronic)$\backslash$n0003-6870 (Linking)},
issn = {00036870},
keywords = {Kinect,Kinematics,Portable motion capture},
pages = {645--649},
pmid = {22018839},
title = {{Evaluation of the Kinect??? sensor for 3-D kinematic measurement in the workplace}},
volume = {43},
year = {2012}
}
@article{Budiu2011,
abstract = {Abstract We present the parallelized implementation of decision forest training as used in Kinect to train the body parts classification system. We describe the practical details of dealing with large training sets and deep trees, and describe how to parallelize over ... $\backslash$n},
author = {Budiu, Mihai and Shotton, Jamie},
file = {:Users/ben/Documents/csMSc/project/gait/research/shottonParallel.pdf:pdf},
journal = {BigLearn Workshop at NIPS},
title = {{Parallelizing the training of the Kinect body parts labeling algorithm}},
url = {http://www.msr-waypoint.net/pubs/170877/top.pdf},
year = {2011}
}
@article{Polyak1964,
abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, \ldots, xn, \ldots, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, \ldots, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
author = {Polyak, B. T.},
doi = {10.1016/0041-5553(64)90137-5},
issn = {00415553},
journal = {USSR Computational Mathematics and Mathematical Physics},
month = jan,
number = {5},
pages = {1--17},
title = {{Some methods of speeding up the convergence of iteration methods}},
url = {http://www.sciencedirect.com/science/article/pii/0041555364901375},
volume = {4},
year = {1964}
}
@inproceedings{Zhu2008,
abstract = {This paper presents a model-based, Cartesian control theoretic approach for estimating human pose from features detected using depth images obtained from a time of flight imaging device. The features represent positions of anatomical landmarks, detected and tracked over time based on a probabilistic inferencing algorithm. The detected features are subsequently used as input to a constrained, closed loop tracking control algorithm which not only estimates the pose of the articulated human model, but also provides feedback to the feature detector in order to resolve ambiguities or to provide estimates of undetected features. Based on a simple kinematic model, constraints such as joint limit avoidance, and self penetration avoidance are enforced within the tracking control framework. We demonstrate the effectiveness of the algorithm with experimental results of upper body pose reconstruction from a small set of features. On average, the entire pipeline runs at approximately 10 frames per second on a standard 3 GHz PC using a 17 degree of freedom upper body human model.},
author = {Zhu, Youding and Dariush, Behzad and Fujimura, Kikuo},
booktitle = {2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops},
doi = {10.1109/CVPRW.2008.4563163},
isbn = {9781424423408},
issn = {2160-7508},
title = {{Controlled human pose estimation from depth image streams}},
year = {2008}
}
@inproceedings{Hu2013,
abstract = {The emergence of Microsoft Kinect has attracted the attention not only from consumers but also from researchers in the field of computer vision. It facilitates the possibility to capture the depth map of the scene in real time and with low cost. Nonetheless, due to the limitations of structured light measurements used by Kinect, the captured depth map suffers random depth missing in the occlusion or smooth regions, which affects the accuracy of many Kinect based applications. In order to fill in the holes existing in Kinect depth map, some approaches that adopted color image guided in-painting or joint bilateral filter have been proposed to represent the missing depth pixel by available depth pixels. However, they are not able to obtain the optimal weights, thus the obtained missing depth values are not best. In this paper, we propose a color image guided locality regularized representation (CGLRR) to reconstruct the missing depth pixels by comprehensively determining the optimal weights of the available depth pixels from collocated patches in color image. Experimental results demonstrate that the proposed algorithm can better fill in the holes of depth map both in smooth and edge region than previous works.},
author = {Hu, Jinhui and Hu, Ruimin and Wang, Zhongyuan and Gong, Yan and Duan, Mang},
booktitle = {IEEE VCIP 2013 - 2013 IEEE International Conference on Visual Communications and Image Processing},
doi = {10.1109/VCIP.2013.6706366},
isbn = {9781479902903},
keywords = {CGLRR,Depth map,Holes filling,Kinect},
title = {{Color image guided locality regularized representation for Kinect depth holes filling}},
year = {2013}
}
@inproceedings{Gupta2013,
abstract = {We address the problems of contour detection, bottom-up grouping and semantic segmentation using RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset [27]. We propose algorithms for object boundary detection and hierarchical segmentation that generalize the gPb-ucm approach of [2] by making effective use of depth information. We show that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We then turn to the problem of semantic segmentation and propose a simple approach that classifies super pixels into the 40 dominant object categories in NYUD2. We use both generic and class-specific features to encode the appearance and geometry of objects. We also show how our approach can be used for scene classification, and how this contextual information in turn improves object recognition. In all of these tasks, we report significant improvements over the state-of-the-art.},
author = {Gupta, Saurabh and Arbelaez, Pablo and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.79},
isbn = {1063-6919},
issn = {10636919},
keywords = {RGBD Recognition,RGBD Segmentation},
pages = {564--571},
pmid = {20530810},
title = {{Perceptual organization and recognition of indoor scenes from RGB-D images}},
year = {2013}
}
@techreport{Labs2003,
abstract = {This contribution presents an overview of the theoretical and practical aspects of the broad family of learning algorithms based on Stochastic Gradient Descent, including Perceptrons, Adalines, K-Means, LVQ, Multi-Layer Networks, and Graph Transformer Networks.},
author = {Labs, N E C and Way, Independence and Nj, Princeton},
booktitle = {Learning},
doi = {10.1007/978-3-540-28650-9\_7},
isbn = {978-3-540-23122-6},
issn = {00335533},
keywords = {stochastic gradient descent,stochastic learning},
pages = {22},
title = {{Stochastic Learning}},
year = {2003}
}
@article{Accv2014,
author = {Li, S. and Chan, Antoni B.},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network.pdf:pdf},
journal = {Asian Conference on Computer Vision (ACCV)},
title = {{3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network}},
year = {2014}
}
@article{Ye2013,
abstract = {Human pose estimation has been actively studied for decades. While traditional approaches rely on 2d data like images or videos, the development of Time-of-Flight cameras and other depth sensors created new opportunities to advance the field. We give an overview of recent approaches that perform human motion analysis which includes depth-based and skeleton-based activity recognition, head pose estimation, facial feature detection, facial performance capture, hand pose estimation and hand gesture recognition. While the focus is on approaches using depth data, we also discuss traditional image based methods to provide a broad overview of recent developments in these areas.},
author = {Ye, Mao and Zhang, Qing and Wang, Liang and Zhu, Jiejie and Yang, Ruigang and Gall, Juergen},
doi = {10.1007/978-3-642-44964-2\_8},
file = {:Users/ben/Documents/csMSc/project/gait/research/journalsFromOutline/attachments/A Survey on Human Motion Analysis from.pdf:pdf},
isbn = {9783642449635},
issn = {16113349},
journal = {Lecture Notes in Computer Science (Time-of-Flight and Depth Imaging. Sensors, Algorithms, and Applications)},
pages = {149--187},
title = {{A survey on human motion analysis from depth data}},
volume = {8200 LNCS},
year = {2013}
}
@phdthesis{Duffner2007,
abstract = {This paper analyze face alignment, gender recognition, face recognition, and achieve to a state-of-the-art accurancy.},
author = {Duffner, Stefan},
booktitle = {Thesis},
title = {{Face Image Analysis With Convolutional Neural Networks}},
year = {2007}
}
@article{Sapp2013,
abstract = {We propose a multimodal, decomposable model for articulated human pose estimation in monocular images. A typical approach to this problem is to use a linear structured model, which struggles to capture the wide range of appearance present in realistic, unconstrained images. In this paper, we instead propose a model of human pose that explicitly captures a variety of pose modes. Unlike other multimodal models, our approach includes both global and local pose cues and uses a convex objective and joint training for mode selection and pose estimation. We also employ a cascaded mode selection step which controls the trade-off between speed and accuracy, yielding a 5x speedup in inference and learning. Our model outperforms state-of-the-art approaches across the accuracy-speed trade-off curve for several pose datasets. This includes our newly-collected dataset of people in movies, FLIC, which contains an order of magnitude more labeled data for training and testing than existing datasets. View full abstract},
author = {Sapp, Ben and Taskar, Ben},
doi = {10.1109/CVPR.2013.471},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/datasets/Sapp2013FLIC.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
pages = {3674--3681},
title = {{MODEC: Multimodal decomposable models for human pose estimation}},
year = {2013}
}
@misc{Zhang2013a,
abstract = {This paper presents a novel method to extract skeletons of complex articulated objects from 3D point cloud sequences collected by the Kinect. Our approach is more robust than the traditional video-based and stereo-based approaches, as the Kinect directly provides 3D information without any markers, 2D-to-3D-transition assumptions, and feature point extraction. We track all the raw 3D points on the object, and utilize the point trajectories to determine the object skeleton. The point tracking is achieved by the 3D non-rigid matching based on the Markov Random Field (MRF) Deformation Model. To reduce the large computational cost of the non-rigid matching, a coarse-to-fine procedure is proposed. To the best of our knowledge, this is the first to extract skeletons of highly deformable objects from 3D point cloud sequences by point tracking. Experiments prove our method's good performance, and the extracted skeletons are successfully applied to the motion capture. © 2012 Elsevier B.V.},
author = {Zhang, Quanshi and Song, Xuan and Shao, Xiaowei and Shibasaki, Ryosuke and Zhao, Huijing},
booktitle = {Neurocomputing},
doi = {10.1016/j.neucom.2011.11.032},
file = {:Users/ben/Documents/csMSc/project/gait/research/Unsupervised skeleton extraction and motion capture from 3D deformable matching.html:html},
issn = {09252312},
keywords = {3D point cloud sequence,Skeleton extraction},
pages = {170--182},
title = {{Unsupervised skeleton extraction and motion capture from 3D deformable matching}},
volume = {100},
year = {2013}
}
@article{Sermanet2013b,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:Users/ben/Library/Application Support/Mendeley Desktop/Downloaded/Sermanet et al. - 2013 - OverFeat Integrated Recognition , Localization and Detection using Convolutional Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6229},
title = {{OverFeat: Integrated Recognition , Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@inproceedings{Fiedler2013,
abstract = {Several approaches to calibration of the Kinect as a range sensor have been presented in the past. Those approaches do not take into account a possible influence of thermal and environmental conditions. This paper shows that variations of the temperature and air draft have a notable influence on Kinect’s images and range measurements. Based on these findings, practical rules are stated to reduce calibration and measurement errors caused by thermal conditions.},
author = {Fiedler, David and M\"{u}ller, Heinrich},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-642-40303-3\_3},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/Impact of Thermal and Environmental Conditions on the Kinect Sensor.pdf:pdf},
isbn = {9783642403026},
issn = {03029743},
keywords = {Calibration,Kinect Sensor,Thermal Influence},
pages = {21--31},
title = {{Impact of thermal and environmental conditions on the kinect sensor}},
volume = {7854},
year = {2013}
}
@article{Bengio2013,
abstract = {After a more than decade-long period of relatively little research ac- tivity in the area of recurrent neural networks, several new develop- ments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more effi- cient training of recurrent networks. These advances have been mo- tivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms ofmodeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gra- dients to help symmetry breaking and credit assignment. The ex- periments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.0901v2},
author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
doi = {10.1109/ICASSP.2013.6639349},
eprint = {arXiv:1212.0901v2},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/training RNN bengio.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Recurrent networks,deep learning,long-term dependencies,representation learning},
pages = {8624--8628},
title = {{Advances in optimizing recurrent networks}},
year = {2013}
}
@article{Tompson2014,
abstract = {This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {1406.2984},
author = {Tompson, Jonathan and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
eprint = {1406.2984},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Tompson2014Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation.pdf:pdf},
journal = {arXiv preprint arXiv:1406.2984},
title = {{Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation}},
url = {http://arxiv.org/abs/1406.2984},
year = {2014}
}
@article{Springenberg2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6806v3},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {arXiv:1412.6806v3},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Springenberg2015.pdf:pdf},
isbn = {9781600066634},
journal = {ICLR},
title = {{Striving for Simplicity: The All Convolutional Net}},
year = {2015}
}
@article{Girshick2011,
abstract = {We present a new approach to general-activity human pose estimation from depth images, building on Hough forests. We extend existing techniques in several ways: real time prediction of multiple 3D joints, explicit learning of voting weights, vote compression to allow larger training sets, and a comparison of several decision-tree training objectives. Key aspects of our work include: regression directly from the raw depth image, without the use of an arbitrary intermediate representation; applicability to general motions (not constrained to particular activities) and the ability to localize occluded as well as visible body joints. Experimental results demonstrate that our method produces state of the art results on several data sets including the challenging MSRC-5000 pose estimation test set, at a speed of about 200 frames per second. Results on silhouettes suggest broader applicability to other imaging modalities.},
author = {Girshick, Ross and Shotton, Jamie and Kohli, Pushmeet and Criminisi, Antonio and Fitzgibbon, Andrew},
doi = {10.1109/ICCV.2011.6126270},
file = {:Users/ben/Documents/csMSc/project/gait/research/Efficient Regression of General-Activity Human Poses from Depth Images.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {415--422},
title = {{Efficient regression of general-activity human poses from depth images}},
year = {2011}
}
@article{Ramakrishna,
author = {Ramakrishna, Varun and Munoz, Daniel and Hebert, Martial and Bagnell, J Andrew and Sheikh, Yaser},
doi = {10.1007/978-3-319-10605-2\_3},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Pose Machines- Articulated Pose Estimation via Inference Machines.pdf:pdf},
isbn = {978-3-319-10604-5},
pages = {1--15},
title = {{Pose Machines : Articulated Pose Estimation via Inference Machines}}
}
@inproceedings{Glorot2010a,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
doi = {10.1.1.207.2059},
issn = {15324435},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2010\_GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Sermanet2013,
abstract = {Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolu...},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.0142v2},
author = {Sermanet, Pierre and Kavukcuoglu, Koray and Chintala, Soumith and Lecun, Yann},
doi = {10.1109/CVPR.2013.465},
eprint = {arXiv:1212.0142v2},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Pedestrian Detection with Unsupervised Multi-Stage Feature Learning.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {computer vision,convolutional,deep learning,detection,pedestrian,unsupervised},
pages = {3626--3633},
title = {{Pedestrian detection with unsupervised multi-stage feature learning}},
year = {2013}
}
@inproceedings{Oquab2014,
abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large- scale visual recognition challenge (ILSVRC2012). The suc- cess of CNNs is attributed to their ability to learn rich mid- level image representations as opposed to hand-designed low-level features used in other image classification meth- ods. Learning CNNs, however, amounts to estimating mil- lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi- ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep- resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
booktitle = {CVPR},
doi = {10.1109/CVPR.2014.222},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/transferLearning/Oquab2014.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {1717--1724},
title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
year = {2014}
}
@inproceedings{Maimone2012,
abstract = {We present a method for reducing interference between multiple structured light-based depth sensors operating in the same spectrum with rigidly attached projectors and cameras. A small amount of motion is applied to a subset of the sensors so that each unit sees its own projected pattern sharply, but sees a blurred version of the patterns of other units. If high spacial frequency patterns are used, each sensor sees its own pattern with higher contrast than the patterns of other units, resulting in simplified pattern disambiguation. An analysis of this method is presented for a group of commodity Microsoft Kinect color-plus-depth sensors with overlapping views. We demonstrate that applying a small vibration with a simple motor to a subset of the Kinect sensors results in reduced interference, as manifested as holes and noise in the depth maps. Using an ar- ray of six Kinects, our system reduced interference-related missing data from from 16.6\% to 1.4\% of the total pixels. Another experiment with three Kinects showed an 82.2\% percent reduction in the measurement error introduced by interference. A side-effect is blurring in the color images of the moving units, which is mitigated with post-processing. We believe our technique will allow inexpensive commodity depth sensors to form the basis of dense large-scale capture systems.},
author = {Maimone, Andrew and Fuchs, Henry},
booktitle = {Proceedings - IEEE Virtual Reality},
doi = {10.1109/VR.2012.6180879},
isbn = {9781467312462},
issn = {1087-8270},
keywords = {input devices,motion,sharpening and deblurring},
pages = {51--54},
title = {{Reducing interference between multiple structured light depth sensors using motion}},
year = {2012}
}
@article{Howard2013,
abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techniques include adding more image transformations to the training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55\% using no external data which is over a 20\% relative improvement on the previous year’s winner.},
archivePrefix = {arXiv},
arxivId = {1312.5402},
author = {Howard, Ag},
eprint = {1312.5402},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Howard2013DataAugmentation.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5402},
title = {{Some Improvements on Deep Convolutional Neural Network Based Image Classification}},
url = {http://arxiv.org/abs/1312.5402},
year = {2013}
}
@article{Chen2013b,
abstract = {Considering the existing depth recovery approaches that have different limitations when applying to Kinect depth data, in this paper, we propose to integrate their effective features including adaptive support region selection, reliable depth selection and color guidance together under a unified framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole-filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term. The fidelity term takes into account the characteristics of Kinect data. The regularization term is designed to incorporate the joint bilateral filtering (JBF) kernel and the joint trilateral filtering (JTF) kernel so as to facilitate both depth hole-filling and denoising. Moreover, the JBF kernel is modified to incorporate the structure information. Both simulations on the benchmark Middlebury dataset and experiments on real Kinect data show that our proposed method achieves state-of-the-art performance in terms of recovery accuracy and visual quality.},
author = {Chen, Chongyu and Cai, Jianfei and Zheng, Jianmin and Cham, Tat Jen and Shi, Guangming},
doi = {10.1109/MMSP.2013.6659255},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/kinect holes/Kinect Depth Recovery Using a Color-guided, Region-adaptive, and Depth-selective Framework.pdf:pdf},
isbn = {9781479901258},
journal = {2013 IEEE International Workshop on Multimedia Signal Processing, MMSP 2013},
number = {212},
pages = {7--12},
title = {{A color-guided, region-adaptive and depth-selective unified framework for Kinect depth recovery}},
volume = {V},
year = {2013}
}
@article{Slama2014,
annote = {4d},
author = {Slama, Rim and Wannous, Hazem and Daoudi, Mohamed},
file = {:Users/ben/Documents/csMSc/project/gait/research/Grassmannian Representation of Motion Depth for 3D Human Gesture and Action Recognition.pdf:pdf},
title = {{Grassmannian Representation of Motion Depth for 3D Human Gesture and Action Recognition}},
year = {2014}
}
@inproceedings{LeCun2010,
abstract = {Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or "features")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.},
author = {LeCun, Yann and Kavukcuoglu, Koray and Farabet, Cl\'{e}ment},
booktitle = {ISCAS 2010 - 2010 IEEE International Symposium on Circuits and Systems: Nano-Bio Circuit Fabrics and Systems},
doi = {10.1109/ISCAS.2010.5537907},
isbn = {9781424453085},
issn = {02714302},
pages = {253--256},
title = {{Convolutional networks and applications in vision}},
year = {2010}
}
@article{Dahan2012,
abstract = {As depth cameras become more popular, pixel depth information becomes easier to obtain. This information can clearly enhance many image processing applications. However, combining depth and color information is not straightforward as these two signals can have different noise characteristics, differences in resolution, and their boundaries do not generally agree. We present a technique that combines depth and color image information from real devices in synergy. In particular, we focus on combining them to improve image segmentation. We use color information to fill and clean depth and use depth to enhance color image segmentation. We demonstrate the utility of the combined segmentation for extracting layers and present a novel image retargeting algorithm for layered images.},
author = {Dahan, Meir Johnathan and Chen, Nir and Shamir, Ariel and Cohen-Or, Daniel},
doi = {10.1007/s00371-011-0667-7},
issn = {01782789},
journal = {Visual Computer},
keywords = {Depth maps,Image segmentation,Retargeting},
number = {12},
pages = {1181--1193},
title = {{Combining color and depth for enhanced image segmentation and retargeting}},
volume = {28},
year = {2012}
}
@article{Jain2013,
abstract = {This paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modified learning technique that learns low-level features and higher-level weak spatial models. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows significant improvement over the current state-of-the-art results. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to outperform all existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on features that might even just cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent then expected. Many researchers previously argued that the kinematic structure and top-down information is crucial for this domain, but with our purely bottom up, and weak spatial model, we could improve other more complicated architectures that currently produce the best results. This mirrors what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.7302v3},
author = {Jain, Arjun and Tompson, Jonathan and Andriluka, Mykhaylo and Taylor, Graham W. and Bregler, Christoph},
eprint = {arXiv:1312.7302v3},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Jain2014.pdf:pdf},
journal = {arXiv preprint arXiv:1312.7302},
title = {{Learning Human Pose Estimation Features with Convolutional Networks}},
url = {http://arxiv.org/abs/1312.7302},
year = {2013}
}
@article{Kingma2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/adam.pdf:pdf},
journal = {arXiv preprint arXiv:1412.6980},
title = {{AdaM: A Method for Stochastic Optimization}},
year = {2015}
}
@article{Khoshelham2012c,
abstract = {Consumer-grade range cameras such as the Kinect sensor have the potential to be used in mapping applications where accuracy requirements are less strict. To realize this potential insight into the geometric quality of the data acquired by the sensor is essential. In this paper we discuss the calibration of the Kinect sensor, and provide an analysis of the accuracy and resolution of its depth data. Based on a mathematical model of depth measurement from disparity a theoretical error analysis is presented, which provides an insight into the factors influencing the accuracy of the data. Experimental results show that the random error of depth measurement increases with increasing distance to the sensor, and ranges from a few millimeters up to about 4 cm at the maximum range of the sensor. The quality of the data is also found to be influenced by the low resolution of the depth measurements.},
annote = {Khoshelham et al. [13] provide an insight into the geometric quality of Kinect depth data based on analyzing the accuracy and resolution of the depth signal. Experimental results show that the random error of depth measurement increases when the distance between the scene and the sensor increases, ranging from a few millimeters at close range to about 4 cm at the maximum range of the sensor.},
author = {Khoshelham, Kourosh and Elberink, Sander Oude},
doi = {10.3390/s120201437},
file = {:Users/ben/Library/Application Support/Mendeley Desktop/Downloaded/Khoshelham, Elberink - 2012 - Accuracy and resolution of kinect depth data for indoor mapping applications.pdf:pdf},
isbn = {1424-8220},
issn = {14248220},
journal = {Sensors},
keywords = {Calibration,Error budget,Imaging,Laser scanning,Point cloud,RGB-D,Range camera,Sensor,Triangulation},
pages = {1437--1454},
pmid = {22438718},
title = {{Accuracy and resolution of kinect depth data for indoor mapping applications}},
volume = {12},
year = {2012}
}
@article{Simonyan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Simonyan2015.pdf:pdf},
journal = {ICLR},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recoginition}},
year = {2015}
}
@article{Obdrzalek2012,
abstract = {The Microsoft Kinect camera is becoming increasingly popular in many areas aside from entertainment, including human activity monitoring and rehabilitation. Many people, however, fail to consider the reliability and accuracy of the Kinect human pose estimation when they depend on it as a measuring system. In this paper we compare the Kinect pose estimation (skeletonization) with more established techniques for pose estimation from motion capture data, examining the accuracy of joint localization and robustness of pose estimation with respect to the orientation and occlusions. We have evaluated six physical exercises aimed at coaching of elderly population. Experimental results present pose estimation accuracy rates and corresponding error bounds for the Kinect system.},
annote = {They record using Kinect microsoft SDK and marker based skeletonizers subject performing a number of typical phyisio training exercises at 0 30 60 90 degrees to camera.},
author = {Obdrzalek, Stepan and Kurillo, Gregorij and Ofli, Ferda and Bajcsy, Ruzena and Seto, Edmund and Jimison, Holly and Pavel, Michael},
doi = {10.1109/EMBC.2012.6346149},
file = {:Users/ben/Documents/csMSc/project/gait/research/Accuracy and Robustness of Kinect Pose Estimation in the Context of Coaching of Elderly Population.pdf:pdf},
isbn = {9781424441198},
issn = {1557170X},
journal = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
pages = {1188--1193},
pmid = {23366110},
title = {{Accuracy and robustness of Kinect pose estimation in the context of coaching of elderly population}},
year = {2012}
}
@article{Orr1997,
author = {Orr, Genevieve B.},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/1257-removing-noise-in-on-line-search-using-adaptive-batch-sizes.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 9},
pages = {232--238},
title = {{Removing noise in on-line search using adaptive batch sizes}},
url = {http://books.google.com/books?hl=en\&lr=\&id=QpD7n95ozWUC\&oi=fnd\&pg=PA232\&dq=Removing+Noise+in+On-Line+Search+using+Adaptive+Batch+Sizes\&ots=iBoojKXQdv\&sig=vQ-l6Fwf\_CwHeE08LjB4RZBXpBs},
volume = {1},
year = {1997}
}
@article{Gupta2014,
abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3\%, which is a 56\% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24\% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {Gupta, Saurabh and Girshick, Ross and Arbelaez, Pablo and Malik, Jitendra},
doi = {10.1007/978-3-319-10584-0\_23},
eprint = {arXiv:1407.5736v1},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Learning Rich Features from RGB-D Images for Object Detection and Segmentation.pdf:pdf},
isbn = {978-3-319-10583-3},
journal = {arXiv preprint arXiv:1407.5736},
keywords = {object detection,object segmentation,rgb-d perception},
title = {{Learning Rich Features from RGB-D Images for Object Detection and Segmentation}},
url = {http://arxiv.org/abs/1407.5736},
year = {2014}
}
@article{Dinh2014,
author = {Dinh, Dong-Luong and Lim, Myeong-Jun and Thang, Nguyen Duc and Lee, Sungyoung and Kim, Tae-Seong},
doi = {10.1007/s10489-014-0535-z},
file = {:Users/ben/Documents/csMSc/project/gait/research/Real-time 3D Human Pose Recovery from a Single Depth Image Using Principal Direction Analysis.pdf:pdf},
issn = {0924-669X},
journal = {Applied Intelligence},
keywords = {3d human pose recovery,body,depth image,part recognition,principal direction analysis},
title = {{Real-time 3D human pose recovery from a single depth image using principal direction analysis}},
url = {http://link.springer.com/10.1007/s10489-014-0535-z},
year = {2014}
}
@article{Kar2010,
abstract = {In this work, we attempt to tackle the problem of skeletal tracking of a human body using the Microsoft Kinect sensor. We use cues from the RGB and depth streams from the sensor to fit a stick skeleton model to the human upper body. A variety of Computer Vision techniques are used with a bottom up approach to estimate the candidate head and upper body postitions using haar-cascade detectorsa and hand positions using skin segmentation data. The data is finally integrated with the Extended Distance Transform skeletonisation algorithm to obtain a fairly accurate estimate of the the skeleton parameters. The results presented show that this method can be extended to perform in real time.},
author = {Kar, Abhishek},
journal = {Methodology},
pages = {1--11},
title = {{Skeletal Tracking using Microsoft Kinect}},
url = {http://www.mendeley.com/research/skeletal-tracking-using-microsoft-kinect/},
year = {2010}
}
@inproceedings{Shen2013,
abstract = {The recent popularity of structured-light depth sensors has enabled $\backslash$nmany new applications from gesture-based user interface to 3D reconstructions. $\backslash$nThe quality of the depth measurements of these systems, however, is far from $\backslash$nperfect. Some depth values can have significant errors, while others can be $\backslash$nmissing altogether. The uncertainty in depth measurements among these sensors $\backslash$ncan significantly degrade the performance of any subsequent vision processing. $\backslash$nIn this paper, we propose a novel probabilistic model to capture various types $\backslash$nof uncertainties in the depth measurement process among structured-light $\backslash$nsystems. The key to our model is the use of depth layers to account for the $\backslash$ndifferences between foreground objects and background scene, the missing depth $\backslash$nvalue phenomenon, and the correlation between color and depth channels. The $\backslash$ndepth layer labeling is solved as a maximum a-posteriori estimation problem, and $\backslash$na Markov Random Field attuned to the uncertainty in measurements is used to $\backslash$nspatially smooth the labeling process. Using the depth-layer labels, we propose $\backslash$na depth correction and completion algorithm that outperforms other techniques in $\backslash$nthe literature.},
author = {Shen, Ju and Cheung, Sen Ching S},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.157},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/kinect holes/Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {Depth image denoising,Kinect,RGB-D sensor,depth completion},
pages = {1187--1194},
title = {{Layer depth denoising and completion for structured-light RGB-D cameras}},
year = {2013}
}
@article{Cho2008,
annote = {Deals with loss of signal in hair},
author = {Cho, Jh and Kim, Sy and Ho, Ys and Lee, K},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/kinect holes/Dynamic 3D Human Actor Generation Method using a Time-of-Flight Depth Camera .pdf:pdf},
journal = {Consumer Electronics, IEEE \ldots},
number = {4},
pages = {1514--1521},
title = {{Dynamic 3D human actor generation method using a time-of-flight depth camera}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4711195},
volume = {54},
year = {2008}
}
@article{Sutskever2013,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6639346},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/sutskever13.pdf:pdf},
isbn = {978-1-4799-0356-6},
journal = {Jmlr W\&Cp},
number = {2010},
pages = {1139--1147},
title = {{On the importance of initialization and momentum in deep learning}},
volume = {28},
year = {2013}
}
@article{Evangelidis2014,
author = {Evangelidis, Georgios and Singh, Gurkirt and Horaud, Radu},
doi = {10.1109/ICPR.2014.772},
file = {:Users/ben/Documents/csMSc/project/gait/research/Skeletal Quads- Human Action Recognition Using Joint Quadruples.pdf:pdf},
isbn = {9781479952083},
issn = {10514651},
journal = {Pattern Recognition (ICPR), 2014 22nd International Conference on},
pages = {4513 -- 4518},
title = {{Skeletal Quads : Human Action Recognition Using Joint Quadruples}},
year = {2014}
}
@inproceedings{Sohn2011,
abstract = {Informative image representations are important in achieving state-of-the-art performance in object recognition tasks. Among feature learning algorithms that are used to develop image representations, restricted Boltzmann machines (RBMs) have good expressive power and build effective representations. However, the difficulty of training RBMs has been a barrier to their wide use. To address this difficulty, we show the connections between mixture models and RBMs and present an efficient training method for RBMs that utilize these connections. To the best of our knowledge, this is the first work showing that RBMs can be trained with almost no hyperparameter tuning to provide classification performance similar to or significantly better than mixture models (e.g., Gaussian mixture models). Along with this efficient training, we evaluate the importance of convolutional training that can capture a larger spatial context with less redundancy, as compared to non-convolutional training. Overall, our method achieves state-of-the-art performance on both Caltech 101 / 256 datasets using a single type of feature.},
author = {Sohn, Kihyuk and Jung, Dae Yon and Lee, Honglak and Hero, Alfred O.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126554},
isbn = {9781457711015},
issn = {1550-5499},
pages = {2643--2650},
title = {{Efficient learning of sparse, distributed, convolutional feature representations for object recognition}},
year = {2011}
}
@article{Szegedy2013,
author = {Szegedy, Christian},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Szegedy2014.pdf:pdf},
journal = {Nips 2013},
pages = {1--9},
title = {{Deep Neural Networks for Object Detection}},
year = {2013}
}
@book{Mitchell1997,
abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning.},
author = {Mitchell, Tom M},
booktitle = {Annual Review Of Computer Science},
doi = {10.1145/242224.242229},
isbn = {0070428077},
issn = {87567016},
number = {1},
pages = {417--433},
pmid = {20236947},
title = {{Machine Learning. Annual Review Of Computer Science}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0070428077},
volume = {4},
year = {1997}
}
@misc{Chattopadhyay2014,
abstract = {We explore the applicability of Kinect RGB-D streams in recognizing gait patterns of individuals. Gait energy volume (GEV) is a recently proposed feature that performs gait recognition in frontal view using only depth image frames from Kinect. Since depth frames from Kinect are inherently noisy, corresponding silhouette shapes are inaccurate, often merging with the background. We register the depth and RGB frames from Kinect to obtain smooth silhouette shape along with depth information. A partial volume reconstruction of the frontal surface of each silhouette is done and a novel feature termed as Pose Depth Volume (PDV) is derived from this volumetric model. Recognition performance of the proposed approach has been tested on a data set captured using Microsoft Kinect in an indoor environment. Experimental results clearly demonstrate the effectiveness of the approach in comparison with other existing methods. © 2013 Elsevier Inc. All rights reserved.},
author = {Chattopadhyay, Pratik and Roy, Aditi and Sural, Shamik and Mukhopadhyay, Jayanta},
booktitle = {Journal of Visual Communication and Image Representation},
doi = {10.1016/j.jvcir.2013.02.010},
file = {:Users/ben/Documents/csMSc/project/gait/research/Pose Depth Volume extraction from RGB-D streams for frontal gait recognition.html:html},
issn = {10473203},
keywords = {Depth registered silhouette,Frontal gait recognition,Key pose,Microsoft Kinect,Pose Depth Volume,RGB-D stream,Silhouette,Voxel volume},
pages = {53--63},
title = {{Pose depth Volume extraction from RGB-D streams for frontal gait recognition}},
volume = {25},
year = {2014}
}
@article{Hu2013a,
abstract = {The emergence of Microsoft Kinect has attracted the attention not only from consumers but also from researchers in the field of computer vision. It facilitates the possibility to capture the depth map of the scene in real time and with low cost. Nonetheless, due to the limitations of structured light measurements used by Kinect, the captured depth map suffers random depth missing in the occlusion or smooth regions, which affects the accuracy of many Kinect based applications. In order to fill in the holes existing in Kinect depth map, some approaches that adopted color image guided in-painting or joint bilateral filter have been proposed to represent the missing depth pixel by available depth pixels. However, they are not able to obtain the optimal weights, thus the obtained missing depth values are not best. In this paper, we propose a color image guided locality regularized representation (CGLRR) to reconstruct the missing depth pixels by comprehensively determining the optimal weights of the available depth pixels from collocated patches in color image. Experimental results demonstrate that the proposed algorithm can better fill in the holes of depth map both in smooth and edge region than previous works.},
annote = {good review of methods, uses color images},
author = {Hu, Jinhui and Hu, Ruimin and Wang, Zhongyuan and Gong, Yan and Duan, Mang},
doi = {10.1109/VCIP.2013.6706366},
file = {:Users/ben/Downloads/COLOR IMAGE GUIDED LOCALITY REGULARIZED REPRESENTATION FOR KINECT.pdf:pdf},
isbn = {9781479902903},
journal = {IEEE VCIP 2013 - 2013 IEEE International Conference on Visual Communications and Image Processing},
keywords = {CGLRR,Depth map,Holes filling,Kinect},
title = {{Color image guided locality regularized representation for Kinect depth holes filling}},
year = {2013}
}
@article{Finnoff1993,
author = {Finnoff, William and Hergert, Ferdinand and Zimmermann, Hans-Georg},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/finoffEARLY STPING.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 5},
pages = {228--235},
title = {{Extended Regularization Methods for Nonconvergent Model Selection}},
url = {http://papers.nips.cc/paper/643-extended-regularization-methods-for-nonconvergent-model-selection.pdf$\backslash$nfiles/2795/Finnoff et al. - 1993 - Extended Regularization Methods for Nonconvergent .pdf$\backslash$nfiles/2796/643-extended-regularization-methods-for-nonconverg},
year = {1993}
}
@article{Kiela2013,
author = {Kiela, Douwe},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/transferLearning/Kiela2014.pdf:pdf},
title = {{Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics}},
year = {2013}
}
@article{Khoshelham2012a,
abstract = {This paper presents an investigation of the geometric quality of depth data obtained by the Kinect sensor. Based on the mathematical model of depth measurement by the sensor a theoretical error analysis is presented, which provides an insight into the factors influencing the accuracy of the data. Experimental results show that the random error of depth measurement increases with increasing distance to the sensor, and ranges from a few millimetres up to about 4 cm at the maximum range of the sensor. The accuracy of the data is also found to be influenced by the low resolution of the depth measurements.},
author = {Khoshelham, K.},
doi = {10.5194/isprsarchives-XXXVIII-5-W12-133-2011},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/ACCURACY ANALYSIS OF KINECT DEPTH DATA .pdf:pdf},
issn = {1682-1777},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {accuracy,calibration,error,indoor mapping,laser scanning,point cloud,range camera,range imaging,rgb-d},
number = {5},
pages = {133--138},
title = {{Accuracy Analysis of Kinect Depth Data}},
volume = {XXXVIII},
year = {2012}
}
@article{Stommel2014,
abstract = {Low-cost game controllers such as Microsoft’s Kinect sensor provide dense, real-time depth measurements of indoor environments at high framerate. The sensor is based on the principle of active stereo using structured infrared light. For surfaces that distract infrared light, no measurements can be obtained. It is important to replace missing values early on, to avoid a slow subsequent conditional evaluations or the propagation of errors into neighboring regions. To solve this problem we present an inpainting method that adds missing values based on background estimates of the unoccluded scene. It is therefore not necessary to hypothesize missing regions based on similarity to other image regions. The procedure also avoids a blurring between foreground and background. By adapting the method to the specific properties of the Kinect (and comparable) cameras, we were able to keep the complexity of the algorithm low, so high speed can be achieved.},
author = {Stommel, Martin and Beetz, Michael and Xu, Weiliang},
doi = {10.1109/JSEN.2013.2291315},
file = {:Users/ben/Downloads/Inpainting of Missing Values in the Kinect Sensor’s.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sensors Journal},
keywords = {Kinect,background subtraction,depth map,missing values,real-time,stereo},
number = {4},
pages = {1107--1116},
title = {{Inpainting of missing values in the kinect sensor's depth maps based on background estimates}},
volume = {14},
year = {2014}
}
@inproceedings{Zinkevich2010,
abstract = {With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the ﬁrst parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime.},
author = {Zinkevich, Martin and Weimer, Markus and Smola, Alexander J. and Li, Lihong},
booktitle = {Advances in Neural Information Processing Systems 23},
pages = {2595--2603},
title = {{Parallelized Stochastic Gradient Descent}},
year = {2010}
}
@article{Berger2011,
abstract = {With the advent of theMicrosoft Kinect, renewed focus has been put on monocular depth-based motion capturing. However, this approach is limited in that an actor has to move facing the camera. Due to the active light nature of the sensor, no more than one device has been used for motion capturing so far. In effect, any pose estimation must fail for poses occluded to the depth camera. Our work investigates on reducing or mitigating the detrimental effects of multiple active light emitters, thereby allowing motion capture from all angles. We systematically evaluate the concurrent use of one to four Kinects, including calibration, error measures and analysis, and present a time-multiplexing approach.},
author = {Berger, Kai and Ruhl, Kai and Schroeder, Yannic and Bruemmer, Christian and Scholz, Alexander and Magnor, Marcus},
doi = {10.2312/PE/VMV/VMV11/317-324},
file = {:Users/ben/Documents/csMSc/project/gait/research/Markerless Motion Capture using multiple Color-Depth Sensors.pdf:pdf},
isbn = {978-3-905673-85-2},
journal = {Sensors Peterborough NH},
pages = {317--324},
title = {{Markerless Motion Capture using multiple Color-Depth Sensors}},
url = {http://graphics.tu-bs.de/media/publications/multikinectsMocap.pdf},
volume = {2011},
year = {2011}
}
@article{Livingston2012,
abstract = {The Microsoft Kinect for Xbox 360 provides a convenient and inexpensive depth sensor and, with the Microsoft software development kit, a skeleton tracker (Figure 2). These have great potential to be useful as virtual environment (VE) control interfaces for avatars or for viewpoint control. In order to determine its suitability for our applications, we devised and conducted tests to measure standard performance specifications for tracking systems. We evaluated the noise, accuracy, resolution, and latency of the skeleton tracking software. We also measured the range in which the person being tracked must be in order to achieve these values.},
author = {Livingston, Mark a. and Sebastian, Jay and Ai, Zhuming and Decker, Jonathan W.},
doi = {10.1109/VR.2012.6180911},
isbn = {978-1-4673-1246-2},
issn = {1087-8270},
journal = {2012 IEEE Virtual Reality (VR)},
pages = {119--120},
title = {{Performance measurements for the Microsoft Kinect skeleton}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6180911},
year = {2012}
}
@inproceedings{Smisek2011,
abstract = {We analyze Kinect as a 3D measuring device, experimentally investigate depth measurement resolution and error properties and make a quantitative comparison of Kinect accuracy with stereo reconstruction from SLR cameras and a 3D-TOF camera. We propose Kinect geometrical model and its calibration procedure providing an accurate calibration of Kinect 3D measurement and Kinect cameras. We demonstrate the functionality of Kinect calibration by integrating it into an SfM pipeline where 3D measurements from a moving Kinect are transformed into a common coordinate system by computing relative poses from matches in color camera.},
author = {Smisek, Jan and Jancosek, Michal and Pajdla, Tomas},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCVW.2011.6130380},
isbn = {9781467300629},
issn = {978-1-4673-0063-6},
pages = {1154--1160},
title = {{3D with Kinect}},
year = {2011}
}
@article{Fothergill2012,
abstract = {Entertainment and gaming systems such as the Wii and XBox Kinect have brought touchless, body-movement based interfaces to the masses. Systems like these enable the estimation of movements of various body parts from raw inertial motion or depth sensor data. However, the interface developer is still left with the challenging task of creating a system that recognizes these movements as embodying meaning. The machine learning approach for tackling this problem requires the collection of data sets that contain the relevant body movements and their associated semantic labels. These data sets directly impact the accuracy and performance of the gesture recognition system and should ideally contain all natural variations of the movements associated with a gesture. This paper addresses the problem of collecting such gesture datasets. In particular, we investigate the question of what is the most appropriate semiotic modality of instructions for conveying to human subjects the movements the system developer needs them to perform. The results of our qualitative and quantitative analysis indicate that the choice of modality has a significant impact on the performance of the learnt gesture recognition system; particularly in terms of correctness and coverage.},
author = {Fothergill, Simon and Mentis, Helena and Kohli, Pushmeet and Nowozin, Sebastian},
doi = {10.1145/2207676.2208303},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/datasets/fothergill2011.pdf:pdf},
isbn = {9781450310154},
issn = {145031015X},
journal = {Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems},
pages = {1737--1746},
title = {{Instructing people for training gestural interactive systems}},
url = {http://dl.acm.org/citation.cfm?doid=2207676.2208303},
year = {2012}
}
@inproceedings{Chen2013a,
abstract = {Considering the existing depth recovery approaches that have different limitations when applying to Kinect depth data, in this paper, we propose to integrate their effective features including adaptive support region selection, reliable depth selection and color guidance together under a unified framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole-filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term. The fidelity term takes into account the characteristics of Kinect data. The regularization term is designed to incorporate the joint bilateral filtering (JBF) kernel and the joint trilateral filtering (JTF) kernel so as to facilitate both depth hole-filling and denoising. Moreover, the JBF kernel is modified to incorporate the structure information. Both simulations on the benchmark Middlebury dataset and experiments on real Kinect data show that our proposed method achieves state-of-the-art performance in terms of recovery accuracy and visual quality.},
author = {Chen, Chongyu and Cai, Jianfei and Zheng, Jianmin and Cham, Tat Jen and Shi, Guangming},
booktitle = {2013 IEEE International Workshop on Multimedia Signal Processing, MMSP 2013},
doi = {10.1109/MMSP.2013.6659255},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/kinect holes/Kinect Depth Recovery Using a Color-guided, Region-adaptive, and Depth-selective Framework.pdf:pdf},
isbn = {9781479901258},
pages = {7--12},
title = {{A color-guided, region-adaptive and depth-selective unified framework for Kinect depth recovery}},
year = {2013}
}
@article{Wilson2003,
abstract = {Gradient descent training of neural networks can be done in either a batch or on-line manner. A widely held myth in the neural network community is that batch training is as fast or faster and/or more 'correct' than on-line training because it supposedly uses a better approximation of the true gradient for its weight updates. This paper explains why batch training is almost always slower than on-line training - often orders of magnitude slower - especially on large training sets. The main reason is due to the ability of on-line training to follow curves in the error surface throughout each epoch, which allows it to safely use a larger learning rate and thus converge with less iterations through the training data. Empirical results on a large (20,000-instance) speech recognition task and on 26 other learning tasks demonstrate that convergence can be reached significantly faster using on-line training than batch training, with no apparent difference in accuracy. © 2003 Elsevier Ltd. All rights reserved.},
author = {Wilson, D. Randall and Martinez, Tony R.},
doi = {10.1016/S0893-6080(03)00138-2},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Wilson.nn03.batch.pdf:pdf},
isbn = {0893-6080 (Print)$\backslash$r0893-6080 (Linking)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Backpropagation,Batch training,Generalization,Gradient descent,Learning rate,On-line training,Optimization,Stochastic approximation},
number = {10},
pages = {1429--1451},
pmid = {14622875},
title = {{The general inefficiency of batch training for gradient descent learning}},
volume = {16},
year = {2003}
}
@article{Chen2011,
abstract = {In this paper we propose a new examplar-based approach to recover 3D human poses from monocular images. Given the visual feature of each frame, pose retrieval is first conducted in the examplar database to find relevant pose candidates. Then, dynamic programming is applied on the pose candidates to recover a continuous pose sequence. We make two contributions within this framework. First, we propose to use an efficient feature selection algorithm to select effective visual feature components. The task is formulated as a trace-ratio criterion which measures the score of the selected feature component subset, and the criterion is efficiently optimized to achieve the global optimum. The selected components are used instead of the original full feature set to improve the accuracy and efficiency of pose recovery. As second contribution, we propose to use sparse representation to retrieve the pose candidates, where the measured visual feature is expressed as a sparse linear combination of the examplars in the database. Sparse representation ensures that semantically similar poses have larger probability to be retrieved. The effectiveness of our approach is validated quantitatively through extensive evaluations on both synthetic and real data, and qualitatively by inspecting the results of the real time system we have implemented. ?? 2010 Elsevier Inc. All rights reserved.},
author = {Chen, Cheng and Yang, Yi and Nie, Feiping and Odobez, Jean Marc},
doi = {10.1016/j.cviu.2010.11.007},
file = {:Users/ben/Documents/csMSc/project/gait/research/3D human pose recovery from image by efficient visual feature selection.pdf:pdf},
isbn = {0818608773},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Feature selection,Motion understanding,Pose recovery,Sparse representation},
pages = {290--299},
title = {{3D human pose recovery from image by efficient visual feature selection}},
volume = {115},
year = {2011}
}
@inproceedings{Saidane2007,
abstract = {This paper presents an automatic recognition method for color text characters extracted from scene images, which is robust to strong distortions, complex background, low res- olution and non uniform lightning. Based on a specific ar- chitecture of convolutional neural networks, the proposed system automatically learns how to recognize characters without making any assumptions, without applying any pre- processing or post-processing and without using tunable parameters. For this purpose, we use a training set of scene text images extracted from the ICDAR 2003 public training database. The proposed method is compared to recent char- acter recognition techniques for scene images based on the ICDAR 2003 public samples dataset in order to contribute to the state-of-the-art method comparison efforts initiated in ICDAR 2003. Experimental results show an encouraging average recognition rate of 84.53\%, ranging from 93.47\% for clear images to 67.86\% for seriously distorted images.},
author = {Sa\"{\i}dane, Zohra and Garcia, Christophe},
booktitle = {International Workshop on Camera-Based Document Analysis and Recognition, 2007. (CBDAR2007)},
pages = {100--106},
title = {{Automatic Scene Text Recognition using a Convolutional Neural Network}},
year = {2007}
}
@article{DanielHerrera2012,
abstract = {We present an algorithm that simultaneously calibrates two color cameras, a depth camera, and the relative pose between them. The method is designed to have three key features: accurate, practical, and applicable to a wide range of sensors. The method requires only a planar surface to be imaged from various poses. The calibration does not use depth discontinuities in the depth image, which makes it flexible and robust to noise. We apply this calibration to a Kinect device and present a new depth distortion model for the depth sensor. We perform experiments that show an improved accuracy with respect to the manufacturer's calibration.},
author = {{Daniel Herrera}, C. and Kannala, Juho and Heikkil\"{a}, Janne},
doi = {10.1109/TPAMI.2012.125},
isbn = {0162-8828 VO - 34},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Camera calibration,Kinect,camera pair,depth camera,distortion},
number = {10},
pages = {2058--2064},
pmid = {22641701},
title = {{Joint depth and color camera calibration with distortion correction}},
volume = {34},
year = {2012}
}
@misc{Johansson1973,
abstract = {This paper reports the first phase of a research program on visual perception of motion patterns characteristic of living organisms in locomotion. Such motion patterns in animals and men are termed here as biological motion. They are characterized by a far higher degree of complexity than the patterns of simple mechanical motions usually studied in our laboratories. In everyday perceptions, the visual information from biological motion and from the corresponding figurative contour patterns (the shape of the body) are intermingled. A method for studying information from the motion pattern per se without interference with the form aspect was devised. In short, the motion of the living body was represented by a few bright spots describing the motions of the main joints. It is found that 10–12 such elements in adequate motion combinations in proximal stimulus evoke a compelling impression of human walking, running, dancing, etc. The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to these biological motion patterns. The validity of this model in the present context was experimentally tested and the results turned out to be highly positive.},
archivePrefix = {arXiv},
arxivId = {19433921},
author = {Johansson, Gunnar},
booktitle = {Perception \& Psychophysics},
doi = {10.3758/BF03212378},
eprint = {19433921},
isbn = {0031-5117},
issn = {0031-5117},
pages = {201--211},
pmid = {22024246},
title = {{Visual perception of biological motion and a model for its analysis}},
volume = {14},
year = {1973}
}
@inproceedings{Nguyen2012,
abstract = {We contribute an empirically derived noise model for the Kinect sensor. We systematically measure both lateral and axial noise distributions, as a function of both distance and angle of the Kinect to an observed surface. The derived noise model can be used to filter Kinect depth maps for a variety of applications. Our second contribution applies our derived noise model to the KinectFusion system to extend filtering, volumetric fusion, and pose estimation within the pipeline. Qualitative results show our method allows reconstruction of finer details and the ability to reconstruct smaller objects and thinner surfaces. Quantitative results also show our method improves pose estimation accuracy.},
author = {Nguyen, Chuong V. and Izadi, Shahram and Lovell, David},
booktitle = {Proceedings - 2nd Joint 3DIM/3DPVT Conference: 3D Imaging, Modeling, Processing, Visualization and Transmission, 3DIMPVT 2012},
doi = {10.1109/3DIMPVT.2012.84},
file = {:Users/ben/Downloads/Modeling Kinect Sensor Noise for Improved 3D Reconstruction and Tracking.pdf:pdf},
isbn = {9780769548739},
pages = {524--530},
title = {{Modeling kinect sensor noise for improved 3D reconstruction and tracking}},
year = {2012}
}
@article{Hinton2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Hinton, Geoffrey},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/srivastava14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@misc{Aggarwal2014,
abstract = {Human activity recognition has been an important area of computer vision research since the 1980s. Various approaches have been proposed with a great portion of them addressing this issue via conventional cameras. The past decade has witnessed a rapid development of 3D data acquisition techniques. This paper summarizes the major techniques in human activity recognition from 3D data with a focus on techniques that use depth data. Broad categories of algorithms are identified based upon the use of different features. The pros and cons of the algorithms in each category are analyzed and the possible direction of future research is indicated.},
author = {Aggarwal, J.K. and Xia, Lu},
booktitle = {Pattern Recognition Letters},
doi = {10.1016/j.patrec.2014.04.011},
file = {:Users/ben/Documents/csMSc/project/gait/research/Human activity recognition from 3D data- A review q.html:html},
issn = {01678655},
keywords = {3D data,Computer vision,Depth image,Human activity recognition},
title = {{Human activity recognition from 3D data: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865514001299},
year = {2014}
}
@article{Prechelt2012,
abstract = {Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (“early stopping”). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using difierent 12 problems and 24 difierent network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4\% on average), but cost much more training time (here: about factor 4 longer on average).},
author = {Prechelt, Lutz},
doi = {10.1007/978-3-642-35289-8-5},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {53--67},
title = {{Early stopping - But when?}},
volume = {7700},
year = {2012}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
author = {Zeiler, M. and Fergus, R.},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Visualizing and Understanding Convolutional Networks.pdf:pdf},
journal = {Computer Vision ECCV},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10590-1\_53},
volume = {8689},
year = {2014}
}
@article{Goodfellow2013a,
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
archivePrefix = {arXiv},
arxivId = {1302.4389},
author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
eprint = {1302.4389},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/goodfellow2013Maxout Networks.pdf:pdf},
journal = {arXiv preprint},
pages = {1319--1327},
title = {{Maxout Networks}},
url = {http://arxiv.org/abs/1302.4389},
year = {2013}
}
@article{Stone2013,
abstract = {A system for capturing habitual, in-home gait measurements using an environmentally mounted depth camera, the Microsoft Kinect, is presented. Previous work evaluating the use of the Kinect sensor for in-home gait measurement in a lab setting has shown the potential of this approach. In this paper, a single Kinect sensor and computer were deployed in the apartments of older adults in an independent living facility for the purpose of continuous, in-home gait measurement. In addition, a monthly fall risk assessment protocol was conducted for each resident by a clinician, which included traditional tools such as the timed up a go and habitual gait speed tests. A probabilistic methodology for generating automated gait estimates over time for the residents of the apartments from the Kinect data is described, along with results from the apartments as compared to two of the traditionally measured fall risk assessment tools. Potential applications and future work are discussed.},
author = {Stone, Erik E. and Skubic, Marjorie},
doi = {10.1109/TBME.2013.2266341},
isbn = {1558-2531 (Electronic)$\backslash$r0018-9294 (Linking)},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Depth camera,Kinect,fall risk,gait},
pages = {2925--2932},
pmid = {23744661},
title = {{Unobtrusive, continuous, in-home gait measurement using the microsoft kinect}},
volume = {60},
year = {2013}
}
@inproceedings{Bo2012a,
abstract = {Consumer depth cameras, such as the Microsoft Kinect, are capable of providing frames of dense depth values at real time. One fundamental question in utilizing depth cameras is how to best extract features from depth frames. Motivated by local descriptors on images, in particular kernel descriptors, we develop a set of kernel features on depth images that model size, 3D shape, and depth edges in a single framework. Through extensive experiments on object recognition, we show that (1) our local features capture different aspects of cues from a depth frame/view that complement one another; (2) our kernel features signiﬁcantly outperform traditional 3D features (e.g. Spin images); and (3) we signiﬁcantly improve the capabilities of depth and RGB-D (color+depth) recognition, achieving 10−15\% improvement in accuracy over the state of the art.},
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
booktitle = {Int. Symp. on Experimental Robotics (ISER)},
doi = {10.1109/IROS.2011.6095119},
isbn = {978-1-61284-456-5},
issn = {21530858},
pages = {1--15},
title = {{Unsupervised Feature Learning for RGB-D Based Object Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6095119$\backslash$nhttp://homes.cs.washington.edu/~lfb/paper/iser12.pdf},
year = {2012}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation
algorithm constitute the best example of a successful gradient based
learning technique. Given an appropriate network architecture,
gradient-based learning algorithms can be used to synthesize a complex
decision surface that can classify high-dimensional patterns, such as
handwritten characters, with minimal preprocessing. This paper reviews
various methods applied to handwritten character recognition and
compares them on a standard handwritten digit recognition task.
Convolutional neural networks, which are specifically designed to deal
with the variability of 2D shapes, are shown to outperform all other
techniques. Real-life document recognition systems are composed of
multiple modules including field extraction, segmentation recognition,
and language modeling. A new learning paradigm, called graph transformer
networks (GTN), allows such multimodule systems to be trained globally
using gradient-based methods so as to minimize an overall performance
measure. Two systems for online handwriting recognition are described.
Experiments demonstrate the advantage of global training, and the
flexibility of graph transformer networks. A graph transformer network
for reading a bank cheque is also described. It uses convolutional
neural network character recognizers combined with global training
techniques to provide record accuracy on business and personal cheques.
It is deployed commercially and reads several million cheques per day
},
author = {LeCun, Yann and Bottou, L\'{e}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Gradient-Based learning applied to document recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Schwarz2012,
abstract = {In this paper, we present a method for human full-body pose estimation from depth data that can be obtained using Time of Flight (ToF) cameras or the Kinect device. Our approach consists of robustly detecting anatomical landmarks in the 3D data and fitting a skeleton body model using constrained inverse kinematics. Instead of relying on appearance-based features for interest point detection that can vary strongly with illumination and pose changes, we build upon a graph-based representation of the depth data that allows us to measure geodesic distances between body parts. As these distances do not change with body movement, we are able to localize anatomical landmarks independent of pose. For differentiation of body parts that occlude each other, we employ motion information, obtained from the optical flow between subsequent intensity images. We provide a qualitative and quantitative evaluation of our pose tracking method on ToF and Kinect sequences containing movements of varying complexity. ?? 2011 Elsevier B.V. All rights reserved.},
annote = {Uses motion information},
author = {Schwarz, Loren Arthur and Mkhitaryan, Artashes and Mateus, Diana and Navab, Nassir},
doi = {10.1016/j.imavis.2011.12.001},
file = {:Users/ben/Documents/csMSc/project/gait/research/Human skeleton tracking from depth data using geodesic distances and optical flow.pdf:pdf},
isbn = {0262-8856},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Depth imaging,Geodesic distances,Human pose estimation},
number = {3},
pages = {217--226},
publisher = {Elsevier B.V.},
title = {{Human skeleton tracking from depth data using geodesic distances and optical flow}},
url = {http://dx.doi.org/10.1016/j.imavis.2011.12.001},
volume = {30},
year = {2012}
}
@article{Breuer2014,
author = {Breuer, Timo and Bodensteiner, Christoph and Arens, Michael},
doi = {10.1117/12.2067155},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/v2/Low-cost commodity depth sensor comparison and accuracy analysis.pdf:pdf},
journal = {Proceedings of SPIE},
keywords = {2,5d time-of-flight continuous modulation,kinect2},
title = {{Low-cost commodity depth sensor comparison and accuracy analysis}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2067155},
volume = {9250},
year = {2014}
}
@article{Qian1999,
abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning- rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
author = {Qian, Ning},
doi = {10.1016/S0893-6080(98)00116-6},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/momentum.pdf:pdf},
isbn = {1212543521},
issn = {08936080},
journal = {Neural Networks},
keywords = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,Momentum,Speed of convergence},
number = {1},
pages = {145--151},
pmid = {12662723},
title = {{On the momentum term in gradient descent learning algorithms}},
volume = {12},
year = {1999}
}
@article{Lawrence1997,
abstract = {We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.},
author = {Lawrence, S and Giles, C L and Tsoi, A C and Back, A D},
doi = {10.1109/72.554195},
isbn = {1045-9227 VO - 8},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
number = {1},
pages = {98--113},
pmid = {18255614},
title = {{Face recognition: a convolutional neural-network approach.}},
volume = {8},
year = {1997}
}
@book{Perry1992,
abstract = {This book encompasses the extensive work of the author and her successful years as a therapist and surgeon, renowned for her expertise in human gait. The text is broken down into four sections: Fundamentals, Normal Gait, Pathological Gait, and Gait Analysis Systems. In addition to the descriptions of the gait functions, a representative group of clinical examples has been included to facilitate the interpretation of the identical gait deviations. The book includes detailed laboratory records and more than 450 expert illustrations and photographs.},
author = {Perry, J.},
booktitle = {Journal of Pediatric Orthopaedics},
doi = {10.1001},
isbn = {9781556421921},
pages = {815},
title = {{Gait Analysis: Normal and Pathological Function. Journal of Pediatric Orthopaedics}},
url = {http://www.worldcat.org/isbn/9781556421921},
volume = {12},
year = {1992}
}
@inproceedings{Yang2012a,
abstract = {This paper proposes a depth hole filling method for RGBD images obtained from the Microsoft Kinect sensor. First, the proposed method labels depth holes based on 8-connectivity. For each labeled depth hole, the proposed method fills depth hole using the depth distribution of neighboring pixels of the depth hole. Then, we refine the hole filling result with cross-bilateral filtering. In experiments, by simply using the depth distribution of neighboring pixels, the proposed method improves the acquired depth map and reduces false filling caused by incorrect depth-color fusion.},
annote = {no mention of execution time},
author = {Yang, Na Eun and Kim, Yong Gon and Park, Rae Hong},
booktitle = {2012 IEEE International Conference on Signal Processing, Communications and Computing, ICSPCC 2012},
doi = {10.1109/ICSPCC.2012.6335696},
file = {:Users/ben/Downloads/06335696 (1).pdf:pdf},
isbn = {9781467321938},
issn = {1467321923},
keywords = {Kinect,cross-bilateral filtering,depth hole filling,depth map enhancement},
pages = {658--661},
title = {{Depth hole filling using the depth distribution of neighboring regions of depth holes in the Kinect sensor}},
year = {2012}
}
@article{Graham2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6071v3},
author = {Graham, Ben},
eprint = {arXiv:1412.6071v3},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Graham2015.pdf:pdf},
journal = {under review ICLR arXiv:1412.6071},
title = {{Fractional Max-Pooling}},
year = {2015}
}
@article{Huh2013,
author = {Huh, Sungjin and Kim, Gyeonghwan},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Huh2013.pdf:pdf},
keywords = {3d model based,geodesic distance,human pose estimation,key points,modified icp,self-occlusion},
pages = {333--342},
title = {{Human Pose Estimation from Depth Image Using Visibility Estimation and Key Points}},
year = {2013}
}
@article{Nesterov1983,
abstract = {Page 1. . . Soviel Math Doki. 269 < I9K3), 3 Vol 27 19 , No. 2 OF WITH 0l/k2) UDC il YU. E. NESTEROV},
author = {Nesterov, Y.},
journal = {Soviet Mathematics},
number = {2},
pages = {372--376},
title = {{A method of solving a convex programming problem with convergence rate O (1/k2)}},
url = {http://www.core.ucl.ac.be/~nesterov/Research/Papers/DAN83.pdf},
volume = {27},
year = {1983}
}
@article{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40\% (achieving a final mAP of 48\% on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {CVPR},
pages = {2--9},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@article{Russakovsky,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0575v3},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Jan, C. V. and Krause, J. and Ma, S.},
eprint = {arXiv:1409.0575v3},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/ImageNet Large Scale Visual Recognition Challenge.pdf:pdf},
journal = {arXiv preprint arXiv:1409.0575},
keywords = {benchmark,dataset,large-scale,object detection,object recognition},
title = {{ImageNet Large Scale Visual Recognition Challenge}}
}
@misc{Ye2011,
abstract = {This paper presents a novel system to estimate body pose configuration from a single depth map. It combines both pose detection and pose refinement. The input depth map is matched with a set of pre-captured motion exemplars to generate a body configuration estimation, as well as semantic labeling of the input point cloud. The initial estimation is then refined by directly fitting the body configuration with the observation (e.g., the input depth). In addition to the new system architecture, our other contributions include modifying a point cloud smoothing technique to deal with very noisy input depth maps, a point cloud alignment and pose search algorithm that is view-independent and efficient. Experiments on a public dataset show that our approach achieves significantly higher accuracy than previous state-of-art methods.},
author = {Ye, Mao and Wang, Xianwang and Yang, Ruigang and Ren, Liu and Pollefeys, M},
booktitle = {Computer Vision (ICCV), 2011 IEEE International Conference on},
doi = {10.1109/ICCV.2011.6126310},
isbn = {1550-5499 VO  -},
issn = {1550-5499},
keywords = {3D pose estimation,Accuracy,Cameras,Databases,Estimation,Joints,Sensors,Shape,body pose configuration estimation,human motion modeling,image matching,image motion analysis,input depth map,input point cloud semantic labeling,object detection,point cloud alignment,point cloud smoothing technique,pose detection,pose estimation,pose refinement,pose search algorithm,single depth image,smoothing methods},
pages = {731--738},
title = {{Accurate 3D pose estimation from a single depth image}},
year = {2011}
}
@article{Shum2013,
abstract = {The recent advancement of motion recognition using Microsoft Kinect stimulates many new ideas in motion capture and virtual reality applications. Utilizing a pattern recognition algorithm, Kinect can determine the positions of different body parts from the user. However, due to the use of a single-depth camera, recognition accuracy drops significantly when the parts are occluded. This hugely limits the usability of applications that involve interaction with external objects, such as sport training or exercising systems. The problem becomes more critical when Kinect incorrectly perceives body parts. This is because applications have limited information about the recognition correctness, and using those parts to synthesize body postures would result in serious visual artifacts. In this paper, we propose a new method to reconstruct valid movement from incomplete and noisy postures captured by Kinect. We first design a set of measurements that objectively evaluates the degree of reliability on each tracked body part. By incorporating the reliability estimation into a motion database query during run time, we obtain a set of similar postures that are kinematically valid. These postures are used to construct a latent space, which is known as the natural posture space in our system, with local principle component analysis. We finally apply frame-based optimization in the space to synthesize a new posture that closely resembles the true user posture while satisfying kinematic constraints. Experimental results show that our method can significantly improve the quality of the recognized posture under severely occluded environments, such as a person exercising with a basketball or moving in a small room.},
author = {Shum, Hubert P H and Ho, Edmond S L and Jiang, Yang and Takagi, Shu},
doi = {10.1109/TCYB.2013.2275945},
file = {:Users/ben/Documents/csMSc/project/gait/research/v3/Hubert P. H. Shum2013.pdf:pdf},
isbn = {2168-2275 (Electronic)$\backslash$r2168-2267 (Linking)},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Human-computer interaction,Kinect,Local principal component analysis,Posture reconstruction},
pages = {1357--1369},
pmid = {23981562},
title = {{Real-time posture reconstruction for Microsoft Kinect}},
volume = {43},
year = {2013}
}
@article{Zhou2014,
abstract = {We describe an approach for simultaneous localization and calibration of a stream of range images. Our approach jointly optimizes the camera trajectory and a calibration function that corrects the camera's unknown nonlinear distortion. Experiments with real-world benchmark data and synthetic data show that our approach increases the accuracy of camera trajectories and geometric models estimated from range video produced by consumer-grade cameras.},
author = {Zhou, Qian-yi and Koltun, Vladlen},
doi = {10.1109/CVPR.2014.65},
file = {:Users/ben/Downloads/Simultaneous Localization and Calibration.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {454--460},
title = {{Simultaneous Localization and Calibration : Self-Calibration of Consumer Depth Cameras}},
year = {2014}
}
@article{Alexandre2013,
author = {Alexandre, Lu\'{\i}s a},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/3D Object Recognition using Convolutional Neural Networks with Transfer Learning between Input Channels.pdf:pdf},
journal = {13th International Conference on Intelligent Autonomous Systems, Springer},
keywords = {3d object recognition,convolutional neural net-,transfer learning},
title = {{3D Object Recognition using Convolutional Neural Networks with Transfer Learning between Input Channels}},
year = {2014}
}
@article{Ji2013a,
abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
author = {Ji, Shuiwang and Yang, Ming and Yu, Kai and Xu, Wei},
doi = {10.1109/TPAMI.2012.59},
isbn = {9781605589077},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Decision Support Techniques,Image Interpretation,Imaging,Movement,Movement: physiology,Neural Networks (Computer),Pattern Recognition,Subtraction Technique,Three-Dimensional,Three-Dimensional: methods},
number = {1},
pages = {221--31},
pmid = {22392705},
title = {{3D convolutional neural networks for human action recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6165309$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/22392705},
volume = {35},
year = {2013}
}
@article{Han2013,
abstract = {With the invention of the low-cost Microsoft Kinect sensor, high-resolution depth and visual (RGB) sensing has become available for widespread use. The complementary nature of the depth and visual information provided by the Kinect sensor opens up new opportunities to solve fundamental problems in computer vision. This paper presents a comprehensive review of recent Kinect-based computer vision algorithms and applications. The reviewed approaches are classified according to the type of vision problems that can be addressed or enhanced by means of the Kinect sensor. The covered topics include preprocessing, object tracking and recognition, human activity analysis, hand gesture analysis, and indoor 3-D mapping. For each category of methods, we outline their main algorithmic contributions and summarize their advantages/differences compared to their RGB counterparts. Finally, we give an overview of the challenges in this field and future research trends. This paper is expected to serve as a tutorial and source of references for Kinect-based computer vision researchers.},
author = {Han, Jungong and Shao, Ling and Xu, Dong and Shotton, Jamie},
doi = {10.1109/TCYB.2013.2265378},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/Enhanced Computer Vision with Microsoft Kinect Sensor- A Review.pdf:pdf},
isbn = {2168-2267 VO  - 43},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Computer vision,Depth image,Information fusion,Kinect sensor},
number = {5},
pages = {1318--1334},
pmid = {23807480},
title = {{Enhanced computer vision with Microsoft Kinect sensor: A review}},
volume = {43},
year = {2013}
}
@article{Heskes1993,
author = {Heskes, Tom M and Kappen, Bert},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Heskes1993.pdf:pdf},
journal = {North-Holland Mathematical Library},
pages = {199--233},
title = {{On-line learning processes in arti cial neural networks}},
volume = {51},
year = {1993}
}
@article{Clark2012,
abstract = {Clinically feasible methods of assessing postural control such as timed standing balance and functional reach tests provide important information, however, they cannot accurately quantify specific postural control mechanisms. The Microsoft Kinect??? system provides real-time anatomical landmark position data in three dimensions (3D), and given that it is inexpensive, portable and simple to setup it may bridge this gap. This study assessed the concurrent validity of the Microsoft Kinect??? against a benchmark reference, a multiple-camera 3D motion analysis system, in 20 healthy subjects during three postural control tests: (i) forward reach, (ii) lateral reach, and (iii) single-leg eyes-closed standing balance. For the reach tests, the outcome measures consisted of distance reached and trunk flexion angle in the sagittal (forward reach) and coronal (lateral reach) planes. For the standing balance test the range and deviation of movement in the anatomical landmark positions for the sternum, pelvis, knee and ankle and the lateral and anterior trunk flexion angle were assessed. The Microsoft Kinect??? and 3D motion analysis systems had comparable inter-trial reliability (ICC difference=0.06. ??. 0.05; range, 0.00-0.16) and excellent concurrent validity, with Pearson's . r-values >0.90 for the majority of measurements (r=0.96. ??. 0.04; range, 0.84-0.99). However, ordinary least products analyses demonstrated proportional biases for some outcome measures associated with the pelvis and sternum. These findings suggest that the Microsoft Kinect??? can validly assess kinematic strategies of postural control. Given the potential benefits it could therefore become a useful tool for assessing postural control in the clinical setting. ?? 2012 Elsevier B.V.},
author = {Clark, Ross a. and Pua, Yong Hao and Fortin, Karine and Ritchie, Callan and Webster, Kate E. and Denehy, Linda and Bryant, Adam L.},
doi = {10.1016/j.gaitpost.2012.03.033},
file = {:Users/ben/Documents/csMSc/project/gait/research/Validity of the Microsoft Kinect for assessment of postural control.pdf:pdf},
isbn = {1879-2219},
issn = {09666362},
journal = {Gait and Posture},
keywords = {Assessment,Balance,Falls,Kinematic,Posture},
number = {3},
pages = {372--377},
pmid = {22633015},
publisher = {Elsevier B.V.},
title = {{Validity of the Microsoft Kinect for assessment of postural control}},
url = {http://dx.doi.org/10.1016/j.gaitpost.2012.03.033},
volume = {36},
year = {2012}
}
@inproceedings{Taigman2014,
abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2014.220},
isbn = {9781479951178},
issn = {10636919},
pages = {8},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
url = {http://www.cs.tau.ac.il/~wolf/papers/deepface\_11\_01\_2013.pdf},
year = {2014}
}
@article{Zhang2000,
abstract = { We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.},
author = {Zhang, Zhengyou},
doi = {10.1109/34.888718},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {2d pattern,Absolute conic,Calibration from planes,Camera calibration,Closed-form solution,Flexible plane-based calibration,Flexible setup,Lens distortion,Maximum likelihood estimation,Projective mapping},
number = {11},
pages = {1330--1334},
pmid = {131},
title = {{A flexible new technique for camera calibration}},
volume = {22},
year = {2000}
}
@article{Cimolin2014,
abstract = {Instrumented 3D-gait analysis (3D-GA) is an important method used to obtain information that is crucial for establishing the level of functional limitation due to pathology, observing its evolution over time and evaluating rehabilitative intervention effects. However, a typical 3D-GA evaluation produces a vast amount of data, and despite its objectivity, its use is complicated, and the data interpretation is difficult. It is even more difficult to obtain an overview on patient cohorts for a comparison. Moreover, there is a growing awareness of the need for a concise index, specifically, a single measure of the 'quality' of a particular gait pattern. Several gait summary measures, which have been used in conjunction with 3D-GA, have been proposed to objectify clinical impression, quantify the degree of gait deviation from normal, stratify the severity of pathology, document the changes in gait patterns over time and evaluate interventions.},
author = {Cimolin, Veronica and Galli, Manuela},
doi = {10.1016/j.gaitpost.2014.02.001},
issn = {1879-2219},
journal = {Gait \& posture},
pages = {1005--10},
pmid = {24613461},
title = {{Summary measures for clinical gait analysis: a literature review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24613461},
volume = {39},
year = {2014}
}
@article{Ionescu2014,
abstract = {We introduce a new dataset, Human3.6M, of 3.6 Million 3D Human poses, acquired by recording the performance of 11 subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models. Besides increasing the size the current state of the art datasets by several orders of magnitude, we aim to complement such datasets with a diverse set of poses encountered in typical human activities (taking photos, posing, greeting, eating, etc.), with synchronized image, motion capture and depth data, and with accurate 3D body scans of all subjects involved. We also provide mixed reality videos where 3D human models are animated using motion capture data and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide large scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. The dataset and code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, are available online at http://vision.imar.ro/human3.6m.},
author = {Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian},
doi = {10.1109/TPAMI.2013.248},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/datasets/Human3.6M.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Modeling and recovery of physical attributes,Motion},
number = {7},
pages = {1325--1339},
pmid = {24344079},
title = {{Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments}},
volume = {36},
year = {2014}
}
@article{Camplani2012,
abstract = {N this paper we present an efficient hole filling strategy that improves the quality of the depth maps obtained with the Microsoft Kinect device. The proposed approach is based on a joint-bilateral filtering framework that includes spatial and temporal information. The missing depth values are obtained applying iteratively a joint-bilateral filter to their neighbor pixels. The filter weights are selected considering three different factors: visual data, depth information and a temporal-consistency map. Video and depth data are combined to improve depth map quality in presence of edges and homogeneous regions. Finally, the temporal-consistency map is generated in order to track the reliability of the depth measurements near the hole regions. The obtained depth values are included iteratively in the filtering process of the successive frames and the accuracy of the hole regions depth values increases while new samples are acquired and filtered.},
author = {Camplani, Massimo and Salgado, Luis},
doi = {10.1117/12.911909},
isbn = {9780819489371},
issn = {0277786X},
journal = {Proceedings of SPIE},
keywords = {depth camera,depth map denoising,hole filling,joint bilateral filter,kinect},
pages = {82900E},
title = {{Efficient spatio-temporal hole filling strategy for Kinect depth maps}},
url = {http://www.gti.ssr.upm.es/~mac/files/papers/camplaniSPIE2012A.pdf},
volume = {8290},
year = {2012}
}
@article{Dutta2012a,
abstract = {Recording posture and movement is important for determining risk of musculoskeletal injury in the workplace, but existing motion capture systems are not suited for field work. Estimates of the 3-D relative positions of four 0.10. m cubes from the Kinect were compared to estimates from a Vicon motion capture system to determine whether the hardware sensing components were sensitive enough to be used as a portable 3-D motion capture system for workplace ergonomic assessments. The root-mean-squared errors (SD) were 0.0065. m (0.0048. m), 0.0109. m (0.0059. m), 0.0057. m (0.0042. m) in the x, y and z directions (with x axis to the right, y axis away from the sensor and z axis upwards). These data were collected over a range of 1.0-3.0. m from the device covering a field of view of 54.0 degrees horizontally and 39.1 degrees vertically. Requirements for software, hardware and subject preparation were also considered to determine the usability of the Kinect in the field. ?? 2011 Elsevier Ltd and The Ergonomics Society.},
author = {Dutta, Tilak},
doi = {10.1016/j.apergo.2011.09.011},
file = {:Users/ben/Documents/csMSc/project/gait/research/ergonomics workplace kinect evaluation.pdf:pdf},
isbn = {1872-9126 (Electronic)$\backslash$n0003-6870 (Linking)},
issn = {00036870},
journal = {Applied Ergonomics},
keywords = {Kinect,Kinematics,Portable motion capture},
number = {4},
pages = {645--649},
pmid = {22018839},
publisher = {Elsevier Ltd},
title = {{Evaluation of the Kinect??? sensor for 3-D kinematic measurement in the workplace}},
url = {http://dx.doi.org/10.1016/j.apergo.2011.09.011},
volume = {43},
year = {2012}
}
@article{Fankhauser,
abstract = {—With the introduction of the Microsoft Kinect for Windows v2 (Kinect v2), an exciting new sensor is available to robotics researchers. Similar to the original Kinect, the sensor is capable of acquiring accurate depth images at high rates. This is useful for robot navigation as dense and robust maps of the environment can be created. Opposed to the original Kinect working with the structured light technology, the Kinect v2 might be used outdoors in sunlight as the range sensor is based on the time-of-flight measurement principle. In this paper, we evaluate the application of the Kinect v2 depth sensor for mobile robot navigation. The results of calibrating the intrinsic camera parameters are presented and the minimal range of the depth sensor is examined. We analyze the data quality of the measurements for indoors and outdoors in overcast and direct sunlight situations. To this end, we introduce empirically derived noise models for the Kinect v2 sensor in both axial and lateral directions. The noise models take the measurement distance, the angle of the observed surface, and the sunlight incidence angle into account. These models can be used in post-processing to filter the Kinect v2 depth images for a variety of applications.},
author = {Fankhauser, P\'{e}ter and Bloesch, Michael and Rodriguez, Diego and Kaestner, Ralf and Hutter, Marco and Siegwart, Roland},
file = {:Users/ben/Downloads/Kinect v2 for Mobile Robot Navigation.pdf:pdf},
title = {{Kinect v2 for Mobile Robot Navigation: Evaluation and Modeling}}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
journal = {arXiv: 1207.0580},
pages = {1--18},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@inproceedings{Baak2011,
abstract = {In recent years, depth cameras have become a widely available sensor type that captures depth images at real-time frame rates. Even though recent approaches have shown that 3D pose estimation from monocular 2.5D depth images has become feasible, there are still challenging problems due to strong noise in the depth data and self-occlusions in the motions being captured. In this paper, we present an efficient and robust pose estimation framework for tracking full-body motions from a single depth image stream. Following a data-driven hybrid strategy that combines local optimization with global retrieval techniques, we contribute several technical improvements that lead to speed-ups of an order of magnitude compared to previous approaches. In particular, we introduce a variant of Dijkstra's algorithm to efficiently extract pose features from the depth data and describe a novel late-fusion scheme based on an efficiently computable sparse Hausdorff distance to combine local and global pose estimates. Our experiments show that the combination of these techniques facilitates real-time tracking with stable results even for fast and complex motions, making it applicable to a wide range of inter-active scenarios.},
author = {Baak, Andreas and Muller, Meinard and Bharaj, Gaurav and Seidel, Hans Peter and Theobalt, Christian},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126356},
isbn = {9781457711015},
issn = {1550-5499},
pages = {1092--1099},
title = {{A data-driven approach for real-time full body pose reconstruction from a depth camera}},
year = {2011}
}
@article{Saxena2012,
abstract = {Extracting high-quality dynamic foreground layers from a video sequence is a challenging problem due to the coupling of color, motion, and occlusion. Many approaches assume that the background scene is static or undergoes the planar perspective transformation. In this paper, we relax these restrictions and present a comprehensive system for accurately computing object motion, layer, and depth information. A novel algorithm that combines different clues to extract the foreground layer is proposed, where a voting-like scheme robust to outliers is employed in optimization. The system is capable of handling difficult examples in which the background is nonplanar and the camera freely moves during video capturing. Our work finds several applications, such as high-quality view interpolation and video editing.},
author = {Gupta, Saurabh and Arbelaez, Pablo and Malik, Jitendra},
doi = {10.1109/ICCVW.2011.6130298},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Gupta13.pdf:pdf},
isbn = {978-1-4673-0063-6},
issn = {1939-3539},
journal = {Eccv},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Color,Computer Simulation,Computer-Assisted,Computer-Assisted: instrumen,Computer-Assisted: instrumentat,Fractals,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Motion,Movement,Pattern Recognition,Phantoms,Reproducibility of Results,Signal Processing,Subtraction Technique,Subtraction Technique: instrumentation,Video Recording,Video Recording: instrumentation,blindness,motor control,perception and action,point cloud classification,r,recon-,scene understanding,sensorimotor integration,sensory substitution,spatial processing,vision rehabilitation,visual impairment},
number = {1},
pages = {1--14},
pmid = {20530810},
title = {{Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6385735$\backslash$nhttp://dl.acm.org/citation.cfm?doid=2366145.2366156$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995448$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumbe},
volume = {1},
year = {2012}
}
@article{Liu2013,
annote = {slow},
author = {Liu, Shaoguo and Wang, Ying and Wang, Jue and Wang, Haibo and Zhang, Jixia and Pan, Chunhong},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/kinect holes/KINECT DEPTH RESTORATION VIA ENERGY MINIMIZATION WITH T V 21 REGULARIZATION.pdf:pdf},
isbn = {9781479923410},
journal = {International Conference on Image Processing},
pages = {724--727},
title = {{Kinect Depth Restoration Via Energy Minimization With TV21 Regularization}},
year = {2013}
}
@article{Roux2012,
abstract = {We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.6258v4},
author = {Roux, Nicolas Le and Schmidt, Mark and Bach, Francis},
eprint = {arXiv:1202.6258v4},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {x},
pages = {1--34},
title = {{A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets}},
url = {http://hal.inria.fr/view\_by\_stamp.php?label=TESTINRIA\&langue=fr\&action\_todo=view\&id=hal-00674995},
volume = {1},
year = {2012}
}
@article{Aggarwal2011,
abstract = {Human activity recognition is an important area of computer vision research. Its applications include surveillance systems, patient monitoring systems, and a variety of systems that involve interactions between persons and electronic devices such as human-computer interfaces. Most of these applications require an automated recognition of high-level activities, composed of multiple simple (or atomic) actions of persons. This article provides a detailed overview of various state-of-the-art research papers on human activity recognition. We discuss both the methodologies developed for simple human actions and those for high-level activities. An approach-based taxonomy is chosen that compares the advantages and limitations of each approach. Recognition methodologies for an analysis of the simple actions of a single person are first presented in the article. Space-time volume approaches and sequential approaches that represent and recognize activities directly from input images are discussed. Next, hierarchical recognition methodologies for high-level activities are presented and compared. Statistical approaches, syntactic approaches, and description-based approaches for hierarchical recognition are discussed in the article. In addition, we further discuss the papers on the recognition of human-object interactions and group activities. Public datasets designed for the evaluation of the recognition methodologies are illustrated in our article as well, comparing the methodologies' performances. This review will provide the impetus for future research in more productive areas.},
author = {Aggarwal, Jk and Ryoo, Ms},
doi = {10.1145/1922649.1922653},
file = {:Users/ben/Documents/csMSc/project/gait/research/Human Activity Analysis- A Review.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys (CSUR)},
pages = {16:1--16:43},
title = {{Human activity analysis: A review}},
url = {http://dl.acm.org/citation.cfm?id=1922653},
volume = {43},
year = {2011}
}
@article{Blake2011,
annote = {[7, 39, 16, 42, 13]},
author = {Shotton, J. and Girshick, R. and Fitzgibbon, A. and Sharp, T. and Cook, M. and Finocchio, M. and Moore, R. and Kohli, P. and Criminisi, A. and Kipman, A. and Blake, A.},
file = {:Users/ben/Documents/csMSc/project/gait/research/journalsFromOutline/attachments/Real-Time Human Pose Recognition in Parts from Single Depth Images.pdf:pdf},
journal = {��IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)��},
title = {{Real-time human pose recognitiom in parts from single depth images}},
year = {2011}
}
@article{Jaderberg2014,
abstract = {The focus of this paper is speeding up the evaluation of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learn- ing tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition, showing a possible 2.5× speedup with no loss in accuracy, and 4.5× speedup with less than 1\% drop in accuracy, still achieving state-of-the-art on standard benchmarks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.3866v1},
author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, A},
eprint = {arXiv:1405.3866v1},
journal = {arXiv preprint arXiv:1405.3866},
title = {{Speeding up Convolutional Neural Networks with Low Rank Expansions}},
url = {http://arxiv.org/abs/1405.3866},
year = {2014}
}
@article{Zhang2002,
author = {Zhang, Zhengyou},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/A Flexible New Technique for Camera Calibration.pdf:pdf},
number = {11},
pages = {1330--1334},
title = {{A Flexible New Technique for Camera Calibration (Technical Report)}},
volume = {22},
year = {2002}
}
@article{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. A particularly significant one is data augmentation, which achieves a boost in performance in shallow methods analogous to that observed with CNN-based methods. Finally, we are planning to provide the configurations and code that achieve the state-of-the-art performance on the PASCAL VOC Classification challenge, along with alternative configurations trading-off performance, computation speed and compactness.},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1405.3531},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Chatfield2014.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
pages = {1--11},
title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
url = {http://arxiv.org/abs/1405.3531},
year = {2014}
}
@article{Le2014,
abstract = {Depth maps taken by the low cost Kinect sensor are often noisy and incomplete. Thus, post-processing for obtaining reliable depth maps is necessary for advanced image and video applications such as object recognition and multi-view rendering. In this paper, we propose adaptive directional filters that fill the holes and suppress the noise in depth maps. Specifically, novel filters whose window shapes are adaptively adjusted based on the $\backslash$r$\backslash$nedge direction of the color image are presented. Experimental results show that our method yields higher quality filtered depth maps than other existing methods, especially at the $\backslash$r$\backslash$nedge boundaries.},
author = {Le, Anh Vu and Jung, Seung Won and Won, Chee Sun},
doi = {10.3390/s140711362},
file = {:Users/ben/Downloads/sensors-14-11362-v2.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Depth map,Image filtering,Joint bilateral filter,Joint trilateral filter,Kinect},
number = {7},
pages = {11362--11378},
pmid = {24971470},
title = {{Directional joint bilateral filter for depth images}},
volume = {14},
year = {2014}
}
@incollection{Bottou2012,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, Leon},
booktitle = {Neural Networks: Tricks of the Trade},
doi = {10.1007/978-3-642-35289-8\_25},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Bottou2012.pdf:pdf},
issn = {2045-2322},
number = {1},
pages = {421--436},
pmid = {25382349},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8\_25},
volume = {1},
year = {2012}
}
@article{Plantard2015,
abstract = {Analyzing human poses with a Kinect is a promising method to evaluate potentials risks of musculoskeletal disorders at workstations. In ecological situations, complex 3D poses and constraints imposed by the environment make it difficult to obtain reliable kinematic information. Thus, being able to predict the potential accuracy of the measurement for such complex 3D poses and sensor placements is challenging in classical experimental setups. To tackle this problem, we propose a new evaluation method based on a virtual mannequin. In this study, we apply this method to the evaluation of joint positions (shoulder, elbow, and wrist), joint angles (shoulder and elbow), and the corresponding RULA (a popular ergonomics assessment grid) upper-limb score for a large set of poses and sensor placements. Thanks to this evaluation method, more than 500,000 configurations have been automatically tested, which would be almost impossible to evaluate with classical protocols. The results show that the kinematic information obtained by the Kinect software is generally accurate enough to fill in ergonomic assessment grids. However inaccuracy strongly increases for some specific poses and sensor positions. Using this evaluation method enabled us to report configurations that could lead to these high inaccuracies. As a supplementary material, we provide a software tool to help designers to evaluate the expected accuracy of this sensor for a set of upper-limb configurations. Results obtained with the virtual mannequin are in accordance with those obtained from a real subject for a limited set of poses and sensor placements.},
author = {Plantard, Pierre and Auvinet, Edouard and Pierres, Anne-Sophie and Multon, Franck},
doi = {10.3390/s150101785},
file = {:Users/ben/Documents/csMSc/project/gait/research/Pose Estimation with a Kinect for Ergonomic Studies.pdf:pdf},
issn = {1424-8220},
journal = {Sensors},
keywords = {Kinect,accuracy,virtual mannequin},
pages = {1785--1803},
title = {{Pose Estimation with a Kinect for Ergonomic Studies: Evaluation of the Accuracy Using a Virtual Mannequin}},
url = {http://www.mdpi.com/1424-8220/15/1/1785/},
volume = {15},
year = {2015}
}
@article{Krizhevsky2012,
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
eprint = {1102.0183},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Krizhevsky2012.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{Chen2013c,
abstract = {Considering the existing depth recovery approaches that have different limitations when applying to Kinect depth data, in this paper, we propose to integrate their effective features including adaptive support region selection, reliable depth selection and color guidance together under a unified framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole-filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term. The fidelity term takes into account the characteristics of Kinect data. The regularization term is designed to incorporate the joint bilateral filtering (JBF) kernel and the joint trilateral filtering (JTF) kernel so as to facilitate both depth hole-filling and denoising. Moreover, the JBF kernel is modified to incorporate the structure information. Both simulations on the benchmark Middlebury dataset and experiments on real Kinect data show that our proposed method achieves state-of-the-art performance in terms of recovery accuracy and visual quality.},
annote = {good discusion of filters but no mention of speed},
author = {Chen, Chongyu and Cai, Jianfei and Zheng, Jianmin and Cham, Tat Jen and Shi, Guangming},
doi = {10.1109/MMSP.2013.6659255},
file = {:Users/ben/Downloads/A Color-Guided, Region-Adaptive and.pdf:pdf},
isbn = {9781479901258},
journal = {2013 IEEE International Workshop on Multimedia Signal Processing, MMSP 2013},
pages = {7--12},
title = {{A color-guided, region-adaptive and depth-selective unified framework for Kinect depth recovery}},
year = {2013}
}
@article{Hale2010,
abstract = {Researching falls in persons with ID is limited by difficulties in applying standardised balance outcome measures. The modified Gait Abnormality Rating Scale (GARS-M), developed to identify falls risk in older adults, requires only that the participant walks and thus may be a feasible falls research tool to use with people with ID. In the present study, we consider the interrater reliability of the GARS-M.},
author = {Hale, Leigh and McIlraith, Lucy and Miller, Clare and Stanley-Clarke, Terri and George, Rebecca},
doi = {10.3109/13668251003702128},
isbn = {1366825100370},
issn = {1469-9532},
journal = {Journal of intellectual \& developmental disability},
pages = {77--81},
pmid = {20560695},
title = {{The interrater reliability of the modified gait abnormality rating scale for use with people with intellectual disability.}},
volume = {35},
year = {2010}
}
@misc{Buys2014a,
abstract = {Human body detection and pose estimation is useful for a wide variety of applications and environments. Therefore a human body detection and pose estimation system must be adaptable and customizable. This paper presents such a system that extracts skeletons from RGB-D sensor data. The system adapts on-line to difficult unstructured scenes taken from a moving camera (since it does not require background subtraction) and benefits from using both color and depth data. It is customizable by virtue of requiring less training data, having a clearly described training method, and a customizable human kinematic model. Results show successful application to data from a moving camera in cluttered indoor environments. This system is open-source, encouraging reuse, comparison, and future research. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Buys, Koen and Cagniart, Cedric and Baksheev, Anatoly and {De Laet}, Tinne and {De Schutter}, Joris and Pantofaru, Caroline},
booktitle = {Journal of Visual Communication and Image Representation},
doi = {10.1016/j.jvcir.2013.03.011},
file = {:Users/ben/Documents/csMSc/project/gait/research/An adaptable system for RGB-D based human body detection and pose estimation.html:html},
issn = {10473203},
keywords = {Body part recognition,Joint locations,Motion capture,Open source,Person detection,Pose detection,RGB-D data,Random decision forest,Real-time},
pages = {39--52},
title = {{An adaptable system for RGB-D based human body detection and pose estimation}},
volume = {25},
year = {2014}
}
@article{Coifman2006,
abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods. © 2006.},
author = {Coifman, Ronald R. and Lafon, St\'{e}phane},
doi = {10.1016/j.acha.2006.04.006},
isbn = {1063-5203},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {Diffusion metric,Diffusion processes,Dimensionality reduction,Eigenmaps,Graph Laplacian,Manifold learning},
number = {1},
pages = {5--30},
title = {{Diffusion maps}},
volume = {21},
year = {2006}
}
@article{Baccouche2011,
abstract = {We propose in this paper a fully automated deep model, which learns to classify human actions without using any prior knowl- edge. The first step of our scheme, based on the extension of Convo- lutional Neural Networks to 3D, automatically learns spatio-temporal features. A Recurrent Neural Network is then trained to classify each sequence considering the temporal evolution of the learned features for each timestep. Experimental results on the KTH dataset show that the proposed approach outperforms existing deep models, and gives compa- rable results with the best related works.},
author = {Baccouche, Moez and Mamalet, Franck and Wolf, C},
doi = {10.1007/978-3-642-25446-8},
isbn = {978-3-642-25445-1},
journal = {Human Behavior \ldots},
keywords = {3d convolutional,deep models,human action recognition,kth human actions dataset,long short-term memory,neural networks},
pages = {29--39},
title = {{Sequential deep learning for human action recognition}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-25446-8\_4},
year = {2011}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/arch/He2015.pdf:pdf},
isbn = {978-3-319-10577-2},
issn = {0162-8828},
journal = {arXiv preprint arXiv:1406.4729},
pages = {1--14},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
url = {http://arxiv.org/abs/1406.4729v1$\backslash$npapers3://publication/uuid/09415D06-A785-4329-82C0-B9FF2B1FEAB7},
volume = {cs.CV},
year = {2014}
}
@article{Newcombe,
author = {Newcombe, Richard a and Molyneaux, David and Kim, David and Davison, Andrew J and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
doi = {10.1109/ISMAR.2011.6092378},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/KinectFusion- Real-Time Dense Surface Mapping and Tracking∗.pdf:pdf},
isbn = {9781457721854},
issn = {<null>},
keywords = {3,4,8,ar,computer graphics,dense reconstruction,depth cameras,gpu,i,image genera-,image processing and com-,index terms,picture,real-time,scanning,slam,tion - digitizing and,tracking,volumetric representation},
pmid = {6162880},
title = {{KinectFusion: Real-Time Dense Surface Mapping and Tracking}}
}
@article{Ji2013,
abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
author = {Ji, Shuiwang and Yang, Ming and Yu, Kai},
doi = {10.1109/TPAMI.2012.59},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Decision Support Techniques,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Movement,Movement: physiology,Neural Networks (Computer),Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Subtraction Technique},
number = {1},
pages = {221--31},
pmid = {22392705},
title = {{3D convolutional neural networks for human action recognition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22392705},
volume = {35},
year = {2013}
}
@article{Mikhelson2014,
abstract = {Automatic camera calibration has remained a hard topic in computer vision since its inception due to its reliance on the image correspondence problem. This problem becomes even more pronounced when calibrating a depth image with a color image due to a lack of simple correspondences between the two modalities. In this work, we develop a completely automatic, very fast, online algorithm that demonstrates how a consumer-grade depth camera can be calibrated with a color camera with minimal user interaction. © 2013 Elsevier Inc. All rights reserved.},
author = {Mikhelson, Ilya V. and Lee, Philip G. and Sahakian, Alan V. and Wu, Ying and Katsaggelos, Aggelos K.},
doi = {10.1016/j.jvcir.2013.03.010},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Automatic,Calibration,Color cameras,Depth cameras,Fast,Online,Point cloud,Point correspondence},
number = {1},
pages = {218--226},
title = {{Automatic, fast, online calibration between depth and color cameras}},
volume = {25},
year = {2014}
}
@article{Uddin2014,
abstract = {Gait recognition at smart home is considered as a primary function of the smart system          nowadays. The significance of gait recognition is high especially for the elderly as gait          is one of the basic activities to promote and preserve their health. In this work, a novel          method was proposed for human gait recognition by processing depth videos from a depth          camera. The gait recognition method utilizes local directional patterns (LDPs) for local          feature extraction from depth silhouettes and hidden Markov models (HMMs) for recognition.          The LDP features were first extracted from the depth silhouettes of a human body from each          frame of a video containing human gait. The dimension of the LDP features was reduced by          principal component analysis. Then, each HMM was trained using the LDP features. Finally,          the recognition was done with a maximum likelihood calculation of the trained HMMs of          different gaits. We focused on training and recognizing two kinds of gaits here, namely,          normal and abnormal. The proposed approach shows superior recognition performance over          other traditional methods of gait recognition. },
author = {Uddin, Md. Zia and Kim, Jeong Tai and Kim, Tae-Seong},
doi = {10.1177/1420326X14522670},
journal = {Indoor and Built Environment },
pages = {133--140},
title = {{Depth video-based gait recognition for smart home using local directional pattern features and hidden Markov model}},
url = {http://ibe.sagepub.com/cgi/content/abstract/23/1/133},
volume = {23 },
year = {2014}
}
@article{Scherer2010,
abstract = {A common practice to gain invariant features in object recog- nition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant proper- ties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
author = {Scherer, Dominik and M\"{u}ller, Andreas and Behnke, Sven},
doi = {10.1007/978-3-642-15825-4\_10},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Scherer2010Pooling.pdf:pdf},
isbn = {3642158242},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {92--101},
title = {{Evaluation of pooling operations in convolutional architectures for object recognition}},
volume = {6354},
year = {2010}
}
@article{Pfister,
author = {Pfister, Tomas and Simonyan, Karen and Charles, James and Zisserman, Andrew},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos.pdf:pdf},
journal = {Asian Conference on Computer Vision (ACCV)},
title = {{Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos}},
year = {2014}
}
@article{Kim2014,
author = {Kim, Jin-bum and Piao, Nanzhou and Kim, Hong-in and Park, Rae-hong},
file = {:Users/ben/Downloads/Depth Hole Filling for 3-D Reconstruction Using.pdf:pdf},
isbn = {1569949581},
keywords = {3-d reconstruction,depth hole,depth map enhancement,rgbd image},
title = {{Depth Hole Filling for 3-D Reconstruction Using Color and Depth Images}},
year = {2014}
}
@article{Schaul2013,
abstract = {Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on numerous established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.},
archivePrefix = {arXiv},
arxivId = {1312.6055},
author = {Schaul, Tom and Antonoglou, Ioannis and Silver, David},
eprint = {1312.6055},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/stochasticUNitTests.pdf:pdf},
journal = {arXiv:1312.6055 [cs]},
pages = {1--13},
title = {{Unit Tests for Stochastic Optimization}},
url = {http://arxiv.org/abs/1312.6055$\backslash$nhttp://www.arxiv.org/pdf/1312.6055.pdf},
year = {2013}
}
@article{Wolfson1990,
abstract = {We evaluated the gait of 49 nursing home residents (27 of whom had a history of recent falls), and 22 controls. Measures consisted of stride length and walking speed, as well as a videotape-based analysis of 16 facets of gait. The study demonstrates that stride length, walking speed, and the assessment of videotaped gait correlated well with each other and were significantly impaired in fallers compared to controls. Arm swing amplitude, upper-lower extremity synchrony, and guardedness of gait were most impaired in fallers. Although subjects who fell were more often demented than controls, it is likely that this represents a selection bias in nursing homes. Visual rating of gait features in the nursing home population is a simple and useful alternative to established methods of gait analysis.},
author = {Wolfson, L. and Whipple, R. and Amerman, P. and Tobin, J. N.},
doi = {10.1093/geronj/45.1.M12},
isbn = {0022-1422 (Print)$\backslash$r0022-1422 (Linking)},
issn = {0022-1422},
journal = {Journal of Gerontology},
pages = {M12--M19},
pmid = {2295773},
title = {{Gait assessment in the elderly: a gait abnormality rating scale and its relation to falls.}},
volume = {45},
year = {1990}
}
@article{Camplani2012a,
abstract = {In this paper we present an adaptive spatio-temporal filter that aims to improve low-cost depth camera accuracy and stability over time. The proposed system is composed by three blocks that are used to build a reliable depth map of static scenes. An adaptive joint-bilateral filter is used to obtain consistent depth maps by jointly considering depth and video information and by adapting its parameters to different levels of estimated noise. Kalman filters are used to reduce the temporal random fluctuations of the measurements. Finally an interpolation algorithm is used to obtain consistent depth maps in the regions where the depth information is not available. Results show that this approach allows to considerably improve the depth maps quality by considering spatio-temporal information and by adapting its parameters to different levels of noise.},
author = {Camplani, Massimo and Salgado, Luis},
doi = {10.1109/ESPA.2012.6152439},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/kinect holes/ADAPTIVE SPATIO-TEMPORAL FILTER FOR LOW-COST CAMERA DEPTH MAPS.pdf:pdf},
isbn = {9781467308984},
journal = {2012 IEEE International Conference on Emerging Signal Processing Applications, ESPA 2012 - Proceedings},
keywords = {Depth Map denoising,Depth camera,Hole Filling,Joint-Bilateral Filter},
pages = {33--36},
title = {{Adaptive spatio-temporal filter for low-cost camera depth maps}},
year = {2012}
}
@article{Khoshelham2012b,
abstract = {Consumer-grade range cameras such as the Kinect sensor have the potential to be used in mapping applications where accuracy requirements are less strict. To realize this potential insight into the geometric quality of the data acquired by the sensor is essential. In this paper we discuss the calibration of the Kinect sensor, and provide an analysis of the accuracy and resolution of its depth data. Based on a mathematical model of depth measurement from disparity a theoretical error analysis is presented, which provides an insight into the factors influencing the accuracy of the data. Experimental results show that the random error of depth measurement increases with increasing distance to the sensor, and ranges from a few millimeters up to about 4 cm at the maximum range of the sensor. The quality of the data is also found to be influenced by the low resolution of the depth measurements.},
author = {Khoshelham, Kourosh and Elberink, Sander Oude},
doi = {10.3390/s120201437},
file = {:Users/ben/Library/Application Support/Mendeley Desktop/Downloaded/Khoshelham, Elberink - 2012 - Accuracy and resolution of kinect depth data for indoor mapping applications.pdf:pdf},
isbn = {1424-8220},
issn = {14248220},
journal = {Sensors},
keywords = {Calibration,Error budget,Imaging,Laser scanning,Point cloud,RGB-D,Range camera,Sensor,Triangulation},
pages = {1437--1454},
pmid = {22438718},
title = {{Accuracy and resolution of kinect depth data for indoor mapping applications}},
volume = {12},
year = {2012}
}
@article{Baccouche2011a,
abstract = {We propose in this paper a fully automated deep model, which learns to classify human actions without using any prior knowl- edge. The first step of our scheme, based on the extension of Convo- lutional Neural Networks to 3D, automatically learns spatio-temporal features. A Recurrent Neural Network is then trained to classify each sequence considering the temporal evolution of the learned features for each timestep. Experimental results on the KTH dataset show that the proposed approach outperforms existing deep models, and gives compa- rable results with the best related works.},
author = {Baccouche, Moez and Mamalet, Franck and Wolf, C.},
doi = {10.1007/978-3-642-25446-8},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Baccouche2010.pdf:pdf},
isbn = {978-3-642-25445-1},
journal = {Lecture Notes in Computer Science: Human Behavior Understanding},
keywords = {3d convolutional,deep models,human action recognition,kth human actions dataset,long short-term memory,neural networks},
pages = {29--39},
title = {{Sequential deep learning for human action recognition}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-25446-8\_4},
volume = {7065},
year = {2011}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
eprint = {arXiv:1103.4296v1},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/DuchiAdaptiveGradients.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Kavukcuoglu2010,
abstract = {We propose an unsupervisedmethod for learningmulti-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall repre- sentation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efficient feed-forward encoder that predicts quasi- sparse features from the input. While patch-based training rarely produces any- thing but oriented edge detectors, we show that convolutional training produces highly diverse filters, including center-surround filters, corner detectors, cross de- tectors, and oriented grating detectors. We show that using these filters in multi- stage convolutional network architecture improves performance on a number of visual recognition and detection tasks.},
author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-Lan and Gregor, Karol and Mathieu, Michael and LeCun, Yann},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/Learning Convolutional Feature Hierarchies for Visual Recognition.pdf:pdf},
isbn = {9781617823800},
issn = {-},
journal = {Advances in neural information processing systems 23},
number = {1},
pages = {1090--1098},
title = {{Learning Convolutional Feature Hierarchies for Visual Recognition}},
year = {2010}
}
@inproceedings{Cruz2012,
abstract = {Kinect is a device introduced in November 2010 as an accessory of Xbox 360. The acquired data has different and complementary natures, combining geometry with visual attributes. For this reason, Kinect is a flexible tool that can be used in applications from several areas such as: Computer Graphics, Image Processing, Computer Vision and Human-Machine Interaction. In this way, the Kinect is a widely used device in industry (games, robotics, theater performers, natural interfaces, etc.) and in research. We will initially present some concepts about the device: the architecture and the sensor. We then will discuss about the data acquisition process: capturing, representation and filtering. Capturing process consists of obtaining a colored image (RGB) and performing a depth measurement (D), with structured light technique. This data is represented by a structure called RGBD Image. We will also talk about the main tools available for developing applications on various platforms. Furthermore, we will discuss some recent projects based on RGBD Images. In particular, those related to Object Recognition, 3D Reconstruction, Augmented Reality, Image Processing, Robotic, and Interaction. In this survey, we will show some research developed by the academic community and some projects developed for the industry. We intend to show the basic principles to begin developing applications using Kinect, and present some projects developed at the VISGRAF Lab. And finally, we intend to discuss the new possibilities, challenges and trends raised by Kinect.},
author = {Cruz, Leandro and Lucio, Djalma and Velho, Luiz},
booktitle = {Proceedings: 25th SIBGRAPI - Conference on Graphics, Patterns and Images Tutorials, SIBGRAPI-T 2012},
doi = {10.1109/SIBGRAPI-T.2012.13},
isbn = {9780769548302},
issn = {978-0-7695-4830-2},
keywords = {Kinect,RGBD Images},
pages = {36--49},
title = {{Kinect and RGBD images: Challenges and applications}},
year = {2012}
}
@article{Plagianakos2008,
abstract = {The efficient supervised training of artificial neural networks is commonly viewed as the minimization of an error function that depends on the weights of the network. This perspective gives some advantage to the development of effective training algorithms, because the problem of minimizing a function is well known in the field of numerical analysis. Typically, deterministic minimization methods are employed, however, in several cases, significant training speed and alleviation of the local minima problem can be achieved when stochastic minimization methods are used. In this paper a method for adapting the learning rate in stochastic gradient descent is presented. The main feature of the proposed learning rate adaptation scheme is that it exploits gradient– related information from the current as well as the two previous pattern presentations. This seems to provide some kind of stabilization in the value of the learning rate and helps the stochastic gradient descent to exhibit fast convergence and a high rate of success. Tests in various problems validate the above mentioned characteristics of the new algorithm.},
author = {Plagianakos, V P and Magoulas, G D and Vrahatis, M N},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/PlagianakosLearningRateAdaption.pdf:pdf},
isbn = {0792369424},
journal = {Information Systems},
keywords = {backpropagation neural networks,batch training,ing rate adaptation,learn-,line training,on,stochastic gradient descent},
pages = {15--26},
title = {{LEARNING RATE ADAPTATION IN STOCHASTIC GRADIENT DESCENT}},
url = {http://www.dcs.bbk.ac.uk/~gmagoulas/c-7.pdf},
year = {2008}
}
@article{Moeslund2006,
abstract = {This survey reviews advances in human motion capture and analysis from 2000 to 2006, following a previous survey of papers up to 2000 [T.B. Moeslund, E. Granum, A survey of computer vision-based human motion capture, Computer Vision and Image Understanding, 81(3) (2001) 231-268.]. Human motion capture continues to be an increasingly active research area in computer vision with over 350 publications over this period. A number of significant research advances are identified together with novel methodologies for automatic initialization, tracking, pose estimation, and movement recognition. Recent research has addressed reliable tracking and pose estimation in natural scenes. Progress has also been made towards automatic understanding of human actions and behavior. This survey reviews recent trends in video-based human capture and analysis, as well as discussing open problems for future research to achieve automatic visual analysis of human movement. © 2006 Elsevier Inc. All rights reserved.},
author = {Moeslund, Thomas B. and Hilton, Adrian and Kr\"{u}ger, Volker},
doi = {10.1016/j.cviu.2006.08.002},
file = {:Users/ben/Documents/csMSc/project/gait/research/A survey of advances in vision-based human motion capture and analysis.pdf:pdf},
isbn = {1077-3142},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Human motion,Initialization,Pose estimation,Recognition,Review,Tracking},
pages = {90--126},
title = {{A survey of advances in vision-based human motion capture and analysis}},
volume = {104},
year = {2006}
}
@article{Donahue2014,
abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
archivePrefix = {arXiv},
arxivId = {1310.1531},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
eprint = {1310.1531},
file = {:Users/ben/Library/Application Support/Mendeley Desktop/Downloaded/Donahue et al. - 2014 - DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition.pdf:pdf},
isbn = {9781634393973},
journal = {International Conference on Machine Learning},
pages = {647--655},
title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1310.1531},
year = {2014}
}
@misc{Vapnik1994a,
abstract = {A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.},
author = {Vapnik, Vladimir and Levin, Esther and Cun, Yann Le},
booktitle = {Neural Computation},
doi = {10.1162/neco.1994.6.5.851},
issn = {0899-7667},
number = {5},
pages = {851--876},
title = {{Measuring the VC-Dimension of a Learning Machine}},
volume = {6},
year = {1994}
}
@article{Yosinski2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.1792v1},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {arXiv:1411.1792v1},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/transferLearning/Yosinski2014.pdf:pdf},
journal = {Nips14},
title = {{How Transferable are Features in Deep Neural Networks?}},
volume = {27},
year = {2014}
}
@article{Jia2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5093v1},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
eprint = {arXiv:1408.5093v1},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/JiaCaffe.pdf:pdf},
isbn = {9781450330633},
journal = {ACM Conference on Multimedia},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@article{Ye2014a,
abstract = {We present a system that allows the user to virtually try on new clothes. It uses a single commodity depth camera to capture the user in 3D. Both the pose and the shape of the user are estimated with a novel real-time template-based approach that performs tracking and shape adaptation jointly. The result is then used to drive realistic cloth simulation, in which the synthesized clothes are overlayed on the input image. The main challenge is to handle missing data and pose ambiguities due to the monocular setup, which captures less than 50 percent of the full body. Our solution is to incorporate automatic shape adaptation and novel constraints in pose tracking. The effectiveness of our system is demonstrated with a number of examples.},
author = {Ye, M and Yang, R},
doi = {10.1109/CVPR.2014.301},
file = {:Users/ben/Documents/csMSc/project/gait/research/v3/Robust Human Body Shape and Pose Tracking2013.pdf:pdf},
isbn = {978-1-4799-5118-5},
journal = {Multi-Camera \ldots},
keywords = {-human motion capture,figure 1,mation,non-rigid surface defor-,our approach tracks both,pose estimation,pose of humans,results with three different,simultaneously,standard datasets are shown,the shape and the},
number = {January},
title = {{Real-time Simultaneous Pose and Shape Estimation for Articulated Objects Using a Single Depth Camera}},
url = {http://vis.uky.edu/~gravity/Publications/2014/ArticulatedPoseShape\_CVPR14.pdf$\backslash$nhttp://europepmc.org/abstract/MED/24650982$\backslash$nhttp://books.google.com/books?hl=en\&lr=\&id=XA\_6o2dhTGEC\&oi=fnd\&pg=PA335\&dq=Real-Time+3D+Body+Pose+Estimation\&ots=\_X-Zs47meG\&sig=I-iy},
year = {2014}
}
@article{Lun2015,
annote = {discusses v1 v v2 and healthcare applications and nice discussion of decision trees, NNs and SVMs. and data sets},
author = {Lun, Roanna and Zhao, Wenbing},
file = {:Users/ben/Downloads/A SURVEY OF APPLICATIONS AND HUMAN MOTION v2.pdf:pdf},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
keywords = {human motion recognition,machine learning,microsoft kinect},
title = {{A Survey of Applications and Human Motion}},
year = {2015}
}
@article{Wang1995,
abstract = {We study learning in a general class of machines which return a
(variable) linear form of a (fixed) set of nonlinear transformations of
points in an input space. A fixed machine in this class accepts inputs X
from an arbitrary input space and produces scalar outputs
Y=\&amp;Sigma;<sub>i=1</sub><sup>d</sup>\&amp;psi;<sub>i</sub>(X)w<sub>i
</sub>*+\&amp;xi;=\&amp;psi;(X)'w*+\&amp;xi; (1). Here, w*=(w<sub>1</sub>*,\&amp;hellip;,w
<sub>d</sub>*)' is a fixed vector of real weights representing the
target concept to be learned, for each i, \&amp;psi;<sub>i</sub>(X) is a
fixed real function of the inputs, with
\&amp;psi;(X)=(\&amp;psi;<sub>1</sub>(X),\&amp;hellip;\&amp;psi;<sub>d</sub>(X))' the
corresponding vector of functions, and \&amp;xi; is a random noise term. We
suppose that the learner receives an i.i.d., random sample of examples
(X<sub>1</sub>,Y<sub>1</sub>),\&amp;hellip;,(X<sub>n</sub>,Y<sub>n</sub>)
generated according to the joint distribution on input-output pairs
(X,Y) induced through the medium of the (unknown) relation (1) and a
fixed (unknown) distribution on input-noise pairs (X,\&amp;xi;). The goal of
the learner is to infer a hypothesis w=(w<sub>1</sub>,\&amp;hellip;w<sub>d
</sub>)' with small (mean-square) generalisation error E(Y-\&amp;psi;(X)'w)
<sup>2</sup> on future random examples (X,Y) generated independently of
the training sample from the same underlying distribution. Here E
denotes expectation with respect to the underlying probability
distribution generating the examples},
author = {Wang, Changfeng Wang Changfeng and Venkatesh, S.S. and Judd, J.S.},
doi = {10.1109/ISIT.1995.531518},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/wang1994.pdf:pdf},
isbn = {0-7803-2453-6},
journal = {Proceedings of 1995 IEEE International Symposium on Information Theory},
pages = {303--310},
title = {{Optimal stopping and effective machine complexity in learning}},
year = {1995}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.0398v1},
author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
eprint = {arXiv:1103.0398v1},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Collobert2011.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {and semantic role labeling,be applied,by trying,chunking,learning algorithm that can,named entity recognition,natural language processing,neural network architecture and,neural networks,part-of-speech tagging,processing tasks including,this versatility is achieved,to various natural language,we propose a unified},
pages = {2493--2537},
title = {{Natural Language Processing (almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{242011,
author = {24, Paper},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/3D with Kinect.pdf:pdf},
journal = {Image (Rochester, N.Y.)},
title = {{3D with Kinect}},
year = {2011}
}
@article{Rana2015,
author = {Rana, Pravin Kumar and Member, Student and Taghia, Jalil and Ma, Zhanyu and Flierl, Markus},
file = {:Users/ben/Downloads/Probabilistic Multiview Depth Image Enhancement.pdf:pdf},
number = {3},
pages = {435--448},
title = {{Probabilistic Multiview Depth Image Enhancement Using Variational Inference}},
volume = {9},
year = {2015}
}
@article{Schraudolph2002,
abstract = {We propose a generic method for iteratively approximating various second-order gradient steps - Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient - in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD.},
author = {Schraudolph, Nicol N},
doi = {10.1162/08997660260028683},
issn = {0899-7667},
journal = {Neural computation},
number = {7},
pages = {1723--1738},
pmid = {12079553},
title = {{Fast curvature matrix-vector products for second-order gradient descent.}},
volume = {14},
year = {2002}
}
@article{Buys2014,
abstract = {Human body detection and pose estimation is useful for a wide variety of applications and environments. Therefore a human body detection and pose estimation system must be adaptable and customizable. This paper presents such a system that extracts skeletons from RGB-D sensor data. The system adapts on-line to difficult unstructured scenes taken from a moving camera (since it does not require background subtraction) and benefits from using both color and depth data. It is customizable by virtue of requiring less training data, having a clearly described training method, and a customizable human kinematic model. Results show successful application to data from a moving camera in cluttered indoor environments. This system is open-source, encouraging reuse, comparison, and future research. ?? 2013 Elsevier Inc. All rights reserved.},
annote = {uses kinematic info, possibly not applicable, maybe adaptable?...},
author = {Buys, Koen and Cagniart, Cedric and Baksheev, Anatoly and {De Laet}, Tinne and {De Schutter}, Joris and Pantofaru, Caroline},
doi = {10.1016/j.jvcir.2013.03.011},
file = {:Users/ben/Documents/csMSc/project/gait/research/An adaptable system for RGB-D based human body detection and pose estimation.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Body part recognition,Joint locations,Motion capture,Open source,Person detection,Pose detection,RGB-D data,Random decision forest,Real-time},
number = {1},
pages = {39--52},
publisher = {Elsevier Inc.},
title = {{An adaptable system for RGB-D based human body detection and pose estimation}},
url = {http://dx.doi.org/10.1016/j.jvcir.2013.03.011},
volume = {25},
year = {2014}
}
@inproceedings{Le2011,
abstract = {Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3\% and 75.8\% respectively, which are approximately 5\% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/\&\#x223C;wzou/},
author = {Le, Quoc V. and Zou, Will Y. and Yeung, Serena Y. and Ng, Andrew Y.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2011.5995496},
isbn = {9781457703942},
issn = {10636919},
pages = {3361--3368},
title = {{Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis}},
year = {2011}
}
@article{Zhang2012a,
abstract = {Recent advances in 3D depth cameras such as Microsoft Kinect sensors (www.xbox.com/en-US/kinect) have created many opportunities for multimedia computing. The Kinect sensor lets the computer directly sense the third dimension (depth) of the players and the environment. It also understands when users talk, knows who they are when they walk up to it, and can interpret their movements and translate them into a format that developers can use to build new experiences. While the Kinect sensor incorporates several advanced sensing hardware, this article focuses on the vision aspect of the Kinect sensor and its impact beyond the gaming industry.},
author = {Zhang, Zhengyou},
doi = {10.1109/MMUL.2012.24},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/Microsoft Kinect Sensor and Its Effect.pdf:pdf},
isbn = {1070-986X},
issn = {1070986X},
journal = {IEEE Multimedia},
keywords = {Microsoft Kinect,computer vision,human-computer interaction,motion capture,multimedia},
pages = {4--10},
title = {{Microsoft kinect sensor and its effect}},
volume = {19},
year = {2012}
}
@article{Barbosa2012,
author = {Barbosa, Igor Barros and Cristani, Marco and {Del Bue}, Alessio and Bazzani, Loris and Murino, Vittorio},
doi = {10.1007/978-3-642-33863-2\_43},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/datasets/Barbosa2012.pdf:pdf},
isbn = {9783642338625},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
keywords = {Kinect,RGB-D sensors,Re-identification},
pages = {433--442},
title = {{Re-identification with RGB-D sensors}},
volume = {7583},
year = {2012}
}
@article{StoyanovTodorandLouloudiAthanasiaandAndreassonHenrikandLilienthal2011,
abstract = {3D range sensing is one of the important topics in robotics, as it is often a component in vital autonomous subsystems like collision avoidance, mapping and semantic perception. The development of affordable, high frame rate and precise 3D range sensors is thus of considerable interest. Recent advances in sensing technology have produced several novel sensors that attempt to meet these requirements. This work is concerned with the development of a holistic method for accuracy evaluation of the measurements produced by such devices. A method for comparison of range sensor output to a set of reference distance measurements is proposed. The ap- proach is then used to compare the behavior of three integrated range sensing devices, to that of a standard actuated laser range sensor. Test cases in an uncontrolled indoor environment are performed in order to evaluate the sensors’ performance in a challenging, realistic application scenario.},
author = {{Stoyanov, Todor and Louloudi, Athanasia and Andreasson, Henrik and Lilienthal}, Achim J.},
journal = {Proceedings of the 5th European Conference on Mobile Robots, ECMR 2011},
pages = {19--24},
title = {{Comparative evaluation of range sensor accuracy in indoor environments}},
url = {http://www.aass.oru.se/Research/Learning/publications/2011/Stoyanov\_etal\_2011-ECMR11-Comparative\_Evaluation\_of\_Range\_Sensor\_Accuracy\_in\_Indoor\_Environments.pdf$\backslash$nhttp://oru.diva-portal.org/smash/record.jsf?pid=diva2:540987},
year = {2011}
}
@article{Nawi2013,
author = {Nawi, Nazri Mohd and Khan, Abdullah and Rehman, Mohammad Zubair},
file = {:Users/ben/Downloads/A New Back-propagation Neural Network optimized with Cuckoo Search Algorithm[ver 1].pdf:pdf},
journal = {Iccsa 2013},
keywords = {algorithm,and artificial bee colony,back propagation neural network,cuckoo search algorithm,local,minima},
pages = {413--426},
title = {{A New Back-Propagation Neural Network Optimized}},
year = {2013}
}
@inproceedings{Zhang2011a,
abstract = {Commodity depth cameras have created many interesting new applications in the research community recently. These applications often require the calibration information between the color and the depth cameras. Traditional checkerboard based calibration schemes fail to work well for the depth camera, since its corner features cannot be reliably detected in the depth image. In this paper, we present a maximum likelihood solution for the joint depth and color calibration based on two principles. First, in the depth image, points on the checker-board shall be co-planar, and the plane is known from color camera calibration. Second, additional point correspondences between the depth and color images may be manually specified or automatically established to help improve calibration accuracy. Uncertainty in depth values has been taken into account systematically. The proposed algorithm is reliable and accurate, as demonstrated by extensive experimental results on simulated and real-world examples.},
author = {Zhang, Cha and Zhang, Zhengyou},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2011.6012191},
isbn = {9781612843490},
issn = {19457871},
keywords = {calibration,depth camera},
title = {{Calibration between depth and color sensors for commodity depth cameras}},
year = {2011}
}
@misc{Seung1992a,
abstract = {Learning from examples in feedforward neural networks is studied within$\backslash$na statistical-mechanical framework. Training is assumed to be stochastic,$\backslash$nleading to a Gibbs distribution of networks characterized by a temperature$\backslash$nparameter T. Learning of realizable rules as well as of unrealizable$\backslash$nrules is considered. In the latter case, the target rule cannot be$\backslash$nperfectly realized by a network of the given architecture. Two useful$\backslash$napproximate theories of learning from examples are studied: the high-temperature$\backslash$nlimit and the annealed approximation. Exact treatment of the quenched$\backslash$ndisorder generated by the random sampling of the examples leads to$\backslash$nthe use of the replica theory. Of primary interest is the generalization$\backslash$ncurve, namely, the average generalization error ?g versus the number$\backslash$nof examples P used for training. The theory implies that, for a reduction$\backslash$nin ?g that remains finite in the large-N limit, P should generally$\backslash$nscale as ?N, where N is the number of independently adjustable weights$\backslash$nin the network. We show that for smooth networks, i.e., those with$\backslash$ncontinuously varying weights and smooth transfer functions, the generalization$\backslash$ncurve asymptotically obeys an inverse power law. In contrast, for$\backslash$nnonsmooth networks other behaviors can appear, depending on the nature$\backslash$nof the nonlinearities as well as the realizability of the rule. In$\backslash$nparticular, a discontinuous learning transition from a state of poor$\backslash$nto a state of perfect generalization can occur in nonsmooth networks$\backslash$nlearning realizable rules. We illustrate both gradual and continuous$\backslash$nlearning with a detailed analytical and numerical study of several$\backslash$nsingle-layer perceptron models. Comparing with the exact replica$\backslash$ntheory of perceptron learning, we find that for realizable rules$\backslash$nthe high-temperature and annealed theories provide very good approximations$\backslash$nto the generalization performance. Assuming this to hold for multilayer$\backslash$nnetworks as well, we propose a classification of possible asymptotic$\backslash$nforms of learning curves in general realizable models. For unrealizable$\backslash$nrules we find that the above approximations fail in general to predict$\backslash$ncorrectly the shapes of the generalization curves. Another indication$\backslash$nof the important role of quenched disorder for unrealizable rules$\backslash$nis that the generalization error is not necessarily a monotonically$\backslash$nincreasing function of temperature. Also, unrealizable rules can$\backslash$npossess genuine spin-glass phases indicative of degenerate minima$\backslash$nseparated by high barriers.},
author = {Seung, H. and Sompolinsky, H. and Tishby, N.},
booktitle = {Physical Review A},
doi = {10.1103/PhysRevA.45.6056},
isbn = {1050-2947 (Print)$\backslash$r1050-2947 (Linking)},
issn = {1050-2947},
number = {8},
pages = {6056--6091},
pmid = {9907706},
title = {{Statistical mechanics of learning from examples}},
volume = {45},
year = {1992}
}
@article{Krebs1985,
abstract = {Gait analysis, like all clinical assessments, is subject to measurement error. Specification of the extent of measurement error is imperative before drawing conclusions from any test. The purpose of this study was to determine the within-rater and between-rater reliability of observational gait analysis in a pediatric sample wearing knee-ankle-foot orthoses. Three expert observers, using a 3-point scale, rated videotaped gait kinematics of 15 children who had lower limb disability and who wore braces. The rating sessions were then repeated, with one month between sessions. Total agreement (identical ratings), both between-raters and within-raters, occurred in two-thirds of the observations, and an additional 29\% of the observations differed by one point. Between-rater intraclass correlation coefficient type 2, 1 was .73; within-rater Pearson product-moment correlation averaged .60. Observational kinematic gait analysis appears to be a convenient, but only moderately reliable, technique.},
author = {Krebs, D. E. and Edelstein, J. E. and Fishman, S.},
doi = {10.1016/0966-6362(95)99082-V},
isbn = {0031-9023 (Print)$\backslash$r0031-9023 (Linking)},
issn = {09666362},
journal = {Physical therapy},
pages = {1027--1033},
pmid = {3892553},
title = {{Reliability of observational kinematic gait analysis.}},
volume = {65},
year = {1985}
}
@misc{Knoop2009a,
abstract = {In this article, we present an approach for the fusion of 2d and 3d measurements for model-based person tracking, also known as Human Motion Capture. The applied body model is defined geometrically with generalized cylinders, and is set up hierarchically with connecting joints of different types. The joint model can be parameterized to control the degrees of freedom, adhesion and stiffness. This results in an articulated body model with constrained kinematic degrees of freedom. The fusion approach incorporates this model knowledge together with the measurements, and tracks the target body iteratively with an extended Iterative Closest Point (ICP) approach. Generally, the ICP is based on the concept of correspondences between measurements and model, which is normally exploited to incorporate 3d point cloud measurements. The concept has been generalized to represent and incorporate also 2d image space features. Together with the 3D point cloud from a 3d time-of-flight (ToF) camera, arbitrary features, derived from 2D camera images, are used in the fusion algorithm for tracking of the body. This gives complementary information about the tracked body, enabling not only tracking of depth motions but also turning movements of the human body, which is normally a hard problem for markerless human motion capture systems. The resulting tracking system, named VooDoo is used to track humans in a Human-Robot Interaction (HRI) context. We only rely on sensors on board the robot, i.e. the color camera, the ToF camera and a laser range finder. The system runs in realtime (∼20 Hz) and is able to robustly track a human in the vicinity of the robot. © 2008 Elsevier B.V. All rights reserved.},
author = {Knoop, Steffen and Vacek, Stefan and Dillmann, R\"{u}diger},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2008.10.017},
file = {:Users/ben/Documents/csMSc/project/gait/research/Fusion of 2d and 3d sensor data for articulated body tracking.html:html},
issn = {09218890},
keywords = {3D body model,Human motion capture,Human robot interaction,Sensor fusion,Time-of-flight},
pages = {321--329},
title = {{Fusion of 2d and 3d sensor data for articulated body tracking}},
volume = {57},
year = {2009}
}
@article{Ye2014,
abstract = {We present a system that allows the user to virtually try on new clothes. It uses a single commodity depth camera to capture the user in 3D. Both the pose and the shape of the user are estimated with a novel real-time template-based approach that performs tracking and shape adaptation jointly. The result is then used to drive realistic cloth simulation, in which the synthesized clothes are overlayed on the input image. The main challenge is to handle missing data and pose ambiguities due to the monocular setup, which captures less than 50 percent of the full body. Our solution is to incorporate automatic shape adaptation and novel constraints in pose tracking. The effectiveness of our system is demonstrated with a number of examples.},
author = {Ye, M and Yang, R},
doi = {10.1109/CVPR.2014.301},
file = {:Users/ben/Documents/csMSc/project/gait/research/v3/Ye2014.pdf:pdf},
isbn = {978-1-4799-5118-5},
journal = {Multi-Camera \ldots},
keywords = {-human motion capture,figure 1,mation,non-rigid surface defor-,our approach tracks both,pose estimation,pose of humans,results with three different,simultaneously,standard datasets are shown,the shape and the},
number = {January},
title = {{Real-time Simultaneous Pose and Shape Estimation for Articulated Objects Using a Single Depth Camera}},
url = {http://vis.uky.edu/~gravity/Publications/2014/ArticulatedPoseShape\_CVPR14.pdf$\backslash$nhttp://europepmc.org/abstract/MED/24650982$\backslash$nhttp://books.google.com/books?hl=en\&lr=\&id=XA\_6o2dhTGEC\&oi=fnd\&pg=PA335\&dq=Real-Time+3D+Body+Pose+Estimation\&ots=\_X-Zs47meG\&sig=I-iy},
year = {2014}
}
@article{Shen2014,
author = {Shen, Yinghua and Li, Jinghua and L\"{u}, Chaohui},
file = {:Users/ben/Downloads/Depth Map Enhancement Method Based on Joint.pdf:pdf},
isbn = {9781479958351},
journal = {International Congress on Image and Signal Processing},
keywords = {- kinect,depth map,essential to fill in,method,original depth maps before,real-time processing,the holes of the,weighted joint bilateral},
pages = {153--158},
title = {{Depth Map Enhancement Method Based on Joint Bilateral Filter}},
year = {2014}
}
@inproceedings{Pishchulin2012,
abstract = {State-of-the-art methods for human detection and pose estimation require many training samples for best performance. While large, manually collected datasets exist, the captured variations w.r.t. appearance, shape and pose are often uncontrolled thus limiting the overall performance. In order to overcome this limitation we propose a new technique to extend an existing training set that allows to explicitly control pose and shape variations. For this we build on recent advances in computer graphics to generate samples with realistic appearance and background while modifying body shape and pose. We validate the effectiveness of our approach on the task of articulated human detection and articulated pose estimation. We report close to state of the art results on the popular Image Parsing [25] human pose estimation benchmark and demonstrate superior performance for articulated human detection. In addition we define a new challenge of combined articulated human detection and pose estimation in real-world scenes.},
author = {Pishchulin, Leonid and Jain, Arjun and Andriluka, Mykhaylo and Thormahlen, Thorsten and Schiele, Bernt},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6248052},
isbn = {9781467312264},
issn = {10636919},
pages = {3178--3185},
title = {{Articulated people detection and pose estimation: Reshaping the future}},
year = {2012}
}
@inproceedings{Kitsunezaki2013,
abstract = {This report presents the results of KINECT applications used in physical rehabilitation tests. Aoyama Gakuin and Kitasato universities collaborated on this project, which is supported by SCOPE. The applications, following standard tests, are for the timed “Up \& Go Test”, the timed “10-Meter Walk Test” and for a Joint “Range of Motion” Measurement”; test results are given. The implementation, evaluation and advantages of a proposed “Real-time ROM Measurement” are also given. The proposed KINECT application will be useful for enhancement of KINECT technical capabilities and for further advancements in medical care.},
author = {Kitsunezaki, Naofumi and Adachi, Eijiro and Masuda, Takashi and Mizusawa, Jun Ichi},
booktitle = {MeMeA 2013 - IEEE International Symposium on Medical Measurements and Applications, Proceedings},
doi = {10.1109/MeMeA.2013.6549755},
isbn = {9781467351966},
keywords = {Joint Range of Motion Measurement,Kinect,Physical Rehabilitation,ROM styling},
pages = {294--299},
title = {{KINECT applications for the physical rehabilitation}},
year = {2013}
}
@article{Bo2012,
abstract = {Recently introduced RGB-D cameras are capable of providing high quality synchronized videos of both color and depth. With its advanced sensing capabilities, this technology represents an opportunity to dramatically increase the capabilities of object recognition. It also raises the problem of developing expressive features for the color and depth channels of these sensors. In this paper we introduce hierarchical matching pursuit (HMP) for RGB-D data. HMP uses sparse coding to learn hierarchical feature representations from raw RGB-D data in an unsupervised way. Extensive experiments on various datasets indicate that the features learned with our approach enable superior object recognition results using linear support vector machines.},
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
doi = {10.1007/978-3-319-00065-7},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Bo2012.pdf:pdf},
isbn = {978-3-319-00064-0},
issn = {21530858},
journal = {ISER, June},
pages = {1--15},
title = {{Unsupervised feature learning for rgb-d based object recognition}},
url = {http://homes.cs.washington.edu/~lfb/paper/iser12.pdf},
year = {2012}
}
@article{Lecun2006,
author = {Lecun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc Aurelio and Huang, Fu Jie},
file = {:Users/ben/Documents/csMSc/project/gait/research/26$\backslash$:03$\backslash$:15 send/A Tutorial on Energy-Based Learning Yann LeCun.pdf:pdf},
pages = {1--59},
title = {{A Tutorial on Energy-Based Learning}},
year = {2006}
}
@article{Zhang2011,
abstract = {Recognizing human activities from common color image sequences faces many challenges, such as complex backgrounds, camera motion, and illumination changes. In this paper, we propose a new 4-dimensional (4D) local spatio-temporal feature that combines both intensity and depth information. The feature detector applies separate filters along the 3D spatial dimensions and the 1D temporal dimension to detect a feature point. The feature descriptor then computes and concatenates the intensity and depth gradients within a 4D hyper cuboid, which is centered at the detected feature point, as a feature. For recognizing human activities, Latent Dirichlet Allocation with Gibbs sampling is used as the classifier. Experiments are performed on a newly created database that contains six human activities, each with 33 samples with complex variations. Experimental results demonstrate the promising performance of the proposed features for the task of human activity recognition.},
author = {Zhang, H. and Parker, L.E.},
doi = {10.1109/IROS.2011.6094489},
file = {:Users/ben/Documents/csMSc/project/gait/research/4-Dimensional Local Spatio-Temporal Features for Human Activity Recognition.pdf:pdf},
isbn = {978-1-61284-456-5},
issn = {2153-0858},
journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
keywords = {3D spatial dimensions,4D hyper cuboid,4D local spatiotemporal features,Cameras,Databases,Feature extraction,Gibbs sampling,Humans,Three dimensional displays,Vectors,Videos,camera motion,color image sequences,complex backgrounds,depth gradients,gradient methods,human activity recognition,illumination changes,image colour analysis,image recognition,image sequences,latent dirichlet allocation,robot vision},
pages = {2044--2049},
title = {{4-Dimensional Local Spatio-Temporal Features for Human Activity Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6094489},
year = {2011}
}
@article{Kavukcuoglu2010a,
abstract = {We propose an unsupervisedmethod for learningmulti-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall repre- sentation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efficient feed-forward encoder that predicts quasi- sparse features from the input. While patch-based training rarely produces any- thing but oriented edge detectors, we show that convolutional training produces highly diverse filters, including center-surround filters, corner detectors, cross de- tectors, and oriented grating detectors. We show that using these filters in multi- stage convolutional network architecture improves performance on a number of visual recognition and detection tasks.},
author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-Lan and Gregor, Karol and Mathieu, Michael and LeCun, Yann},
isbn = {9781617823800},
issn = {-},
journal = {Advances in neural information processing systems 23},
pages = {1090--1098},
title = {{Learning Convolutional Feature Hierarchies for Visual Recognition}},
year = {2010}
}
@article{Zeiler2013,
abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3557v1},
author = {Zeiler, Matthew and Fergus, Rob},
eprint = {arXiv:1301.3557v1},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/zieler2013.pdf:pdf},
journal = {arXiv preprint arXiv:1301.3557},
pages = {1--9},
title = {{Stochastic pooling for regularization of deep convolutional neural networks}},
url = {http://arxiv.org/abs/1301.3557},
year = {2013}
}
@inproceedings{Plagemann2010,
abstract = {We deal with the problem of detecting and identifying body parts in depth images at video frame rates. Our solution involves a novel interest point detector for mesh and range data that is particularly well suited for analyzing human shape. The interest points, which are based on identifying geodesic extrema on the surface mesh, coincide with salient points of the body, which can be classified as, e.g., hand, foot or head using local shape descriptors. Our approach also provides a natural way of estimating a 3D orientation vector for a given interest point. This can be used to normalize the local shape descriptors to simplify the classification problem as well as to directly estimate the orientation of body parts in space. Experiments involving ground truth labels acquired via an active motion capture system show that our interest points in conjunction with a boosted patch classifier are significantly better in detecting body parts in depth images than state-of-the-art sliding-window based detectors.},
author = {Plagemann, Christian and Ganapathi, Varun and Koller, Daphne and Thrun, Sebastian},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509559},
file = {:Users/ben/Documents/csMSc/project/gait/research/Real-time Identification and Localization of Body Parts from Depth Images.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
pages = {3108--3113},
title = {{Real-time identification and localization of body parts from depth images}},
year = {2010}
}
@article{Rognvaldsson2012,
author = {R\"{o}gnvaldsson, Thorsteinn S.},
doi = {10.1007/978-3-642-35289-8-6},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/weightDecay.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {69--89},
title = {{A Simple trick for estimating the weight decay parameter}},
volume = {7700},
year = {2012}
}
@article{Whittle1996,
abstract = {Gait analysis has now advanced to the point where it is used as a routine part of patient management in certain centers. It is best thought of as a special investigation, which is used together with the history, physical examination and other special investigations to perform a detailed assessment of a patient with a walking disorder. Clinical gait analysis usually consists of 5 elements: videotape examination, measurement of general gait parameters, kinematic analysis, kinetic measurement, and electromyography (EMG). Kinematic measurements are usually made by television cameras, linked into a computer, which define the movements of the major joints of the lower limb in 3 dimensions. The primary kinetic measurement is that of the force beneath each foot while walking. By combining kinematic and kinetic data, it is possible to calculate the joint moments and powers, again in 3 dimensions. The joint angle, moment and power, and the EMG from specific muscles, provide a detailed description of the mechanics of gait. Such information enables much better decisions to be reached on the best way to treat the patient. The primary diagnosis which can be aided by gait analysis is cerebral palsy, but the technique is also useful in a number of other conditions},
author = {Whittle, M.},
doi = {10.1016/0167-9457(96)00006-1},
isbn = {0167-9457},
issn = {01679457},
journal = {Human Movement Science},
keywords = {00 copyright 0 1996,00006,0167 9457,1 423 7554046,1 423 7852215,15,96,cecasun,cerebral palsy,e mail,edu,elsevier,fax,gait,gait analysis,locomotion,mwhittle,pii so1 67 9457,tel,utc,walking},
pages = {369--387},
title = {{Clinical gait analysis: A Review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0167945796000061},
volume = {15},
year = {1996}
}
@article{Viterbi1967,
abstract = { The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above<tex>R\_\{0\}</tex>, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above<tex>R\_\{0\}</tex>and whose performance bears certain similarities to that of sequential decoding algorithms.},
author = {Viterbi, A.},
doi = {10.1109/TIT.1967.1054010},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {2},
title = {{Error bounds for convolutional codes and an asymptotically optimum decoding algorithm}},
volume = {13},
year = {1967}
}
@article{Johnson2010,
abstract = {We investigate the task of 2D articulated human pose estimation in unconstrained still images. This is extremely challenging because of variation in pose, anatomy, clothing, and imaging conditions. Current methods use simple models of body part appearance and plausible configurations due to limitations of available training data and constraints on computational expense. We show that such models severely limit accuracy. Building on the successful pictorial structure model (PSM) we propose richer models of both appearance and pose, using state-of-the-art discriminative classifiers without introducing unacceptable computational expense. We introduce a new annotated database of challenging consumer images, an order of magnitude larger than currently available datasets, and demonstrate over 50\% relative improvement in pose estimation accuracy over a stateof- the-art method.},
author = {Johnson, Sam and Everingham, Mark},
doi = {10.5244/C.24.12},
file = {:Users/ben/Documents/csMSc/project/gait/research/Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation.pdf:pdf},
isbn = {1-901725-40-5},
journal = {Procedings of the British Machine Vision Conference 2010},
number = {i},
pages = {12.1--12.11},
title = {{Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation}},
url = {http://www.bmva.org/bmvc/2010/conference/paper12/index.html},
year = {2010}
}
@article{Chen2014,
author = {Chen, Qifeng},
doi = {10.1109/CVPR.2014.500},
file = {:Users/ben/Downloads/Fast MRF Optimization with Application to Depth Reconstruction.pdf:pdf},
isbn = {978-1-4799-5118-5},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Fast MRF Optimization with Application to Depth Reconstruction}},
volume = {1063},
year = {2014}
}
@article{Hinton2012a,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
journal = {arXiv: 1207.0580},
pages = {1--18},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Li2014,
author = {Li, Sijin and Chan, Antoni B},
doi = {10.1109/CVPRW.2014.78},
file = {:Users/ben/Documents/csMSc/project/gait/research/cnns/Li2014.pdf:pdf},
isbn = {9781479943081},
journal = {CPVR2014},
title = {{Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network}},
year = {2014}
}
@inproceedings{Sun2012,
abstract = {Random forests have been successfully applied to various high level computer vision tasks such as human pose estimation and object segmentation. These models are extremely efficient but work under the assumption that the output variables (such as body part locations or pixel labels) are independent. In this paper, we present a conditional regression forest model for human pose estimation that incorporates dependency relationships between output variables through a global latent variable while still maintaining a low computational cost. We show that the incorporation of a global latent variable encoding torso orientation, or human height, etc., can dramatically increase the accuracy of body joint location prediction. Our model also allows efficient and seamless incorporation of prior knowledge about the problem instance such as the height or orientation of the human subject which can be available from the problem context or via a temporal model. We show that our method significantly outperforms state-of-the-art methods for pose estimation from depth images. The conditional regression model proposed in the paper is general and can be applied to other problems where random forests are used.},
author = {Sun, Min and Kohli, Pushmeet and Shotton, Jamie},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6248079},
isbn = {9781467312264},
issn = {10636919},
pages = {3394--3401},
title = {{Conditional regression forests for human pose estimation}},
year = {2012}
}
@inproceedings{Yang2012,
abstract = {This paper proposes a depth hole filling method for RGBD images obtained from the Microsoft Kinect sensor. First, the proposed method labels depth holes based on 8-connectivity. For each labeled depth hole, the proposed method fills depth hole using the depth distribution of neighboring pixels of the depth hole. Then, we refine the hole filling result with cross-bilateral filtering. In experiments, by simply using the depth distribution of neighboring pixels, the proposed method improves the acquired depth map and reduces false filling caused by incorrect depth-color fusion.},
author = {Yang, Na Eun and Kim, Yong Gon and Park, Rae Hong},
booktitle = {2012 IEEE International Conference on Signal Processing, Communications and Computing, ICSPCC 2012},
doi = {10.1109/ICSPCC.2012.6335696},
file = {:Users/ben/Downloads/06335696 (1).pdf:pdf},
isbn = {9781467321938},
issn = {1467321923},
keywords = {Kinect,cross-bilateral filtering,depth hole filling,depth map enhancement},
pages = {658--661},
title = {{Depth hole filling using the depth distribution of neighboring regions of depth holes in the Kinect sensor}},
year = {2012}
}
@article{Matsuo2013,
author = {Matsuo, Takuya and Kodera, Naoki},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/kinect holes/Depth Map Refinement Using Reliability Based Joint Trilateral Filter.pdf:pdf},
journal = {ECTI Transactions on Computer and Information Technology},
keywords = {depth mapn,filteringjoint trilateral filtering,joint trilateral filteringn,matching,post,refinement filter,stereo,stereo matching},
number = {2},
pages = {108--117},
title = {{Depth Map Refinement Using Reliability Based Joint Trilateral Filter}},
volume = {7},
year = {2013}
}
@article{Chen2013,
abstract = {Analysis of human behaviour through visual information has been a highly active research topic in the computer vision community. This was previously achieved via images from a conventional camera, however recently depth sensors have made a new type of data available. This survey starts by explaining the advantages of depth imagery, then describes the new sensors that are available to obtain it. In particular, the Microsoft Kinect has made high-resolution real-time depth cheaply available. The main published research on the use of depth imagery for analysing human activity is reviewed. Much of the existing work focuses on body part detection and pose estimation. A growing research area addresses the recognition of human actions. The publicly available datasets that include depth imagery are listed, as are the software libraries that can acquire it from a sensor. This survey concludes by summarising the current state of work on this topic, and pointing out promising future research directions. For both researchers and practitioners who are familiar with this topic and those who are new to this field, the review will aid in the selection, and development, of algorithms using depth data. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Chen, Lulu and Wei, Hong and Ferryman, James},
doi = {10.1016/j.patrec.2013.02.006},
file = {:Users/ben/Documents/csMSc/project/gait/research/journalsFromOutline/attachments/A survey of human motion analysis using depth imagery.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {3D body model,Depth sensor,Human action recognition,Human pose estimation,Range data,Survey},
number = {15},
pages = {1995--2006},
publisher = {Elsevier B.V.},
title = {{A survey of human motion analysis using depth imagery}},
url = {http://dx.doi.org/10.1016/j.patrec.2013.02.006},
volume = {34},
year = {2013}
}
@article{DanielHerrera2012a,
abstract = {We present an algorithm that simultaneously calibrates two color cameras, a depth camera, and the relative pose between them. The method is designed to have three key features: accurate, practical, and applicable to a wide range of sensors. The method requires only a planar surface to be imaged from various poses. The calibration does not use depth discontinuities in the depth image, which makes it flexible and robust to noise. We apply this calibration to a Kinect device and present a new depth distortion model for the depth sensor. We perform experiments that show an improved accuracy with respect to the manufacturer's calibration.},
author = {Herrera, Daniel C. and Kannala, Juho and Heikkil\"{a}, Janne},
doi = {10.1109/TPAMI.2012.125},
file = {:Users/ben/Documents/csMSc/project/gait/research/kinect/Joint depth and color camera calibration with distortion correction.pdf:pdf},
isbn = {0162-8828 VO - 34},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Camera calibration,Kinect,camera pair,depth camera,distortion},
number = {10},
pages = {2058--2064},
pmid = {22641701},
title = {{Joint depth and color camera calibration with distortion correction}},
volume = {34},
year = {2012}
}
