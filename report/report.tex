\documentclass[11pt]{article} % Try also "scrartcl" or "paper"
\usepackage{helvet}
\linespread{1}
 \usepackage[margin=1.2cm]{geometry}   % to change margins
 \usepackage{titling}             % Uncomment both to   
 \setlength{\droptitle}{-2.3cm}     % change title position 
\usepackage[affil-it]{authblk}
\usepackage{bibentry}
\usepackage{cite}
\usepackage{graphicx,bm,times,subfig,amsmath,amsfonts,listings,url}
\usepackage{color}
\usepackage[page]{appendix}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\author{Ben Crabbe}
\affil{University of Bristol, UK}
\title{%\vspace{-1.5cm}            % Another way to do
Java Graphics Report}
\begin{document}


\maketitle


\lstset{language=Java}
\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=Java,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{mymauve},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  language=Java,                 % the language of the code
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  %stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}


\section{Introduction}

\begin{figure}
\centering\includegraphics*[width=0.4\linewidth,clip]{NNfig1}\caption{The illustration of equations 1,2,3 and 4. 
%\end{equation}
\label{NNfig1}  } \end{figure}

For my research project I am working with neural networks using a high level framework. To get some experience before I begin properly I have decided to use this assignment to explore some of the issues with training neural networks. I hope it will also provide some good OO design practise.


\section{Forward Propagation}

A neural network consists of layers of one or more units or 'neurons' as shown in figure \ref{NNfig1}. Each neuron, $i$,  recieves inputs, $x_i$, from each of the neurons in the layer immediate below it. It computes a weighted sum of these inputs plus a bias term, $b_i$, and applies some non linear 'activation' function, $f(.)$, producing that neurons output, $a_i$. The weights in these sums can be thought of as the connection strength between two neurons, these and the bias terms are the adjustable parameters.
\begin{align}
\centering
a_1^{(2)} &= f(W_{11}^{(2)}x_1 + W_{12}^{(2)} x_2 + W_{13}^{(2)} x_3 + b_1^{(2)})  \\
a_2^{(2)} &= f(W_{21}^{(2)}x_1 + W_{22}^{(2)} x_2 + W_{23}^{(2)} x_3 + b_2^{(2)})  \\
a_3^{(2)} &= f(W_{31}^{(2)}x_1 + W_{32}^{(2)} x_2 + W_{33}^{(2)} x_3 + b_3^{(2)})  \\
h_{W,b}(x) &= a_1^{(3)} =  f(W_{11}^{(3)}a_1^{(2)} + W_{12}^{(3)} a_2^{(2)} + W_{13}^{(3)} a_3^{(2)} + b_1^{(3)}) 
\end{align}
in general, and in vector form:
\begin{align}
\centering
\boldsymbol a^{(l)} = 
\begin{bmatrix}
f(\boldsymbol W^{(l)}_1\cdot \boldsymbol a^{(l-1)} +b^{(l)}_1)\\ 
f(\boldsymbol W^{(l)}_2\cdot \boldsymbol a^{(l-1)} +b^{(l)}_2)\\ 
\vdots\\ 
f(\boldsymbol W^{(l)}_n\cdot \boldsymbol a^{(l-1)} +b^{(l)}_n)\\
\end{bmatrix}
\end{align}

To implement these I have made these classes:
\begin{itemize}
\item{Driver: a simple initialiser class with some static testing methods. It also contains a random number generator, I thought it was better to have 1 of these in a static variable rather than creating one each time it was needed. }
\item{Neuron: contains a vector of connection weights}
\item{HiddenLayer:  contains a list of neurons. Has a method "FloatMatrix getOutput(FloatMatrix input)" which computes equations 1, 2, 3, 4 etc.}
\item{InputLayer:  I have made this class to handle producing inputs, to start with I want to examine the networks ability to produce a sine function, so there is a function generateRandomSinSample which produces a random number between $-\pi/2$ and $\pi/2$. I thought }
\item{Network: contains the an InputLayer and a list of HiddenLayers. Has a function "FloatMatrix computeFowardPass(FloatMatrix input)" which computes the whole forward propagation taking input through the network returning the output.}
\end{itemize}
 
FloatMatrix is a class from the jblas\footnote{http://jblas.org/} library. I have used these for every vector since they provide optimised computations which should speed up the network. 

One tricky issue is how best to deal with the bias' in each neuron, it is essentially the same as a weight but its input value does not come from the layer below, instead it is a constant. One way that is illustrated in figure \ref{NNfig1} is to add a constant term to the input of each layer, then we can just treat the bias as an extra weight. Therefore each layer has numberOfNeurons in the previous layer + 1 weights. When calculating the output of each layer with the getOutput method:
\begin{lstlisting}
    FloatMatrix getOutput(FloatMatrix input)
    {
        if(input.length!=numberOfInputs) {
            throw new Error("ERROR: input vector to layer" + layerNumber +
             " is not correct size. expected: " +
            numberOfInputs + " got: " + input.length);
        }
        FloatMatrix inputPlusBiasConstant = addConstantTermToInputVector(input);
        
        FloatMatrix outputVector = new FloatMatrix(neurons.size());
        int i=0;
        for(Neuron n: neurons) {
            outputVector.put(i, 0,
            activationFunction(n.getWeightVector().dot(inputPlusBiasConstant)));
            ++i;
        }
        return outputVector;
    }
\end{lstlisting}
we add an additional element to input vector with     "private FloatMatrix addConstantTermToInputVector(FloatMatrix input)" and everything else is taken care of.

Issues were ironed  out by writing this test: 

\begin{lstlisting}
    void testsNetworkDefinition(int... definition)
    {
        System.out.println("Network initialised with layers: " + Arrays.toString(layerWidths) );
       
        System.out.println("\n\nLAYER 0");
        Driver.is(inputLayer.getWidth(), definition[0],
        "checking the sizes of the layers and the number of inputs they should recieve correct");
        int i=1;
        for(HiddenLayer l: hiddenLayers) {
            System.out.println("\n\nLAYER " + i);

            Driver.is(l.getWidth(),definition[i],"has correct number of neurons");
            Driver.is(l.getNumberOfInputs(),definition[i-1],"has the correct number of connections");
            ++i;
        }
        System.out.println("\n\nLAYER 0");
        Driver.is(inputLayer.getInput().length, definition[0],
        "is the size of the InputLayer.getInput() returned vector correct");
        Driver.is(hiddenLayers.get(0).getNumberOfInputs(), inputLayer.getInput().length,
        "is the size of the input vector the size expected by the first layer.");
        
        System.out.println("\n\nLAYER 1");
        FloatMatrix input = inputLayer.getInput();
        FloatMatrix firstLayerOutput = hiddenLayers.get(0).getOutput(input);
        Driver.is(firstLayerOutput.length, definition[1],
        "is the output of the first hidden layer the correct size");
        
        i=2;
        FloatMatrix input2forTest;
        input.copy(firstLayerOutput);
        while(i<definition.length) {
            System.out.println("\n\nLAYER " + hiddenLayers.get(i-1).getLayerNumber());
            Driver.is(input.length, hiddenLayers.get(i-1).getNumberOfInputs(),
            "is the output of the previous layer correctly sized");
            input = hiddenLayers.get(i-1).getOutput(input);
            ++i;
        }
    }

\end{lstlisting}

Here is the full code at this stage:
\subsection{Driver}
\begin{lstlisting}

import java.util.*;
/**
Driver

Initialisation class
 
*/
class Driver
{
    static Random randomNumberGen;
    static int numberOfFails;
    
    Driver()
    {
        this.randomNumberGen = new Random();
        numberOfFails=0;
    }
    
    static void is(Object x, Object y)
    {
        System.out.print("testing: " + x.toString() + " = " + y.toString() );

        if (x==y || (x != null && x.equals(y)) ) {
            System.out.println("...pass");
            return;
        }
        System.out.println("...fail");
    }
    
    static void is(Object x, Object y, String description)
    {
        System.out.println("test description: " + description );

        System.out.print("testing: " + x.toString() + " = " + y.toString() );

        if (x==y || (x != null && x.equals(y)) ) {
            System.out.println("...pass");
            return;
        }
        System.out.println("...fail");
        ++numberOfFails;
    }
    
    static void finishTesting(String suiteName)
    {
        System.out.println("\n\n" + suiteName + " finished with " + numberOfFails + " fails.");
        System.out.println("***************************************************************\n\n");

        numberOfFails=0;
    }
    
    static void tests()
    {
        float randomX = Driver.randomNumberGen.nextFloat()*(float)Math.PI;
        System.out.println("x = " + randomX);
        System.out.println("y = " + (float)Math.sin(randomX));

    }

    public static void main(String[] args)
    {
        Driver program = new Driver();
        Network net = new Network(1,5,4,1);
        //Trainer trainer = new Trainer(Network);
    }
}
\end{lstlisting}


\subsection{HiddenLayer}
\begin{lstlisting}
import org.jblas.*;
import java.util.*;
/**
Layers
    
 
*/

class HiddenLayer
{
    private int numberOfInputs;
    private List<Neuron> neurons;
    private int layerNumber;

    HiddenLayer(int layerNumber, int numberOfNeurons, int numberOfInputs)
    {
        this.layerNumber = layerNumber;
        this.numberOfInputs = numberOfInputs;
        neurons = new ArrayList<Neuron>();
        for(int i=1; i<=numberOfNeurons; ++i) {
            neurons.add(new Neuron(numberOfInputs));
        }
    }
    
    FloatMatrix getOutput(FloatMatrix input)
    {
        if(input.length!=numberOfInputs) {
            System.out.println("ERROR: input vector to layer" + layerNumber +
             " is not correct size. expected: " +
            numberOfInputs + " got: " + input.length);
            throw new Error();
        }
        FloatMatrix inputPlusBiasConstant = addConstantTermToInputVector(input);
        
        FloatMatrix outputVector = new FloatMatrix(neurons.size());
        int i=0;
        for(Neuron n: neurons) {
            outputVector.put(i, 0,
            activationFunction(n.getWeightVector().dot(inputPlusBiasConstant)));
            ++i;
        }
        return outputVector;
    }
    
    //rectified linear unit (ReLU) non linear activation applied on each neuron
    private float activationFunction(float z)
    {
        if(z>0) return z;
        else return 0;
    }
    
    private FloatMatrix addConstantTermToInputVector(FloatMatrix input)
    {
        FloatMatrix inputPlusBiasConstant = new FloatMatrix(numberOfInputs+1);
        inputPlusBiasConstant.put(0,0,-1);
        for(int i=1; i<=numberOfInputs; ++i) {
            inputPlusBiasConstant.put(i,0,input.get(i-1,0));
        }
        return inputPlusBiasConstant;
    }
    
    int getWidth()
    {
        return neurons.size();
    }
    
    int getNumberOfInputs()
    {
        return numberOfInputs;
    }
    
    int getLayerNumber()
    {
        return layerNumber;
    }
    
    static void tests()
    {

    }

    public static void main(String[] args)
    {
      HiddenLayer test = new HiddenLayer(1,3,2);
    }
}
\end{lstlisting}


\subsection{Neuron}

\begin{lstlisting}
import org.jblas.*;
import java.util.*;

/** Each neuron, i, in a layer, l, with l-1 having n neurons has a
    connection vector 
        W_i^l = [ b, W_1i, W_2i, ..., W_ni]
    where W_ni is the connection to unit n in l-1
    and b is a bias term.
*/
class Neuron
{
    private int numberOfConnections;
    private FloatMatrix weights;//numberOfConnections+1 for bias
    
    Neuron(int numberOfInputs)
    {
        //there are n+1 connections. to n units in prev layer and a bias
        numberOfConnections=numberOfInputs+1;
        weights = FloatMatrix.rand(numberOfConnections);
    }

    FloatMatrix getWeightVector()
    {
        return weights.dup();
    }
    
    void tests()
    {
        System.out.println(weights.toString());

    }

    public static void main(String[] args)
    {
        Neuron n = new Neuron(3);
        n.tests();
    }
}

\end{lstlisting}

\subsection{InputLayer}
\begin{lstlisting}
import org.jblas.*;
import java.util.*;


class InputLayer
{
    private int width;

    InputLayer(int width)
    {
        this.width = width;
    }
    
    FloatMatrix getInput()
    {
        return generateRandomSinSample();
    }
    
    private FloatMatrix generateRandomSinSample()
    {
        float randomX = Driver.randomNumberGen.nextFloat()*(float)Math.PI-((float)Math.PI/2);
        //float randomY = (float)Math.sin(randomX);
        /*FloatMatrix(int newRows, int newColumns, float... newData)
          Create a new matrix with newRows rows, newColumns columns using newData> as the data.
        */
        return new FloatMatrix(1,1,randomX);
    }
    
    int getWidth()
    {
        return width;
    }
    
    private void tests()
    {
        float randomX = Driver.randomNumberGen.nextFloat()*(float)Math.PI-((float)Math.PI/2);
        System.out.println("x = " + randomX);
        System.out.println("y = " + (float)Math.sin(randomX));

    }
    
    public static void main(String[] args)
    {
        InputLayer il = new InputLayer(3);
        il.tests();
        
    }
}
\end{lstlisting}

\subsection{Network}
\begin{lstlisting}
import org.jblas.*;
import java.util.*;

class Network
{
    private InputLayer inputLayer;
    private List<HiddenLayer> hiddenLayers;
    private int[] layerWidths;
        
    Network(int...definition)
    {
        layerWidths = definition.clone();
        inputLayer = new InputLayer(layerWidths[0]);
        hiddenLayers = new ArrayList<HiddenLayer>();
        for(int i=1; i<definition.length; ++i) {
            hiddenLayers.add(new HiddenLayer(i, layerWidths[i], layerWidths[i-1]));
        }
    }
    
    FloatMatrix computeFowardPass(FloatMatrix input)
    {
        //FloatMatrix input = inputLayer.getInput();
        //System.out.println("input: " + input.toString());
        int i=1;
        for(HiddenLayer l: hiddenLayers) {
            input = l.getOutput(input);
            //System.out.println("layer " + i + ": " + input.toString());
            ++i;
        }
        return input;
    }
    
    void testsNetworkDefinition(int... definition)
    {
        System.out.println("Network initialised with layers: " + Arrays.toString(layerWidths) );
       
        System.out.println("\n\nLAYER 0");
        Driver.is(inputLayer.getWidth(), definition[0],
        "checking the sizes of the layers and the number of inputs they should recieve correct");
        int i=1;
        for(HiddenLayer l: hiddenLayers) {
            System.out.println("\n\nLAYER " + i);

            Driver.is(l.getWidth(),definition[i],"has correct number of neurons");
            Driver.is(l.getNumberOfInputs(),definition[i-1],"has the correct number of connections");
            ++i;
        }
        System.out.println("\n\nLAYER 0");
        Driver.is(inputLayer.getInput().length, definition[0],
        "is the size of the InputLayer.getInput() returned vector correct");
        Driver.is(hiddenLayers.get(0).getNumberOfInputs(), inputLayer.getInput().length,
        "is the size of the input vector the size expected by the first layer.");
        
        System.out.println("\n\nLAYER 1");
        FloatMatrix input = inputLayer.getInput();
        FloatMatrix firstLayerOutput = hiddenLayers.get(0).getOutput(input);
        Driver.is(firstLayerOutput.length, definition[1],
        "is the output of the first hidden layer the correct size");
        
        i=2;
        FloatMatrix input2forTest;
        input.copy(firstLayerOutput);
        while(i<definition.length) {
            System.out.println("\n\nLAYER " + hiddenLayers.get(i-1).getLayerNumber());
            Driver.is(input.length, hiddenLayers.get(i-1).getNumberOfInputs(),
            "is the output of the previous layer correctly sized");
            input = hiddenLayers.get(i-1).getOutput(input);
            ++i;
        }
      //  System.out.println("forward pass: " + computeFowardPass().toString());
    }

   public static void main(String[] args)
   {
        Driver d = new Driver();
        Network net =  new Network(1,5,1);
        net.testsNetworkDefinition(1,5,1);
       
        Network net2 =  new Network(1,9,8,7,3,7);
        net2.testsNetworkDefinition(1,9,8,7,3,7);
        Driver.finishTesting("testsNetworkDefinition");
   }
}
\end{lstlisting}


\section{Training}
I am interested in training the network using backpropagation and (stochastic) gradient descent. To handle the training I have made a Trainer class. The Trainer should be able to train any number of networks therefore the network should a member of it rather than the other way around.

The network is presented with an example consisting of input, $x$, and expected output, $y$. We compute the actual output of the network, $h_{W,b}(x)$ (the network is initialised with small random weights) and calculate the error as
\begin{align}
J(W,b; x,y) = \frac{1}{2} \left\| h_{W,b}(x) - y \right\|^2.
\end{align}
we then update each weight and bias by
\begin{align}
W_{ij}^{(l)} &= W_{ij}^{(l)} - \alpha \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b) \\
b_{i}^{(l)} &= b_{i}^{(l)} - \alpha \frac{\partial}{\partial b_{i}^{(l)}} J(W,b)
\end{align}
to compute the values of $\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b)$ and $\frac{\partial}{\partial b_{i}^{(l)}} J(W,b)$ we use backpropagation. To quote its description from 

\url{http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/}

\begin{displayquote}
The intuition behind the backpropagation algorithm is as follows. Given a training example $(x,y)$, we will first run a “forward pass” to compute all the activations throughout the network, including the output value of the hypothesis  $h_{W,b}(x)$. Then, for each node(neuron) $i$ in layer $l$, we would like to compute an “error term”  $\delta_i^{(l)}$ that measures how much that node was “responsible” for any errors in our output. For an output node, we can directly measure the difference between the network’s activation and the true target value, and use that to define $\delta_i^{(n_l)}$ (where layer $n_l$ is the output layer). For the hidden units we will compute $\delta_i^{(l)}$ based on a weighted average of the error terms of the nodes that uses $a^{(l)}_i$ as an input.
\end{displayquote}


This is the vectorised algorithm:
\begin{itemize}

\item[1.]{Perform a feedforward pass, computing the activations for layers $L_2, L_3,$ up to the output layer $L_{n_l}$, using the equations defining the forward propagation steps}

\item[2.]{For the output layer (layer $n_l$), set
\begin{equation}
\boldsymbol \delta^{(n_l)} = - (\boldsymbol y - \boldsymbol a^{(n_l)}) \bullet f'(\boldsymbol z^{(n_l)})
\end{equation}

 where $\boldsymbol a^{(n_l)} $ is the output vector, $\boldsymbol y$ is the expected output, and $\boldsymbol z^{(n_l)} = [\boldsymbol W^{(n_l)}_1\cdot \boldsymbol a^{(n_l-1)} +b^{(n_l)}_1,\hdots,\boldsymbol W^{(n_l)}_n\cdot \boldsymbol a^{(n_l-1)} +b^{(n_l)}_n]$ is the vector of the weighted sums of inputs for each neuron in the final layer. The function $f'(.)$ should be applied element wise to z and depends on the form of the activation function, I will use  $f(x) = max(0, x)$ therefore  $f'( z_i^{(n_l)}) =  z_i^{(n_l)}$ if $ z_i^{(n_l)}>0$ and $=0$ if $ z_i^{(n_l)}\leq 0$. The $\bullet$ denotes element wise multiplication. }

\item[3.]{And for the hidden layers
\begin{align}
\boldsymbol \delta^{(l)} &= \boldsymbol W^{(l+1)}\boldsymbol \delta^{(l+1)}  \bullet f'(\boldsymbol z^{(l)}) \\
&=\begin{bmatrix}
W_{11} &\hdots&W_{1n}\\ 
\vdots &\ddots&\vdots \\
W_{m1} &\hdots&W_{mn}\\ 
\end{bmatrix}
\boldsymbol \delta^{(l+1)}  \bullet f'(\boldsymbol z^{(l)})
\end{align}
where $W^{(l+1)}_{ij}$ is the weight in the connection between neuron $i$ in layer $l$ and neuron $j$ in layer $l+1$, or the $j$th element of the $i$th neurons weight vector in the layer $l+1$. }

\item[4.]{Then compute the matrix of partial derivatives  $\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b)$ and $\frac{\partial}{\partial b_{i}^{(l)}} J(W,b)$ for each layer as
\begin{align}
\nabla_{W^{(l)}} J(W,b) &= \delta^{(l)} (a^{(l-1)})^T \\
\nabla_{b^{(l)}} J(W,b) &= \delta^{(l)}
\end{align}
}
\end{itemize}

which is implemented with this method in the Network class:

\begin{lstlisting}
    List<List<FloatMatrix>> backpropagate(FloatMatrix networkOutput, FloatMatrix expectedOutput)
    {
        //lists of matricies for each layer. These will hold the dE/dW^(l)_ij dE/db^(l)_i terms
        List<FloatMatrix> gradLoss_wrtW = new ArrayList<FloatMatrix>(hiddenLayers.size());
        List<FloatMatrix> gradLoss_wrtB = new ArrayList<FloatMatrix>(hiddenLayers.size());
        
        //get each layer's position in hiddenLayers list also postion in gradLoss_wrtW/b
        int lastLayer = hiddenLayers.size()-1;
        
        //add each layer to 0 position in the list, then they all shift down each time
        //gradLoss_wrtB_l = that layers delta vector
        FloatMatrix deltasLplus1 = hiddenLayers.get(lastLayer).computeDeltas(networkOutput, 
				expectedOutput);
        gradLoss_wrtB.add(0, deltasLplus1);
        gradLoss_wrtW.add(0, deltasLplus1.mmul( hiddenLayers.get(lastLayer).getInputs().transpose()));
        
        FloatMatrix deltasL;
        for(int layer=lastLayer-1; layer>=0; --layer) {
            deltasL = hiddenLayers.get(layer).computeDeltas( hiddenLayers.get(layer+1).
																																						getWeightMatrix(),
                                                             deltasLplus1);
            gradLoss_wrtB.add(0, deltasL);
            gradLoss_wrtW.add(0, deltasL.mmul( hiddenLayers.get(layer).getInputs().transpose()));
            deltasLplus1.copy(deltasL);
        }
        List<List<FloatMatrix>> gradWandB = new ArrayList<List<FloatMatrix>>();
        gradWandB.add(gradLoss_wrtW);
        gradWandB.add(gradLoss_wrtB);
        return gradWandB;
    }
\end{lstlisting}

Because the deltas computation was different for the output layer I created an OutputLayer subclass of Hiddenlayer 

\begin{lstlisting}
import org.jblas.*;
import java.util.*;
/**
Layers
    
 
*/

class OutputLayer extends HiddenLayer
{
    OutputLayer(int layerNumber, int numberOfNeurons, int numberOfInputs)
    {
        super(layerNumber, numberOfNeurons, numberOfInputs);
    }
    
    FloatMatrix computeDeltas(FloatMatrix networkOutput, FloatMatrix expectedOutput)
    {
        //d = (y-a):
        deltas = expectedOutput.sub(networkOutput);
        //d = -d bullet f'(z)
        deltas.muli(-1).muli(fDashOfActivationZ);
        return deltas.dup();
    }
    
    void testComputeDeltasFinalLayer()
    {
        fDashOfActivationZ = FloatMatrix.rand(numberOfNeurons);
        FloatMatrix networkOutput = FloatMatrix.rand(numberOfNeurons, 1);
        FloatMatrix expectedOutput = FloatMatrix.rand(numberOfNeurons, 1);

        FloatMatrix deltas = computeDeltas(networkOutput, expectedOutput);
        Driver.printMatrixDetails("deltas", deltas);
        Driver.is(deltas.rows, numberOfNeurons, "does delta Matrix have correct rows (number of neurons)");
        Driver.is(deltas.columns, 1, "does delta Matrix have correct columns");
    }
    
    void tests()
    {
        testComputeDeltasFinalLayer();
    }

    public static void main(String[] args)
    {
        OutputLayer l = new OutputLayer(1, 2, 3);
        l.tests();
        OutputLayer l2 = new OutputLayer(1, 19, 21);
        l2.tests();
    }
}
\end{lstlisting}
regular hidden layers compute deltas with 
\begin{lstlisting}
    FloatMatrix computeDeltas(FloatMatrix weightMatrixLplus1, FloatMatrix deltasLplus1)
    {
        if(weightMatrixLplus1.columns!=numberOfNeurons) {
            System.out.println("computeDeltas in layer" + layerNumber + " recieved: ");
            Driver.printMatrixDetails("weightMatrixLplus1", weightMatrixLplus1);
            Driver.printMatrixDetails("deltasLplus1", deltasLplus1);
            throw new Error("weightMatrixLplus1 should have " + numberOfNeurons +
                            "columns."  );
        }
        //deltas = W^(l+1) * d^(l+1)
        deltas = weightMatrixLplus1.transpose().mmul(deltasLplus1);
        //d = d bullet f'(z)
        deltas.muli(fDashOfActivationZ);
        return deltas.dup();
    }
\end{lstlisting}
I have changed the forwards pass methods to store all the information needed in the backwardsPass methods, I also had to change a number of the access modifiers to protected to allow the Output layer to set them. 

\begin{lstlisting}
    private int numberOfInputs;
    private List<Neuron> neurons;
    private int layerNumber;
    protected int numberOfNeurons;
    private FloatMatrix activationZ;
    protected FloatMatrix fDashOfActivationZ;
    protected FloatMatrix deltas;
    private FloatMatrix inputs;
    private FloatMatrix outputs;

    FloatMatrix getOutput(FloatMatrix input)
    {
        if(input.length!=numberOfInputs) {
            throw new Error("ERROR: input vector to layer" + layerNumber +
             " is not correct size. expected: " +
            numberOfInputs + " got: " + input.length);
        }
        //save the input for backwards pass
        this.inputs = input.dup();
        FloatMatrix inputPlusBiasConstant = addConstantTermToInputVector(input);
        FloatMatrix outputVector = new FloatMatrix(numberOfNeurons);
        int i=0;
        for(Neuron n: neurons) {
            float neuronActivation = n.getWeightVector().dot(inputPlusBiasConstant);
            activationZ.put(i, 0, neuronActivation);
            fDashOfActivationZ.put(i, 0, activationFunctionDash(neuronActivation));
            outputVector.put(i, 0, activationFunction(neuronActivation));
            ++i;
        }
        outputs = outputVector.dup();
        return outputVector;
    }
\end{lstlisting}
I had a number of bugs in the backPropagate method which took some time to iron out. Matricies wern't the right size to be multiplied etc.. I wasn't entirely confident that I had the correct equations in the first place since the tutorial I was following (the stanford link above) used a  different definition of the network where the weights belong to the layer producing the input rather than recieving it. The backPropagate/compute deltas methods were difficult to test since they were using values computed in the forwards pass.

 I started out checking the my weight matrix extraction method in the HiddenLayers was working correctly

\begin{lstlisting}
       FloatMatrix getWeightMatrix()
    {
        FloatMatrix weightMatrix = new FloatMatrix(numberOfNeurons, numberOfInputs);
        int i=0;
        for(Neuron n: neurons) {
            weightMatrix.putRow(i, n.getWeightVectorNoBias());
            ++i;
        }
        return weightMatrix;
    }

    void testGetWeightMatrix()
    {
        FloatMatrix weightMatrix = getWeightMatrix();
        int unit=0;
        for(Neuron n: neurons) {
            FloatMatrix weightsN = n.getWeightVectorNoBias();
            for(int i=0; i<numberOfInputs; ++i) {
                Driver.is( weightsN.get(i,0), weightMatrix.get(unit, i),
                "checking that weight matrix equals each element of the weights");
            }
            ++unit;
        }
    }
    
    \end{lstlisting}

then I checked that compute deltas were working as expected:
\begin{lstlisting}
    void testComputeDeltas()
    {
        fDashOfActivationZ = FloatMatrix.rand(numberOfNeurons);
        FloatMatrix weightMatrixLplus1 = FloatMatrix.rand(3, numberOfNeurons);
        FloatMatrix deltasLplus1 = FloatMatrix.rand(3);

        FloatMatrix deltas = computeDeltas(weightMatrixLplus1,deltasLplus1);
        Driver.printMatrixDetails("deltas", deltas);
        Driver.is(deltas.rows, numberOfNeurons, "does delta Matrix have correct rows (number of neurons)");
        Driver.is(deltas.columns, 1, "does delta Matrix have correct columns");
        
        weightMatrixLplus1 = FloatMatrix.rand(20, numberOfNeurons);
        deltasLplus1 = FloatMatrix.rand(20);

        deltas = computeDeltas(weightMatrixLplus1,deltasLplus1);
        Driver.printMatrixDetails("deltas", deltas);
        Driver.is(deltas.rows, numberOfNeurons, "does delta Matrix have correct rows (number of neurons)");
        Driver.is(deltas.columns, 1, "does delta Matrix have correct columns");
    }
\end{lstlisting}

and then I took a copy of backPropate and filled it with calls to new method in HiddenLayer class printLayerDetails() and a static helper method in Driver printMatrixDetails :
\begin{lstlisting}
    void printLayerDetails()
    {
        System.out.println("\nLAYER " + layerNumber);
        System.out.println("number of inputs: " + numberOfInputs);
        System.out.println("number of neurons: " + numberOfNeurons);
        Driver.printMatrixDetails("activationZ", activationZ);
        Driver.printMatrixDetails("fDashOfActivationZ", fDashOfActivationZ);
        Driver.printMatrixDetails("deltas", deltas);
        Driver.printMatrixDetails("inputs", inputs);
        Driver.printMatrixDetails("outputs", outputs);
        FloatMatrix weightMatrix = getWeightMatrix();
        Driver.printMatrixDetails("weightMatrix", weightMatrix);
        FloatMatrix biasVector = getBiasVector();
        Driver.printMatrixDetails("biasVector", biasVector);
        System.out.println("\n");
    }

    static void printMatrixDetails(String name, FloatMatrix m)
    {
        System.out.println("FloatMatrix " + name + " has " + m.rows + " rows and " + m.columns + " columns. Contains: ");
        System.out.println(m.toString()+ "\n");
    }
\end{lstlisting}





which I found very helpful for checking the state of the layers in the backPropogate method:
\begin{lstlisting}
    
    List<List<FloatMatrix>> backpropagateWithChecks(FloatMatrix networkOutput, FloatMatrix expectedOutput)
    {
        Driver.is(networkOutput.length, layerWidths[layerWidths.length-1], "are the netOutputs right size");
        Driver.is(expectedOutput.length, layerWidths[layerWidths.length-1]);

        //lists of matricies for each layer. These will hold the dE/dW^(l)_ij dE/db^(l)_i terms
        List<FloatMatrix> gradLoss_wrtW = new ArrayList<FloatMatrix>(hiddenLayers.size());
        List<FloatMatrix> gradLoss_wrtB = new ArrayList<FloatMatrix>(hiddenLayers.size());
        
        //get each layer's position in hiddenLayers list also postion in gradLoss_wrtW/b
        int lastLayer = hiddenLayers.size()-1;
        
        //add each layer to 0 position in the list, then they all shift down each time
        //gradLoss_wrtB_l = that layers delta vector
        FloatMatrix deltasLplus1 = hiddenLayers.get(lastLayer).computeDeltas(networkOutput, expectedOutput);
        gradLoss_wrtB.add(0, deltasLplus1);
        gradLoss_wrtW.add(0, deltasLplus1.mmul( hiddenLayers.get(lastLayer).getInputs().transpose()));
        hiddenLayers.get(lastLayer).printLayerDetails();

        Driver.is(hiddenLayers.get(lastLayer).getNumberOfNeurons(), deltasLplus1.rows,
             " are output layer deltas the sma number of rows as number of neurons");
        Driver.is(1, deltasLplus1.columns,  " are output layer deltas have 1 col");
        Driver.printMatrixDetails("gradLoss_wrtW.get(0)", gradLoss_wrtW.get(0));
        
        FloatMatrix deltasL;
        for(int layer=lastLayer-1; layer>=0; --layer) {
            hiddenLayers.get(layer).printLayerDetails();
            deltasL = hiddenLayers.get(layer).computeDeltas( hiddenLayers.get(layer+1).getWeightMatrix(),
                                                             deltasLplus1);
            Driver.is(hiddenLayers.get(layer).getNumberOfNeurons(), deltasL.rows,
             " are layer deltas the sma number of rows as number of neurons");
            Driver.is(1, deltasL.columns,  " do layer deltas have 1 col");
            
            gradLoss_wrtB.add(0, deltasL);
            
            Driver.is(gradLoss_wrtB.get(0).rows,
             hiddenLayers.get(layer).getNumberOfNeurons(),
             " are each hiddenLayers gradLoss_wrtB the sma number of rows as its number of neurons");
            Driver.is(gradLoss_wrtB.get(0).columns,
             1,
             " are each hiddenLayers gradLoss_wrtB has 1 col");
            
            gradLoss_wrtW.add(0, deltasL.mmul( hiddenLayers.get(layer).getInputs().transpose()));
            
            Driver.printMatrixDetails("gradLoss_wrtW.get(0)", gradLoss_wrtW.get(0));

            Driver.is(gradLoss_wrtW.get(0).rows,
             hiddenLayers.get(layer).getWeightMatrix().rows,
             " are each hiddenLayers gradLoss_wrtW the sma number of rows as its weight matrix");
              Driver.is(gradLoss_wrtW.get(0).columns,
             hiddenLayers.get(layer).getWeightMatrix().columns,
             " are each hiddenLayers gradLoss_wrtW the sma number of cols as its weight matrix");
             deltasLplus1.copy(deltasL);
        }
        List<List<FloatMatrix>> gradWandB = new ArrayList<List<FloatMatrix>>();
        gradWandB.add(gradLoss_wrtW);
        gradWandB.add(gradLoss_wrtB);
        Driver.finishTesting("backpropagateWithChecks");
        return gradWandB;
    }
    
    void testBackPropagate()
    {
        System.out.println("testBackPropagate");
        testForwardPass();
        List<List<FloatMatrix>> gradWandB = backpropagateWithChecks(
            FloatMatrix.rand(layerWidths[layerWidths.length-1]),
            FloatMatrix.rand(layerWidths[layerWidths.length-1])    );
        List<FloatMatrix> gradW = gradWandB.get(0);
        List<FloatMatrix> gradB = gradWandB.get(1);

        for(int layer=0; layer<hiddenLayers.size(); ++layer) {
            hiddenLayers.get(layer).printLayerDetails();
            Driver.printMatrixDetails("gradW.get(layer)", gradW.get(layer));
            Driver.is(gradW.get(layer).rows, hiddenLayers.get(layer).getNumberOfNeurons(),
            "does GradW rows equal number of nuerons");
            Driver.is(gradW.get(layer).columns, hiddenLayers.get(layer).getWeightMatrix().columns,
            "does GradW columns equal weightMatrix columns");
        }
        Driver.finishTesting("testBackPropagate");
    }
\end{lstlisting}





The Trainer class which handles all of this accumulating weight/bias updates over a batch of examples

\begin{lstlisting}
import org.jblas.*;
import java.util.*;

class Trainer
{
    private Network net;
    private int inputWidth;
    private int trainingBatchSize=10000;
    private int validationSetSize=20;
    private float weightDecay=0;
    private float momentum=(float)0.9;
    private float learningRate=(float)0.001;

    Trainer(Network net, int inputWidth)
    {
        this.net = net;
        this.inputWidth = inputWidth;
    }
    
    void trainNetwork()
    {
        int batchNumber=1;
        while(batchNumber<10000) {
            presentTrainingBatch();
            float validationError = measureValidationError();
            System.out.println("Batch number = " + batchNumber +
                               ". Validation error = " + validationError);
            ++batchNumber;
        }
    }
    
    private float measureValidationError()
    {
        float validationError = 0;
        for(int i=1; i<=validationSetSize; ++i) {
            List<FloatMatrix> example = getExample();
            FloatMatrix netOutput = net.computeFowardPass(example.get(0));
            validationError += computeExampleLoss(netOutput, example.get(1));
        }
        return validationError/(float)validationSetSize;
    }
    
    private void presentTrainingBatch()
    {
        List<FloatMatrix> deltaW_l = new ArrayList<FloatMatrix>();
        List<FloatMatrix> deltaB_l = new ArrayList<FloatMatrix>();
        for(int i=1; i<=trainingBatchSize; ++i) {
            List<FloatMatrix> example = getExample();
            List<List<FloatMatrix>> gradLoss_wrtWandB = presentTrainingExample(example.get(0), example.get(1));
            List<FloatMatrix> gradLoss_wrtW = gradLoss_wrtWandB.get(0);
            List<FloatMatrix> gradLoss_wrtB = gradLoss_wrtWandB.get(1);
            if(i==1) {
                deltaW_l = Driver.copySizingInMatrixList(gradLoss_wrtW);
                deltaB_l = Driver.copySizingInMatrixList(gradLoss_wrtB);
                net.initialisePreviousUpdates(gradLoss_wrtW,gradLoss_wrtB);
            }
            //accumulate each update gradLoss_wrtW/B in deltaW_l
            Driver.addFloatMatrixListsi(deltaW_l, gradLoss_wrtW);
            Driver.addFloatMatrixListsi(deltaB_l, gradLoss_wrtB);
        }
        Driver.scalarMultiplyFloatMatrixListsi(deltaW_l, (1/(float)trainingBatchSize));
        Driver.scalarMultiplyFloatMatrixListsi(deltaB_l, (1/(float)trainingBatchSize));
        net.updateParameters(deltaW_l, deltaB_l, weightDecay, momentum, learningRate);
    }
    
    private List<List<FloatMatrix>> presentTrainingExample(FloatMatrix input, FloatMatrix label)
    {
        FloatMatrix netOutput = net.computeFowardPass(input);
        return net.backpropagate(netOutput, label);
    }
    
    private float computeExampleLoss(FloatMatrix netOutput, FloatMatrix expectedOutput)
    {
        netOutput.subi(expectedOutput);
        return (float)0.5*netOutput.dot(netOutput);
    }
    
    //returns a 2 floatMatricies, first is the input, 2nd is the expected output
    private List<FloatMatrix> getExample()
    {
        return generateRandomSinSample();
    }
    
    private List<FloatMatrix> generateRandomSinSample()
    {
        List<FloatMatrix> example = new ArrayList<FloatMatrix>();
    
        float randomX = Driver.randomNumberGen.nextFloat()*(float)Math.PI-((float)Math.PI/2);
        float randomY = (float)Math.sin(randomX);
        /*FloatMatrix(int newRows, int newColumns, float... newData)
          Create a new matrix with newRows rows, newColumns columns using newData> as the data.
        */
        example.add(new FloatMatrix(1,1,randomX));
        example.add(new FloatMatrix(1,1,randomY));
        return example;
    }
    
 
   
}
\end{lstlisting}
\begin{lstlisting}

\end{lstlisting}
\begin{lstlisting}

\end{lstlisting}
\begin{lstlisting}

\end{lstlisting}
\begin{lstlisting}

\end{lstlisting}
\begin{lstlisting}

\end{lstlisting}
\begin{lstlisting}

\end{lstlisting}
\end{document}










